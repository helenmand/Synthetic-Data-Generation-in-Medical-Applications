{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e939e61-f3d2-4ea3-882f-fc40185f33b2",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24c8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class KneeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def get_data_loader(root_dir, batch_size, image_size):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        dataset = KneeDataset(root_dir, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        print(f\"Found {len(dataset)} images in {root_dir}\")\n",
    "\n",
    "        return dataloader\n",
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for subdir, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpeg\"):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_loader(root_dir, batch_size, image_size):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        dataset = ChestDataset(root_dir, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "        print(f\"Found {len(dataset)} images in {root_dir}\")\n",
    "\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2daa7-4b6b-4ef9-ab91-35fdb5ae2548",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b609f7-aa5f-40a9-898c-1f2878b807aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6595d-e7d5-4c06-8d2e-c32f451c9575",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7e05e6-50bb-464d-abc4-3ba3f8a83816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, generator, discriminator, gen_optimizer, dis_optimizer,\n",
    "                 gp_weight=10, critic_iterations=5, print_every=50,\n",
    "                 use_cuda=False):\n",
    "        self.G = generator\n",
    "        self.G_opt = gen_optimizer\n",
    "        self.D = discriminator\n",
    "        self.D_opt = dis_optimizer\n",
    "        self.losses = {'G': [], 'D': [], 'GP': [], 'gradient_norm': []}\n",
    "        self.num_steps = 0\n",
    "        self.use_cuda = use_cuda\n",
    "        self.gp_weight = gp_weight\n",
    "        self.critic_iterations = critic_iterations\n",
    "        self.print_every = print_every\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "    def _critic_train_iteration(self, data):\n",
    "        \"\"\" \"\"\"\n",
    "        # Get generated data\n",
    "        batch_size = data.size()[0]\n",
    "        generated_data = self.sample_generator(batch_size)\n",
    "\n",
    "        # Calculate probabilities on real and generated data\n",
    "        data = Variable(data)\n",
    "        if self.use_cuda:\n",
    "            data = data.cuda()\n",
    "        d_real = self.D(data)\n",
    "        d_generated = self.D(generated_data)\n",
    "\n",
    "        # Get gradient penalty\n",
    "        gradient_penalty = self._gradient_penalty(data, generated_data)\n",
    "        self.losses['GP'].append(gradient_penalty.item())\n",
    "\n",
    "        # Create total loss and optimize\n",
    "        self.D_opt.zero_grad()\n",
    "        d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n",
    "        d_loss.backward()\n",
    "\n",
    "        self.D_opt.step()\n",
    "\n",
    "        # Record loss\n",
    "        self.losses['D'].append(d_loss.item())\n",
    "\n",
    "    def _generator_train_iteration(self, data):\n",
    "        \"\"\" \"\"\"\n",
    "        self.G_opt.zero_grad()\n",
    "\n",
    "        # Get generated data\n",
    "        batch_size = data.size()[0]\n",
    "        generated_data = self.sample_generator(batch_size)\n",
    "\n",
    "        # Calculate loss and optimize\n",
    "        d_generated = self.D(generated_data)\n",
    "        g_loss = - d_generated.mean()\n",
    "        g_loss.backward()\n",
    "        self.G_opt.step()\n",
    "\n",
    "        # Record loss\n",
    "        self.losses['G'].append(g_loss.item())\n",
    "\n",
    "    def _gradient_penalty(self, real_data, generated_data):\n",
    "        batch_size = real_data.size()[0]\n",
    "\n",
    "        # Calculate interpolation\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        if self.use_cuda:\n",
    "            alpha = alpha.cuda()\n",
    "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "        if self.use_cuda:\n",
    "            interpolated = interpolated.cuda()\n",
    "\n",
    "        # Calculate probability of interpolated examples\n",
    "        prob_interpolated = self.D(interpolated)\n",
    "\n",
    "        # Calculate gradients of probabilities with respect to examples\n",
    "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(\n",
    "                               prob_interpolated.size()),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "        # so flatten to easily take norm per example in batch\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().item())\n",
    "\n",
    "        # Derivatives of the gradient close to 0 can cause problems because of\n",
    "        # the square root, so manually calculate norm and add epsilon\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "        # Return gradient penalty\n",
    "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "    def _train_epoch(self, data_loader):\n",
    "        for i, data in enumerate(data_loader):\n",
    "            self.num_steps += 1\n",
    "            self._critic_train_iteration(data[0])\n",
    "            # Only update generator every |critic_iterations| iterations\n",
    "            if self.num_steps % self.critic_iterations == 0:\n",
    "                self._generator_train_iteration(data[0])\n",
    "\n",
    "            if i % self.print_every == 0:\n",
    "                print(\"Iteration {}\".format(i + 1))\n",
    "                print(\"D: {}\".format(self.losses['D'][-1]))\n",
    "                print(\"GP: {}\".format(self.losses['GP'][-1]))\n",
    "                print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
    "                if self.num_steps > self.critic_iterations:\n",
    "                    print(\"G: {}\".format(self.losses['G'][-1]))\n",
    "\n",
    "    def train(self, data_loader, epochs, save_training_gif=True):\n",
    "        if save_training_gif:\n",
    "            # Fix latents to see how image generation improves during training\n",
    "            fixed_latents = Variable(self.G.sample_latent(64))\n",
    "            if self.use_cuda:\n",
    "                fixed_latents = fixed_latents.cuda()\n",
    "            training_progress_images = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nEpoch {}\".format(epoch + 1))\n",
    "            self._train_epoch(data_loader)\n",
    "\n",
    "            if save_training_gif:\n",
    "                # Generate batch of images and convert to grid\n",
    "                img_grid = make_grid(self.G(fixed_latents).cpu().data)\n",
    "                # Convert to numpy and transpose axes to fit imageio convention\n",
    "                # i.e. (width, height, channels)\n",
    "                img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
    "                # Add image grid to training progress\n",
    "                training_progress_images.append(img_grid)\n",
    "\n",
    "        if save_training_gif:\n",
    "            imageio.mimsave('./training_{}_epochs.gif'.format(epochs),\n",
    "                            training_progress_images)\n",
    "\n",
    "    def sample_generator(self, num_samples):\n",
    "        latent_samples = Variable(self.G.sample_latent(num_samples))\n",
    "        if self.use_cuda:\n",
    "            latent_samples = latent_samples.cuda()\n",
    "        generated_data = self.G(latent_samples)\n",
    "        return generated_data\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        generated_data = self.sample_generator(num_samples)\n",
    "        # Remove color channel\n",
    "        return generated_data.data.cpu().numpy()[:, 0, :, :]\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'G_state_dict': self.G.state_dict(),\n",
    "            'D_state_dict': self.D.state_dict(),\n",
    "            'G_opt_state_dict': self.G_opt.state_dict(),\n",
    "            'D_opt_state_dict': self.D_opt.state_dict(),\n",
    "            'num_steps': self.num_steps,\n",
    "            'losses': self.losses,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.G.load_state_dict(checkpoint['G_state_dict'])\n",
    "        self.D.load_state_dict(checkpoint['D_state_dict'])\n",
    "        self.G_opt.load_state_dict(checkpoint['G_opt_state_dict'])\n",
    "        self.D_opt.load_state_dict(checkpoint['D_opt_state_dict'])\n",
    "        self.num_steps = checkpoint['num_steps']\n",
    "        self.losses = checkpoint['losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd74f8a-adde-4591-a25a-aeadcc69f780",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1e2f4-7b5d-4b51-bcbe-6c105a242dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the Generator and Discriminator classes (as shown above)\n",
    "# from models import Generator, Discriminator\n",
    "\n",
    "class Opt:\n",
    "    n_epochs = 200\n",
    "    batch_size = 64\n",
    "    lr = 0.0002\n",
    "    b1 = 0.5\n",
    "    b2 = 0.999\n",
    "    n_cpu = 8\n",
    "    latent_dim = 100\n",
    "    img_size = 128\n",
    "    channels = 1\n",
    "    sample_interval = 400\n",
    "\n",
    "opt = Opt()\n",
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(f\"Using CUDA: {cuda}\")\n",
    "\n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(opt)\n",
    "discriminator = Discriminator(opt)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = KneeDataset.get_data_loader('/home/e/emandan/ml/datasets/kneeXray', opt.batch_size, opt.img_size)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Training loop\n",
    "batches_done = 0\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        fake_imgs = generator(z)\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_imgs = generator(z)\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(fake_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "        batches_done += 1\n",
    "\n",
    "torch.save(generator.state_dict(), 'generator_final.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_final.pth')\n",
    "\n",
    "generator = Generator(opt)\n",
    "discriminator = Discriminator(opt)\n",
    "\n",
    "# Load the state dictionaries\n",
    "generator.load_state_dict(torch.load('generator_wgangp.pth'))\n",
    "discriminator.load_state_dict(torch.load('discriminator_wgangp.pth'))\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "generator.eval()\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79b411-d3e6-45fc-a096-6a747b336d2a",
   "metadata": {},
   "source": [
    "# chestXray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d48a94-5e09-4977-9f68-3cd44334e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "Found 5856 images in /home/e/emandan/ml/datasets/chestXray\n",
      "[Epoch 0/200] [Batch 0/46] [D loss: 9.231800] [G loss: 0.003322]\n",
      "[Epoch 0/200] [Batch 5/46] [D loss: 8.971395] [G loss: 0.001922]\n",
      "[Epoch 0/200] [Batch 10/46] [D loss: 8.447470] [G loss: 0.001100]\n",
      "[Epoch 0/200] [Batch 15/46] [D loss: 7.163417] [G loss: -0.008622]\n",
      "[Epoch 0/200] [Batch 20/46] [D loss: 2.018724] [G loss: -0.074129]\n",
      "[Epoch 0/200] [Batch 25/46] [D loss: -10.336364] [G loss: -0.419502]\n",
      "[Epoch 0/200] [Batch 30/46] [D loss: -18.587147] [G loss: -1.232784]\n",
      "[Epoch 0/200] [Batch 35/46] [D loss: -21.309423] [G loss: -2.610467]\n",
      "[Epoch 0/200] [Batch 40/46] [D loss: -23.910854] [G loss: -3.582813]\n",
      "[Epoch 0/200] [Batch 45/46] [D loss: -26.599634] [G loss: -6.597435]\n",
      "[Epoch 1/200] [Batch 0/46] [D loss: -25.378969] [G loss: -8.186962]\n",
      "[Epoch 1/200] [Batch 5/46] [D loss: -25.964159] [G loss: -9.570477]\n",
      "[Epoch 1/200] [Batch 10/46] [D loss: -27.021345] [G loss: -12.604177]\n",
      "[Epoch 1/200] [Batch 15/46] [D loss: -29.293674] [G loss: -13.684625]\n",
      "[Epoch 1/200] [Batch 20/46] [D loss: -31.223509] [G loss: -16.799641]\n",
      "[Epoch 1/200] [Batch 25/46] [D loss: -26.165152] [G loss: -19.990707]\n",
      "[Epoch 1/200] [Batch 30/46] [D loss: -24.198294] [G loss: -24.832428]\n",
      "[Epoch 1/200] [Batch 35/46] [D loss: -27.750048] [G loss: -23.947475]\n",
      "[Epoch 1/200] [Batch 40/46] [D loss: -27.817760] [G loss: -23.738964]\n",
      "[Epoch 1/200] [Batch 45/46] [D loss: -31.164993] [G loss: -24.429947]\n",
      "[Epoch 2/200] [Batch 0/46] [D loss: -25.917377] [G loss: -27.884691]\n",
      "[Epoch 2/200] [Batch 5/46] [D loss: -25.510635] [G loss: -28.358028]\n",
      "[Epoch 2/200] [Batch 10/46] [D loss: -23.820091] [G loss: -31.003784]\n",
      "[Epoch 2/200] [Batch 15/46] [D loss: -29.699743] [G loss: -31.618116]\n",
      "[Epoch 2/200] [Batch 20/46] [D loss: -30.816147] [G loss: -30.108425]\n",
      "[Epoch 2/200] [Batch 25/46] [D loss: -24.577503] [G loss: -34.328201]\n",
      "[Epoch 2/200] [Batch 30/46] [D loss: -19.716532] [G loss: -34.962051]\n",
      "[Epoch 2/200] [Batch 35/46] [D loss: -18.330162] [G loss: -38.793678]\n",
      "[Epoch 2/200] [Batch 40/46] [D loss: -20.151299] [G loss: -35.376442]\n",
      "[Epoch 2/200] [Batch 45/46] [D loss: -16.696609] [G loss: -37.991306]\n",
      "[Epoch 3/200] [Batch 0/46] [D loss: -17.204836] [G loss: -36.855934]\n",
      "[Epoch 3/200] [Batch 5/46] [D loss: -12.230856] [G loss: -36.353271]\n",
      "[Epoch 3/200] [Batch 10/46] [D loss: -13.492252] [G loss: -39.068615]\n",
      "[Epoch 3/200] [Batch 15/46] [D loss: -13.315681] [G loss: -35.655952]\n",
      "[Epoch 3/200] [Batch 20/46] [D loss: -11.547212] [G loss: -35.520348]\n",
      "[Epoch 3/200] [Batch 25/46] [D loss: -12.120968] [G loss: -35.141434]\n",
      "[Epoch 3/200] [Batch 30/46] [D loss: -15.489626] [G loss: -30.552639]\n",
      "[Epoch 3/200] [Batch 35/46] [D loss: -13.017852] [G loss: -29.124485]\n",
      "[Epoch 3/200] [Batch 40/46] [D loss: -8.148405] [G loss: -27.734253]\n",
      "[Epoch 3/200] [Batch 45/46] [D loss: -11.035785] [G loss: -28.737259]\n",
      "[Epoch 4/200] [Batch 0/46] [D loss: -7.637677] [G loss: -26.842262]\n",
      "[Epoch 4/200] [Batch 5/46] [D loss: -7.764565] [G loss: -26.802361]\n",
      "[Epoch 4/200] [Batch 10/46] [D loss: -8.242995] [G loss: -24.334661]\n",
      "[Epoch 4/200] [Batch 15/46] [D loss: -9.100753] [G loss: -21.371216]\n",
      "[Epoch 4/200] [Batch 20/46] [D loss: -8.339515] [G loss: -23.318750]\n",
      "[Epoch 4/200] [Batch 25/46] [D loss: -6.783997] [G loss: -18.847328]\n",
      "[Epoch 4/200] [Batch 30/46] [D loss: -8.534096] [G loss: -19.507786]\n",
      "[Epoch 4/200] [Batch 35/46] [D loss: -8.829937] [G loss: -19.353073]\n",
      "[Epoch 4/200] [Batch 40/46] [D loss: -7.079022] [G loss: -19.358887]\n",
      "[Epoch 4/200] [Batch 45/46] [D loss: -6.998257] [G loss: -14.886457]\n",
      "[Epoch 5/200] [Batch 0/46] [D loss: -8.397070] [G loss: -16.267735]\n",
      "[Epoch 5/200] [Batch 5/46] [D loss: -8.244642] [G loss: -14.375340]\n",
      "[Epoch 5/200] [Batch 10/46] [D loss: -7.806919] [G loss: -13.248029]\n",
      "[Epoch 5/200] [Batch 15/46] [D loss: -6.242932] [G loss: -15.438109]\n",
      "[Epoch 5/200] [Batch 20/46] [D loss: -5.789218] [G loss: -15.330800]\n",
      "[Epoch 5/200] [Batch 25/46] [D loss: -5.085872] [G loss: -11.745848]\n",
      "[Epoch 5/200] [Batch 30/46] [D loss: -6.779118] [G loss: -11.554972]\n",
      "[Epoch 5/200] [Batch 35/46] [D loss: -7.408622] [G loss: -12.270029]\n",
      "[Epoch 5/200] [Batch 40/46] [D loss: -7.540673] [G loss: -8.216641]\n",
      "[Epoch 5/200] [Batch 45/46] [D loss: -7.936013] [G loss: -9.434478]\n",
      "[Epoch 6/200] [Batch 0/46] [D loss: -5.706708] [G loss: -11.707643]\n",
      "[Epoch 6/200] [Batch 5/46] [D loss: -4.652896] [G loss: -10.637509]\n",
      "[Epoch 6/200] [Batch 10/46] [D loss: -4.992617] [G loss: -10.019940]\n",
      "[Epoch 6/200] [Batch 15/46] [D loss: -5.116925] [G loss: -6.236406]\n",
      "[Epoch 6/200] [Batch 20/46] [D loss: -5.429813] [G loss: -7.857383]\n",
      "[Epoch 6/200] [Batch 25/46] [D loss: -7.446299] [G loss: -7.126830]\n",
      "[Epoch 6/200] [Batch 30/46] [D loss: -4.382646] [G loss: -6.216043]\n",
      "[Epoch 6/200] [Batch 35/46] [D loss: -7.951591] [G loss: -7.337057]\n",
      "[Epoch 6/200] [Batch 40/46] [D loss: -6.758217] [G loss: -7.837924]\n",
      "[Epoch 6/200] [Batch 45/46] [D loss: -7.557505] [G loss: -8.301971]\n",
      "[Epoch 7/200] [Batch 0/46] [D loss: -7.418221] [G loss: -7.161096]\n",
      "[Epoch 7/200] [Batch 5/46] [D loss: -3.920352] [G loss: -6.467405]\n",
      "[Epoch 7/200] [Batch 10/46] [D loss: -5.586774] [G loss: -4.930223]\n",
      "[Epoch 7/200] [Batch 15/46] [D loss: -4.957727] [G loss: -4.415443]\n",
      "[Epoch 7/200] [Batch 20/46] [D loss: -5.552034] [G loss: -4.381465]\n",
      "[Epoch 7/200] [Batch 25/46] [D loss: -5.925034] [G loss: -4.791852]\n",
      "[Epoch 7/200] [Batch 30/46] [D loss: -4.903364] [G loss: -4.031834]\n",
      "[Epoch 7/200] [Batch 35/46] [D loss: -6.511469] [G loss: -1.944426]\n",
      "[Epoch 7/200] [Batch 40/46] [D loss: -6.025800] [G loss: -6.848577]\n",
      "[Epoch 7/200] [Batch 45/46] [D loss: -3.473661] [G loss: -7.835789]\n",
      "[Epoch 8/200] [Batch 0/46] [D loss: -6.169787] [G loss: -6.937127]\n",
      "[Epoch 8/200] [Batch 5/46] [D loss: -5.483796] [G loss: -7.214918]\n",
      "[Epoch 8/200] [Batch 10/46] [D loss: -5.474942] [G loss: -3.511385]\n",
      "[Epoch 8/200] [Batch 15/46] [D loss: -6.386794] [G loss: -5.309851]\n",
      "[Epoch 8/200] [Batch 20/46] [D loss: -5.258370] [G loss: -3.066386]\n",
      "[Epoch 8/200] [Batch 25/46] [D loss: -4.723617] [G loss: -4.265090]\n",
      "[Epoch 8/200] [Batch 30/46] [D loss: -4.279127] [G loss: -0.641803]\n",
      "[Epoch 8/200] [Batch 35/46] [D loss: -4.278474] [G loss: 0.521276]\n",
      "[Epoch 8/200] [Batch 40/46] [D loss: -5.963267] [G loss: -1.396378]\n",
      "[Epoch 8/200] [Batch 45/46] [D loss: -4.617596] [G loss: -1.910712]\n",
      "[Epoch 9/200] [Batch 0/46] [D loss: -2.548819] [G loss: -3.683327]\n",
      "[Epoch 9/200] [Batch 5/46] [D loss: -4.061179] [G loss: -5.137116]\n",
      "[Epoch 9/200] [Batch 10/46] [D loss: -4.248814] [G loss: -9.607800]\n",
      "[Epoch 9/200] [Batch 15/46] [D loss: -3.483405] [G loss: -6.785141]\n",
      "[Epoch 9/200] [Batch 20/46] [D loss: -4.964076] [G loss: -5.874321]\n",
      "[Epoch 9/200] [Batch 25/46] [D loss: -5.527089] [G loss: -4.172735]\n",
      "[Epoch 9/200] [Batch 30/46] [D loss: -5.411687] [G loss: -4.779202]\n",
      "[Epoch 9/200] [Batch 35/46] [D loss: -3.807308] [G loss: -3.369897]\n",
      "[Epoch 9/200] [Batch 40/46] [D loss: -4.552233] [G loss: -4.241859]\n",
      "[Epoch 9/200] [Batch 45/46] [D loss: -6.293747] [G loss: -2.157153]\n",
      "[Epoch 10/200] [Batch 0/46] [D loss: -3.790340] [G loss: -2.107531]\n",
      "[Epoch 10/200] [Batch 5/46] [D loss: -5.002592] [G loss: 1.587621]\n",
      "[Epoch 10/200] [Batch 10/46] [D loss: -3.021792] [G loss: 0.516305]\n",
      "[Epoch 10/200] [Batch 15/46] [D loss: -3.479023] [G loss: -3.139753]\n",
      "[Epoch 10/200] [Batch 20/46] [D loss: -4.212946] [G loss: -2.095413]\n",
      "[Epoch 10/200] [Batch 25/46] [D loss: -2.627625] [G loss: 0.242648]\n",
      "[Epoch 10/200] [Batch 30/46] [D loss: -3.132309] [G loss: 0.264963]\n",
      "[Epoch 10/200] [Batch 35/46] [D loss: -3.014242] [G loss: 1.482528]\n",
      "[Epoch 10/200] [Batch 40/46] [D loss: -3.006380] [G loss: -1.696761]\n",
      "[Epoch 10/200] [Batch 45/46] [D loss: -4.037126] [G loss: -5.104086]\n",
      "[Epoch 11/200] [Batch 0/46] [D loss: -3.300826] [G loss: -5.073081]\n",
      "[Epoch 11/200] [Batch 5/46] [D loss: -2.516652] [G loss: -6.575558]\n",
      "[Epoch 11/200] [Batch 10/46] [D loss: -2.871836] [G loss: -5.624720]\n",
      "[Epoch 11/200] [Batch 15/46] [D loss: -1.971931] [G loss: -2.888684]\n",
      "[Epoch 11/200] [Batch 20/46] [D loss: -4.614347] [G loss: -2.908049]\n",
      "[Epoch 11/200] [Batch 25/46] [D loss: -3.311144] [G loss: -1.006325]\n",
      "[Epoch 11/200] [Batch 30/46] [D loss: -4.383701] [G loss: 0.185260]\n",
      "[Epoch 11/200] [Batch 35/46] [D loss: -3.154231] [G loss: 2.198845]\n",
      "[Epoch 11/200] [Batch 40/46] [D loss: -2.821220] [G loss: -0.838697]\n",
      "[Epoch 11/200] [Batch 45/46] [D loss: -2.918220] [G loss: -2.204322]\n",
      "[Epoch 12/200] [Batch 0/46] [D loss: -0.938737] [G loss: -2.862423]\n",
      "[Epoch 12/200] [Batch 5/46] [D loss: -3.702154] [G loss: -3.520818]\n",
      "[Epoch 12/200] [Batch 10/46] [D loss: -2.398041] [G loss: -0.731712]\n",
      "[Epoch 12/200] [Batch 15/46] [D loss: -3.424170] [G loss: -0.059945]\n",
      "[Epoch 12/200] [Batch 20/46] [D loss: -4.060103] [G loss: -1.973586]\n",
      "[Epoch 12/200] [Batch 25/46] [D loss: -3.336647] [G loss: -3.974103]\n",
      "[Epoch 12/200] [Batch 30/46] [D loss: -1.434736] [G loss: -3.204216]\n",
      "[Epoch 12/200] [Batch 35/46] [D loss: -3.182311] [G loss: -1.511428]\n",
      "[Epoch 12/200] [Batch 40/46] [D loss: -4.573658] [G loss: 1.704823]\n",
      "[Epoch 12/200] [Batch 45/46] [D loss: -2.676701] [G loss: 2.539342]\n",
      "[Epoch 13/200] [Batch 0/46] [D loss: -3.030440] [G loss: 1.627994]\n",
      "[Epoch 13/200] [Batch 5/46] [D loss: -2.871992] [G loss: 4.061983]\n",
      "[Epoch 13/200] [Batch 10/46] [D loss: -5.360157] [G loss: 2.656934]\n",
      "[Epoch 13/200] [Batch 15/46] [D loss: -2.418723] [G loss: 2.124482]\n",
      "[Epoch 13/200] [Batch 20/46] [D loss: -2.611312] [G loss: 3.174923]\n",
      "[Epoch 13/200] [Batch 25/46] [D loss: -3.652637] [G loss: 0.268616]\n",
      "[Epoch 13/200] [Batch 30/46] [D loss: -3.214735] [G loss: -0.377210]\n",
      "[Epoch 13/200] [Batch 35/46] [D loss: -2.650739] [G loss: -3.670109]\n",
      "[Epoch 13/200] [Batch 40/46] [D loss: -2.075285] [G loss: -5.136677]\n",
      "[Epoch 13/200] [Batch 45/46] [D loss: -2.785967] [G loss: -6.809085]\n",
      "[Epoch 14/200] [Batch 0/46] [D loss: -3.757316] [G loss: -7.569537]\n",
      "[Epoch 14/200] [Batch 5/46] [D loss: -3.601859] [G loss: -7.881341]\n",
      "[Epoch 14/200] [Batch 10/46] [D loss: -2.553987] [G loss: -5.409404]\n",
      "[Epoch 14/200] [Batch 15/46] [D loss: -2.379529] [G loss: -6.209250]\n",
      "[Epoch 14/200] [Batch 20/46] [D loss: -2.189645] [G loss: -2.848144]\n",
      "[Epoch 14/200] [Batch 25/46] [D loss: -1.198889] [G loss: -0.288815]\n",
      "[Epoch 14/200] [Batch 30/46] [D loss: -2.255424] [G loss: 2.349019]\n",
      "[Epoch 14/200] [Batch 35/46] [D loss: -2.751999] [G loss: -0.396440]\n",
      "[Epoch 14/200] [Batch 40/46] [D loss: -1.653068] [G loss: 2.925176]\n",
      "[Epoch 14/200] [Batch 45/46] [D loss: -1.255089] [G loss: 2.270091]\n",
      "[Epoch 15/200] [Batch 0/46] [D loss: -2.491445] [G loss: 0.954108]\n",
      "[Epoch 15/200] [Batch 5/46] [D loss: -2.337235] [G loss: 1.498777]\n",
      "[Epoch 15/200] [Batch 10/46] [D loss: -2.194004] [G loss: 1.104151]\n",
      "[Epoch 15/200] [Batch 15/46] [D loss: -3.070922] [G loss: -1.354277]\n",
      "[Epoch 15/200] [Batch 20/46] [D loss: -2.813741] [G loss: -4.943549]\n",
      "[Epoch 15/200] [Batch 25/46] [D loss: -2.471272] [G loss: -5.791931]\n",
      "[Epoch 15/200] [Batch 30/46] [D loss: -3.272186] [G loss: -7.149511]\n",
      "[Epoch 15/200] [Batch 35/46] [D loss: -1.543109] [G loss: -4.169796]\n",
      "[Epoch 15/200] [Batch 40/46] [D loss: -2.646078] [G loss: -1.134348]\n",
      "[Epoch 15/200] [Batch 45/46] [D loss: -1.470064] [G loss: -3.516891]\n",
      "[Epoch 16/200] [Batch 0/46] [D loss: -2.796318] [G loss: -4.153791]\n",
      "[Epoch 16/200] [Batch 5/46] [D loss: -3.316046] [G loss: 0.108621]\n",
      "[Epoch 16/200] [Batch 10/46] [D loss: -2.307123] [G loss: -0.896249]\n",
      "[Epoch 16/200] [Batch 15/46] [D loss: -2.823021] [G loss: -2.077106]\n",
      "[Epoch 16/200] [Batch 20/46] [D loss: -2.541981] [G loss: 1.079319]\n",
      "[Epoch 16/200] [Batch 25/46] [D loss: -1.233477] [G loss: 1.898732]\n",
      "[Epoch 16/200] [Batch 30/46] [D loss: -2.070616] [G loss: -1.106578]\n",
      "[Epoch 16/200] [Batch 35/46] [D loss: -2.232126] [G loss: -0.176867]\n",
      "[Epoch 16/200] [Batch 40/46] [D loss: -4.035953] [G loss: -0.544899]\n",
      "[Epoch 16/200] [Batch 45/46] [D loss: -2.737420] [G loss: 2.060366]\n",
      "[Epoch 17/200] [Batch 0/46] [D loss: -1.840743] [G loss: 1.795667]\n",
      "[Epoch 17/200] [Batch 5/46] [D loss: 0.113684] [G loss: 3.556087]\n",
      "[Epoch 17/200] [Batch 10/46] [D loss: -2.917050] [G loss: 1.078580]\n",
      "[Epoch 17/200] [Batch 15/46] [D loss: -2.959248] [G loss: -0.796019]\n",
      "[Epoch 17/200] [Batch 20/46] [D loss: -3.366151] [G loss: -1.515477]\n",
      "[Epoch 17/200] [Batch 25/46] [D loss: -1.193920] [G loss: -1.885199]\n",
      "[Epoch 17/200] [Batch 30/46] [D loss: -2.249824] [G loss: -3.393030]\n",
      "[Epoch 17/200] [Batch 35/46] [D loss: -1.518280] [G loss: -3.627600]\n",
      "[Epoch 17/200] [Batch 40/46] [D loss: -2.488944] [G loss: -4.923832]\n",
      "[Epoch 17/200] [Batch 45/46] [D loss: -4.246650] [G loss: -0.995471]\n",
      "[Epoch 18/200] [Batch 0/46] [D loss: -2.182018] [G loss: -1.342475]\n",
      "[Epoch 18/200] [Batch 5/46] [D loss: -1.170321] [G loss: 1.494460]\n",
      "[Epoch 18/200] [Batch 10/46] [D loss: -1.418568] [G loss: 3.487847]\n",
      "[Epoch 18/200] [Batch 15/46] [D loss: -2.727171] [G loss: 0.600436]\n",
      "[Epoch 18/200] [Batch 20/46] [D loss: -4.388738] [G loss: -0.215086]\n",
      "[Epoch 18/200] [Batch 25/46] [D loss: -2.056669] [G loss: -1.866348]\n",
      "[Epoch 18/200] [Batch 30/46] [D loss: -2.746811] [G loss: 1.715620]\n",
      "[Epoch 18/200] [Batch 35/46] [D loss: -3.430703] [G loss: -1.758475]\n",
      "[Epoch 18/200] [Batch 40/46] [D loss: -0.828238] [G loss: -5.356743]\n",
      "[Epoch 18/200] [Batch 45/46] [D loss: -3.032167] [G loss: -6.853568]\n",
      "[Epoch 19/200] [Batch 0/46] [D loss: 0.053278] [G loss: -7.128931]\n",
      "[Epoch 19/200] [Batch 5/46] [D loss: -1.995395] [G loss: -6.371029]\n",
      "[Epoch 19/200] [Batch 10/46] [D loss: -2.563024] [G loss: -9.081599]\n",
      "[Epoch 19/200] [Batch 15/46] [D loss: -3.133830] [G loss: -6.460489]\n",
      "[Epoch 19/200] [Batch 20/46] [D loss: -3.242754] [G loss: -2.644901]\n",
      "[Epoch 19/200] [Batch 25/46] [D loss: -0.975901] [G loss: 1.035214]\n",
      "[Epoch 19/200] [Batch 30/46] [D loss: -1.680267] [G loss: 7.453086]\n",
      "[Epoch 19/200] [Batch 35/46] [D loss: -1.260269] [G loss: 7.566076]\n",
      "[Epoch 19/200] [Batch 40/46] [D loss: -1.512921] [G loss: 6.774806]\n",
      "[Epoch 19/200] [Batch 45/46] [D loss: -3.041557] [G loss: 5.986860]\n",
      "[Epoch 20/200] [Batch 0/46] [D loss: -2.376833] [G loss: 5.923158]\n",
      "[Epoch 20/200] [Batch 5/46] [D loss: -2.646226] [G loss: 0.718962]\n",
      "[Epoch 20/200] [Batch 10/46] [D loss: -2.040850] [G loss: -2.627336]\n",
      "[Epoch 20/200] [Batch 15/46] [D loss: -1.521120] [G loss: -5.398029]\n",
      "[Epoch 20/200] [Batch 20/46] [D loss: -1.849723] [G loss: -9.365578]\n",
      "[Epoch 20/200] [Batch 25/46] [D loss: -1.490057] [G loss: -13.276556]\n",
      "[Epoch 20/200] [Batch 30/46] [D loss: -1.669302] [G loss: -13.986946]\n",
      "[Epoch 20/200] [Batch 35/46] [D loss: -3.173775] [G loss: -12.801270]\n",
      "[Epoch 20/200] [Batch 40/46] [D loss: -1.082842] [G loss: -10.079603]\n",
      "[Epoch 20/200] [Batch 45/46] [D loss: -2.205488] [G loss: -6.935797]\n",
      "[Epoch 21/200] [Batch 0/46] [D loss: -0.805152] [G loss: -4.713117]\n",
      "[Epoch 21/200] [Batch 5/46] [D loss: -1.938202] [G loss: -0.299993]\n",
      "[Epoch 21/200] [Batch 10/46] [D loss: -0.702360] [G loss: 2.068749]\n",
      "[Epoch 21/200] [Batch 15/46] [D loss: -2.173188] [G loss: 2.928508]\n",
      "[Epoch 21/200] [Batch 20/46] [D loss: -1.529361] [G loss: 4.527774]\n",
      "[Epoch 21/200] [Batch 25/46] [D loss: -2.504873] [G loss: 5.261776]\n",
      "[Epoch 21/200] [Batch 30/46] [D loss: -1.864115] [G loss: 5.883183]\n",
      "[Epoch 21/200] [Batch 35/46] [D loss: -1.710922] [G loss: 5.102600]\n",
      "[Epoch 21/200] [Batch 40/46] [D loss: -0.446788] [G loss: 1.046701]\n",
      "[Epoch 21/200] [Batch 45/46] [D loss: -4.211583] [G loss: -2.687411]\n",
      "[Epoch 22/200] [Batch 0/46] [D loss: -1.479540] [G loss: -3.983069]\n",
      "[Epoch 22/200] [Batch 5/46] [D loss: -2.998502] [G loss: -9.320169]\n",
      "[Epoch 22/200] [Batch 10/46] [D loss: -4.506952] [G loss: -11.290535]\n",
      "[Epoch 22/200] [Batch 15/46] [D loss: -2.963468] [G loss: -12.169086]\n",
      "[Epoch 22/200] [Batch 20/46] [D loss: -3.164747] [G loss: -9.976196]\n",
      "[Epoch 22/200] [Batch 25/46] [D loss: -1.115854] [G loss: -6.588424]\n",
      "[Epoch 22/200] [Batch 30/46] [D loss: -1.732283] [G loss: -5.474787]\n",
      "[Epoch 22/200] [Batch 35/46] [D loss: -3.516193] [G loss: -3.014667]\n",
      "[Epoch 22/200] [Batch 40/46] [D loss: -1.507996] [G loss: 2.680355]\n",
      "[Epoch 22/200] [Batch 45/46] [D loss: -2.189156] [G loss: 2.346665]\n",
      "[Epoch 23/200] [Batch 0/46] [D loss: -2.298914] [G loss: 2.200202]\n",
      "[Epoch 23/200] [Batch 5/46] [D loss: -3.731717] [G loss: 3.093174]\n",
      "[Epoch 23/200] [Batch 10/46] [D loss: -1.963483] [G loss: 5.783506]\n",
      "[Epoch 23/200] [Batch 15/46] [D loss: -2.533432] [G loss: 4.135724]\n",
      "[Epoch 23/200] [Batch 20/46] [D loss: -2.017370] [G loss: 2.613696]\n",
      "[Epoch 23/200] [Batch 25/46] [D loss: -1.768694] [G loss: 4.009598]\n",
      "[Epoch 23/200] [Batch 30/46] [D loss: -2.617401] [G loss: 1.471174]\n",
      "[Epoch 23/200] [Batch 35/46] [D loss: -1.930419] [G loss: -2.561516]\n",
      "[Epoch 23/200] [Batch 40/46] [D loss: -2.839499] [G loss: -6.065474]\n",
      "[Epoch 23/200] [Batch 45/46] [D loss: -1.381552] [G loss: -8.610800]\n",
      "[Epoch 24/200] [Batch 0/46] [D loss: -0.634054] [G loss: -9.378587]\n",
      "[Epoch 24/200] [Batch 5/46] [D loss: -1.532647] [G loss: -9.127848]\n",
      "[Epoch 24/200] [Batch 10/46] [D loss: -0.953430] [G loss: -8.950254]\n",
      "[Epoch 24/200] [Batch 15/46] [D loss: -2.088093] [G loss: -6.697590]\n",
      "[Epoch 24/200] [Batch 20/46] [D loss: -1.619895] [G loss: -1.743226]\n",
      "[Epoch 24/200] [Batch 25/46] [D loss: -3.020332] [G loss: 0.170927]\n",
      "[Epoch 24/200] [Batch 30/46] [D loss: -1.858087] [G loss: -0.215064]\n",
      "[Epoch 24/200] [Batch 35/46] [D loss: -2.975312] [G loss: 1.189061]\n",
      "[Epoch 24/200] [Batch 40/46] [D loss: -1.819392] [G loss: 2.768731]\n",
      "[Epoch 24/200] [Batch 45/46] [D loss: -2.711133] [G loss: 3.494111]\n",
      "[Epoch 25/200] [Batch 0/46] [D loss: 0.106206] [G loss: 3.434172]\n",
      "[Epoch 25/200] [Batch 5/46] [D loss: -1.827088] [G loss: 4.077740]\n",
      "[Epoch 25/200] [Batch 10/46] [D loss: -2.112686] [G loss: 2.809283]\n",
      "[Epoch 25/200] [Batch 15/46] [D loss: -1.520896] [G loss: 4.918177]\n",
      "[Epoch 25/200] [Batch 20/46] [D loss: -2.107827] [G loss: 4.149579]\n",
      "[Epoch 25/200] [Batch 25/46] [D loss: -3.493348] [G loss: 0.989056]\n",
      "[Epoch 25/200] [Batch 30/46] [D loss: -1.189804] [G loss: -4.115515]\n",
      "[Epoch 25/200] [Batch 35/46] [D loss: -0.688072] [G loss: -7.361474]\n",
      "[Epoch 25/200] [Batch 40/46] [D loss: -1.395505] [G loss: -13.235130]\n",
      "[Epoch 25/200] [Batch 45/46] [D loss: -2.704445] [G loss: -13.263142]\n",
      "[Epoch 26/200] [Batch 0/46] [D loss: -0.420253] [G loss: -12.960054]\n",
      "[Epoch 26/200] [Batch 5/46] [D loss: -2.100312] [G loss: -10.356403]\n",
      "[Epoch 26/200] [Batch 10/46] [D loss: -0.904754] [G loss: -4.156014]\n",
      "[Epoch 26/200] [Batch 15/46] [D loss: -2.200942] [G loss: 0.012328]\n",
      "[Epoch 26/200] [Batch 20/46] [D loss: -2.445714] [G loss: 2.739723]\n",
      "[Epoch 26/200] [Batch 25/46] [D loss: -1.758497] [G loss: 5.937004]\n",
      "[Epoch 26/200] [Batch 30/46] [D loss: -0.327604] [G loss: 3.347920]\n",
      "[Epoch 26/200] [Batch 35/46] [D loss: -2.198144] [G loss: 0.671459]\n",
      "[Epoch 26/200] [Batch 40/46] [D loss: -2.197079] [G loss: -0.427445]\n",
      "[Epoch 26/200] [Batch 45/46] [D loss: -3.890717] [G loss: -0.827551]\n",
      "[Epoch 27/200] [Batch 0/46] [D loss: -1.034443] [G loss: -0.325620]\n",
      "[Epoch 27/200] [Batch 5/46] [D loss: -2.907807] [G loss: -2.713787]\n",
      "[Epoch 27/200] [Batch 10/46] [D loss: -2.359672] [G loss: -3.416880]\n",
      "[Epoch 27/200] [Batch 15/46] [D loss: -1.799828] [G loss: -4.935565]\n",
      "[Epoch 27/200] [Batch 20/46] [D loss: -1.327644] [G loss: -6.576683]\n",
      "[Epoch 27/200] [Batch 25/46] [D loss: -1.377125] [G loss: -9.202688]\n",
      "[Epoch 27/200] [Batch 30/46] [D loss: -0.560330] [G loss: -9.181783]\n",
      "[Epoch 27/200] [Batch 35/46] [D loss: -1.093744] [G loss: -8.937982]\n",
      "[Epoch 27/200] [Batch 40/46] [D loss: -2.641568] [G loss: -8.536842]\n",
      "[Epoch 27/200] [Batch 45/46] [D loss: -2.505941] [G loss: -7.961413]\n",
      "[Epoch 28/200] [Batch 0/46] [D loss: -2.621063] [G loss: -8.093861]\n",
      "[Epoch 28/200] [Batch 5/46] [D loss: -0.895956] [G loss: -4.945532]\n",
      "[Epoch 28/200] [Batch 10/46] [D loss: -3.440288] [G loss: -2.937255]\n",
      "[Epoch 28/200] [Batch 15/46] [D loss: -2.266264] [G loss: 2.306918]\n",
      "[Epoch 28/200] [Batch 20/46] [D loss: -2.630044] [G loss: 5.004564]\n",
      "[Epoch 28/200] [Batch 25/46] [D loss: -1.262515] [G loss: 7.646386]\n",
      "[Epoch 28/200] [Batch 30/46] [D loss: -0.981020] [G loss: 4.684333]\n",
      "[Epoch 28/200] [Batch 35/46] [D loss: -1.546933] [G loss: 4.022053]\n",
      "[Epoch 28/200] [Batch 40/46] [D loss: 1.318878] [G loss: 4.017735]\n",
      "[Epoch 28/200] [Batch 45/46] [D loss: -1.211575] [G loss: -2.320311]\n",
      "[Epoch 29/200] [Batch 0/46] [D loss: -2.117114] [G loss: -1.936663]\n",
      "[Epoch 29/200] [Batch 5/46] [D loss: -2.581077] [G loss: -7.320353]\n",
      "[Epoch 29/200] [Batch 10/46] [D loss: -1.428474] [G loss: -11.444345]\n",
      "[Epoch 29/200] [Batch 15/46] [D loss: -0.396774] [G loss: -14.522367]\n",
      "[Epoch 29/200] [Batch 20/46] [D loss: -2.038547] [G loss: -16.201214]\n",
      "[Epoch 29/200] [Batch 25/46] [D loss: -1.831284] [G loss: -16.114861]\n",
      "[Epoch 29/200] [Batch 30/46] [D loss: -1.853120] [G loss: -16.273464]\n",
      "[Epoch 29/200] [Batch 35/46] [D loss: -1.020130] [G loss: -13.543268]\n",
      "[Epoch 29/200] [Batch 40/46] [D loss: -1.188573] [G loss: -7.171227]\n",
      "[Epoch 29/200] [Batch 45/46] [D loss: -0.306007] [G loss: 0.480355]\n",
      "[Epoch 30/200] [Batch 0/46] [D loss: -1.126871] [G loss: 0.237656]\n",
      "[Epoch 30/200] [Batch 5/46] [D loss: -3.795846] [G loss: 3.993280]\n",
      "[Epoch 30/200] [Batch 10/46] [D loss: -1.142052] [G loss: 8.887451]\n",
      "[Epoch 30/200] [Batch 15/46] [D loss: -1.101291] [G loss: 6.583539]\n",
      "[Epoch 30/200] [Batch 20/46] [D loss: -0.350349] [G loss: 3.382781]\n",
      "[Epoch 30/200] [Batch 25/46] [D loss: -2.793936] [G loss: 1.271048]\n",
      "[Epoch 30/200] [Batch 30/46] [D loss: 0.409626] [G loss: 1.476133]\n",
      "[Epoch 30/200] [Batch 35/46] [D loss: -0.917280] [G loss: -2.579833]\n",
      "[Epoch 30/200] [Batch 40/46] [D loss: -2.640687] [G loss: -6.948365]\n",
      "[Epoch 30/200] [Batch 45/46] [D loss: -1.776585] [G loss: -8.880175]\n",
      "[Epoch 31/200] [Batch 0/46] [D loss: -0.445419] [G loss: -10.046915]\n",
      "[Epoch 31/200] [Batch 5/46] [D loss: -1.624742] [G loss: -11.040983]\n",
      "[Epoch 31/200] [Batch 10/46] [D loss: -3.578831] [G loss: -9.291219]\n",
      "[Epoch 31/200] [Batch 15/46] [D loss: -1.521546] [G loss: -9.557747]\n",
      "[Epoch 31/200] [Batch 20/46] [D loss: -1.348570] [G loss: -5.443899]\n",
      "[Epoch 31/200] [Batch 25/46] [D loss: -3.499049] [G loss: -2.238244]\n",
      "[Epoch 31/200] [Batch 30/46] [D loss: -1.573962] [G loss: 1.736954]\n",
      "[Epoch 31/200] [Batch 35/46] [D loss: -1.358940] [G loss: 2.248022]\n",
      "[Epoch 31/200] [Batch 40/46] [D loss: -2.746323] [G loss: 3.294212]\n",
      "[Epoch 31/200] [Batch 45/46] [D loss: -2.789652] [G loss: 4.752273]\n",
      "[Epoch 32/200] [Batch 0/46] [D loss: -2.639258] [G loss: 5.950696]\n",
      "[Epoch 32/200] [Batch 5/46] [D loss: -1.366127] [G loss: 3.635042]\n",
      "[Epoch 32/200] [Batch 10/46] [D loss: -2.073034] [G loss: 0.803094]\n",
      "[Epoch 32/200] [Batch 15/46] [D loss: -2.323031] [G loss: -2.603392]\n",
      "[Epoch 32/200] [Batch 20/46] [D loss: -2.506595] [G loss: -4.812121]\n",
      "[Epoch 32/200] [Batch 25/46] [D loss: -3.468474] [G loss: -6.972557]\n",
      "[Epoch 32/200] [Batch 30/46] [D loss: -1.891773] [G loss: -8.851368]\n",
      "[Epoch 32/200] [Batch 35/46] [D loss: -0.110495] [G loss: -9.929651]\n",
      "[Epoch 32/200] [Batch 40/46] [D loss: -1.969048] [G loss: -11.473668]\n",
      "[Epoch 32/200] [Batch 45/46] [D loss: -2.429270] [G loss: -13.106972]\n",
      "[Epoch 33/200] [Batch 0/46] [D loss: -2.152884] [G loss: -12.803066]\n",
      "[Epoch 33/200] [Batch 5/46] [D loss: -1.484896] [G loss: -9.692255]\n",
      "[Epoch 33/200] [Batch 10/46] [D loss: -1.649533] [G loss: -5.252023]\n",
      "[Epoch 33/200] [Batch 15/46] [D loss: -2.883357] [G loss: -0.268822]\n",
      "[Epoch 33/200] [Batch 20/46] [D loss: -2.424086] [G loss: 1.846575]\n",
      "[Epoch 33/200] [Batch 25/46] [D loss: -3.774805] [G loss: 6.050279]\n",
      "[Epoch 33/200] [Batch 30/46] [D loss: -3.000119] [G loss: 7.393712]\n",
      "[Epoch 33/200] [Batch 35/46] [D loss: -0.693817] [G loss: 9.156446]\n",
      "[Epoch 33/200] [Batch 40/46] [D loss: -1.465563] [G loss: 7.762684]\n",
      "[Epoch 33/200] [Batch 45/46] [D loss: 0.559537] [G loss: 6.219591]\n",
      "[Epoch 34/200] [Batch 0/46] [D loss: 0.272293] [G loss: 4.658480]\n",
      "[Epoch 34/200] [Batch 5/46] [D loss: -1.819807] [G loss: 0.620911]\n",
      "[Epoch 34/200] [Batch 10/46] [D loss: -2.986813] [G loss: -4.194208]\n",
      "[Epoch 34/200] [Batch 15/46] [D loss: -3.386938] [G loss: -8.459864]\n",
      "[Epoch 34/200] [Batch 20/46] [D loss: -2.618191] [G loss: -14.454357]\n",
      "[Epoch 34/200] [Batch 25/46] [D loss: -2.129597] [G loss: -17.001553]\n",
      "[Epoch 34/200] [Batch 30/46] [D loss: -0.943150] [G loss: -18.489388]\n",
      "[Epoch 34/200] [Batch 35/46] [D loss: -3.269914] [G loss: -17.269222]\n",
      "[Epoch 34/200] [Batch 40/46] [D loss: -0.721828] [G loss: -15.371741]\n",
      "[Epoch 34/200] [Batch 45/46] [D loss: -1.411559] [G loss: -12.127994]\n",
      "[Epoch 35/200] [Batch 0/46] [D loss: -1.255455] [G loss: -11.447395]\n",
      "[Epoch 35/200] [Batch 5/46] [D loss: -2.611757] [G loss: -5.263463]\n",
      "[Epoch 35/200] [Batch 10/46] [D loss: -2.296716] [G loss: -0.349253]\n",
      "[Epoch 35/200] [Batch 15/46] [D loss: -3.471258] [G loss: 6.442039]\n",
      "[Epoch 35/200] [Batch 20/46] [D loss: -0.953282] [G loss: 5.850530]\n",
      "[Epoch 35/200] [Batch 25/46] [D loss: -1.246033] [G loss: 7.054559]\n",
      "[Epoch 35/200] [Batch 30/46] [D loss: -0.068703] [G loss: 7.415922]\n",
      "[Epoch 35/200] [Batch 35/46] [D loss: -1.161647] [G loss: 6.215038]\n",
      "[Epoch 35/200] [Batch 40/46] [D loss: -1.897999] [G loss: 4.326914]\n",
      "[Epoch 35/200] [Batch 45/46] [D loss: -1.543737] [G loss: 1.695453]\n",
      "[Epoch 36/200] [Batch 0/46] [D loss: -0.057335] [G loss: 0.666562]\n",
      "[Epoch 36/200] [Batch 5/46] [D loss: -0.246760] [G loss: -6.616294]\n",
      "[Epoch 36/200] [Batch 10/46] [D loss: -3.044452] [G loss: -12.106053]\n",
      "[Epoch 36/200] [Batch 15/46] [D loss: -2.873910] [G loss: -16.263330]\n",
      "[Epoch 36/200] [Batch 20/46] [D loss: -2.334854] [G loss: -16.726812]\n",
      "[Epoch 36/200] [Batch 25/46] [D loss: -1.416844] [G loss: -19.431311]\n",
      "[Epoch 36/200] [Batch 30/46] [D loss: -1.136844] [G loss: -16.540976]\n",
      "[Epoch 36/200] [Batch 35/46] [D loss: -2.060477] [G loss: -13.443406]\n",
      "[Epoch 36/200] [Batch 40/46] [D loss: -0.965293] [G loss: -10.921324]\n",
      "[Epoch 36/200] [Batch 45/46] [D loss: -0.346345] [G loss: -7.651510]\n",
      "[Epoch 37/200] [Batch 0/46] [D loss: -1.503406] [G loss: -4.028953]\n",
      "[Epoch 37/200] [Batch 5/46] [D loss: -0.438867] [G loss: 2.977482]\n",
      "[Epoch 37/200] [Batch 10/46] [D loss: -3.433328] [G loss: 7.667073]\n",
      "[Epoch 37/200] [Batch 15/46] [D loss: -1.680616] [G loss: 9.205446]\n",
      "[Epoch 37/200] [Batch 20/46] [D loss: -1.569499] [G loss: 7.773867]\n",
      "[Epoch 37/200] [Batch 25/46] [D loss: -2.072206] [G loss: 7.008271]\n",
      "[Epoch 37/200] [Batch 30/46] [D loss: 0.238247] [G loss: 5.592495]\n",
      "[Epoch 37/200] [Batch 35/46] [D loss: 0.994745] [G loss: 0.016791]\n",
      "[Epoch 37/200] [Batch 40/46] [D loss: -2.122641] [G loss: -6.162297]\n",
      "[Epoch 37/200] [Batch 45/46] [D loss: -0.217249] [G loss: -7.321615]\n",
      "[Epoch 38/200] [Batch 0/46] [D loss: -2.118843] [G loss: -7.073556]\n",
      "[Epoch 38/200] [Batch 5/46] [D loss: -0.799337] [G loss: -9.361892]\n",
      "[Epoch 38/200] [Batch 10/46] [D loss: -1.039645] [G loss: -9.390763]\n",
      "[Epoch 38/200] [Batch 15/46] [D loss: -1.497217] [G loss: -6.823450]\n",
      "[Epoch 38/200] [Batch 20/46] [D loss: -3.053037] [G loss: -4.907903]\n",
      "[Epoch 38/200] [Batch 25/46] [D loss: -1.138329] [G loss: -4.144763]\n",
      "[Epoch 38/200] [Batch 30/46] [D loss: -0.944369] [G loss: -2.484021]\n",
      "[Epoch 38/200] [Batch 35/46] [D loss: -0.742553] [G loss: -3.186840]\n",
      "[Epoch 38/200] [Batch 40/46] [D loss: -2.085308] [G loss: -3.161706]\n",
      "[Epoch 38/200] [Batch 45/46] [D loss: -1.200322] [G loss: -3.283900]\n",
      "[Epoch 39/200] [Batch 0/46] [D loss: -0.306093] [G loss: -3.350377]\n",
      "[Epoch 39/200] [Batch 5/46] [D loss: -0.857634] [G loss: -2.853230]\n",
      "[Epoch 39/200] [Batch 10/46] [D loss: -1.887796] [G loss: -0.021223]\n",
      "[Epoch 39/200] [Batch 15/46] [D loss: -3.018718] [G loss: 0.709927]\n",
      "[Epoch 39/200] [Batch 20/46] [D loss: -1.249711] [G loss: 3.394354]\n",
      "[Epoch 39/200] [Batch 25/46] [D loss: -1.306714] [G loss: 3.173379]\n",
      "[Epoch 39/200] [Batch 30/46] [D loss: -1.400627] [G loss: 0.999916]\n",
      "[Epoch 39/200] [Batch 35/46] [D loss: -0.510571] [G loss: -0.822059]\n",
      "[Epoch 39/200] [Batch 40/46] [D loss: -2.029384] [G loss: -2.642326]\n",
      "[Epoch 39/200] [Batch 45/46] [D loss: -1.603047] [G loss: -2.640305]\n",
      "[Epoch 40/200] [Batch 0/46] [D loss: -2.555447] [G loss: -3.401256]\n",
      "[Epoch 40/200] [Batch 5/46] [D loss: -1.482107] [G loss: -5.923824]\n",
      "[Epoch 40/200] [Batch 10/46] [D loss: -2.276731] [G loss: -8.901027]\n",
      "[Epoch 40/200] [Batch 15/46] [D loss: -0.690478] [G loss: -9.153696]\n",
      "[Epoch 40/200] [Batch 20/46] [D loss: -3.117459] [G loss: -9.376744]\n",
      "[Epoch 40/200] [Batch 25/46] [D loss: -3.431173] [G loss: -5.243606]\n",
      "[Epoch 40/200] [Batch 30/46] [D loss: -2.796120] [G loss: -1.491068]\n",
      "[Epoch 40/200] [Batch 35/46] [D loss: -1.845348] [G loss: -0.485357]\n",
      "[Epoch 40/200] [Batch 40/46] [D loss: -1.829021] [G loss: -1.751408]\n",
      "[Epoch 40/200] [Batch 45/46] [D loss: -2.184555] [G loss: -0.189547]\n",
      "[Epoch 41/200] [Batch 0/46] [D loss: -2.448396] [G loss: -0.651463]\n",
      "[Epoch 41/200] [Batch 5/46] [D loss: -0.983305] [G loss: -2.842593]\n",
      "[Epoch 41/200] [Batch 10/46] [D loss: -2.359430] [G loss: -2.198095]\n",
      "[Epoch 41/200] [Batch 15/46] [D loss: -2.150105] [G loss: -2.543506]\n",
      "[Epoch 41/200] [Batch 20/46] [D loss: -1.441606] [G loss: -4.246858]\n",
      "[Epoch 41/200] [Batch 25/46] [D loss: -1.632188] [G loss: -4.601274]\n",
      "[Epoch 41/200] [Batch 30/46] [D loss: -2.222254] [G loss: -5.850148]\n",
      "[Epoch 41/200] [Batch 35/46] [D loss: 0.027896] [G loss: -6.779236]\n",
      "[Epoch 41/200] [Batch 40/46] [D loss: -0.351973] [G loss: -7.817073]\n",
      "[Epoch 41/200] [Batch 45/46] [D loss: -0.814440] [G loss: -10.652235]\n",
      "[Epoch 42/200] [Batch 0/46] [D loss: -2.544740] [G loss: -9.631579]\n",
      "[Epoch 42/200] [Batch 5/46] [D loss: -1.468773] [G loss: -7.105888]\n",
      "[Epoch 42/200] [Batch 10/46] [D loss: -2.481427] [G loss: -6.342633]\n",
      "[Epoch 42/200] [Batch 15/46] [D loss: -1.733029] [G loss: -4.706544]\n",
      "[Epoch 42/200] [Batch 20/46] [D loss: -0.495504] [G loss: -3.431426]\n",
      "[Epoch 42/200] [Batch 25/46] [D loss: -3.282394] [G loss: -2.107696]\n",
      "[Epoch 42/200] [Batch 30/46] [D loss: -0.351495] [G loss: -0.275224]\n",
      "[Epoch 42/200] [Batch 35/46] [D loss: -2.274081] [G loss: -1.021304]\n",
      "[Epoch 42/200] [Batch 40/46] [D loss: -1.914287] [G loss: -0.409403]\n",
      "[Epoch 42/200] [Batch 45/46] [D loss: -1.145684] [G loss: -1.758045]\n",
      "[Epoch 43/200] [Batch 0/46] [D loss: -1.612506] [G loss: -3.760251]\n",
      "[Epoch 43/200] [Batch 5/46] [D loss: 0.475508] [G loss: -3.268353]\n",
      "[Epoch 43/200] [Batch 10/46] [D loss: -0.340297] [G loss: -5.259619]\n",
      "[Epoch 43/200] [Batch 15/46] [D loss: -1.505194] [G loss: -5.078694]\n",
      "[Epoch 43/200] [Batch 20/46] [D loss: 0.065819] [G loss: -5.553270]\n",
      "[Epoch 43/200] [Batch 25/46] [D loss: -1.694761] [G loss: -7.195577]\n",
      "[Epoch 43/200] [Batch 30/46] [D loss: -2.099206] [G loss: -6.189742]\n",
      "[Epoch 43/200] [Batch 35/46] [D loss: -1.218601] [G loss: -8.092813]\n",
      "[Epoch 43/200] [Batch 40/46] [D loss: -2.305483] [G loss: -9.442342]\n",
      "[Epoch 43/200] [Batch 45/46] [D loss: -0.483759] [G loss: -10.734688]\n",
      "[Epoch 44/200] [Batch 0/46] [D loss: -2.087553] [G loss: -8.832273]\n",
      "[Epoch 44/200] [Batch 5/46] [D loss: -1.175866] [G loss: -8.181773]\n",
      "[Epoch 44/200] [Batch 10/46] [D loss: -0.912777] [G loss: -6.411465]\n",
      "[Epoch 44/200] [Batch 15/46] [D loss: -1.129910] [G loss: -1.318152]\n",
      "[Epoch 44/200] [Batch 20/46] [D loss: -2.360941] [G loss: 0.626984]\n",
      "[Epoch 44/200] [Batch 25/46] [D loss: -2.120605] [G loss: 2.374136]\n",
      "[Epoch 44/200] [Batch 30/46] [D loss: -2.048964] [G loss: 0.320926]\n",
      "[Epoch 44/200] [Batch 35/46] [D loss: -2.097898] [G loss: 0.461219]\n",
      "[Epoch 44/200] [Batch 40/46] [D loss: -2.370461] [G loss: -0.458254]\n",
      "[Epoch 44/200] [Batch 45/46] [D loss: -0.845246] [G loss: -3.577932]\n",
      "[Epoch 45/200] [Batch 0/46] [D loss: -1.315178] [G loss: -4.021486]\n",
      "[Epoch 45/200] [Batch 5/46] [D loss: -2.419322] [G loss: -3.588806]\n",
      "[Epoch 45/200] [Batch 10/46] [D loss: -1.300940] [G loss: -3.287867]\n",
      "[Epoch 45/200] [Batch 15/46] [D loss: -1.459807] [G loss: -4.107037]\n",
      "[Epoch 45/200] [Batch 20/46] [D loss: -1.390337] [G loss: -1.142095]\n",
      "[Epoch 45/200] [Batch 25/46] [D loss: -1.408319] [G loss: -1.727329]\n",
      "[Epoch 45/200] [Batch 30/46] [D loss: -1.296106] [G loss: -2.877706]\n",
      "[Epoch 45/200] [Batch 35/46] [D loss: -1.278161] [G loss: -1.993906]\n",
      "[Epoch 45/200] [Batch 40/46] [D loss: -1.141666] [G loss: -2.657133]\n",
      "[Epoch 45/200] [Batch 45/46] [D loss: -1.577993] [G loss: -4.616413]\n",
      "[Epoch 46/200] [Batch 0/46] [D loss: -2.485631] [G loss: -4.092312]\n",
      "[Epoch 46/200] [Batch 5/46] [D loss: -1.493426] [G loss: -4.262564]\n",
      "[Epoch 46/200] [Batch 10/46] [D loss: -1.420781] [G loss: -7.406451]\n",
      "[Epoch 46/200] [Batch 15/46] [D loss: -2.348676] [G loss: -6.964364]\n",
      "[Epoch 46/200] [Batch 20/46] [D loss: -0.285361] [G loss: -4.405403]\n",
      "[Epoch 46/200] [Batch 25/46] [D loss: -1.007155] [G loss: -1.782179]\n",
      "[Epoch 46/200] [Batch 30/46] [D loss: -1.988553] [G loss: -2.969134]\n",
      "[Epoch 46/200] [Batch 35/46] [D loss: -1.945297] [G loss: -5.189523]\n",
      "[Epoch 46/200] [Batch 40/46] [D loss: -1.173781] [G loss: -4.472280]\n",
      "[Epoch 46/200] [Batch 45/46] [D loss: -0.929278] [G loss: -6.103778]\n",
      "[Epoch 47/200] [Batch 0/46] [D loss: -2.093823] [G loss: -5.278370]\n",
      "[Epoch 47/200] [Batch 5/46] [D loss: -0.874455] [G loss: -7.433132]\n",
      "[Epoch 47/200] [Batch 10/46] [D loss: -1.731686] [G loss: -4.681704]\n",
      "[Epoch 47/200] [Batch 15/46] [D loss: -1.874962] [G loss: -3.928308]\n",
      "[Epoch 47/200] [Batch 20/46] [D loss: -0.593430] [G loss: -2.722468]\n",
      "[Epoch 47/200] [Batch 25/46] [D loss: -1.482977] [G loss: -4.718551]\n",
      "[Epoch 47/200] [Batch 30/46] [D loss: -1.285904] [G loss: -3.348887]\n",
      "[Epoch 47/200] [Batch 35/46] [D loss: -1.216145] [G loss: -2.461807]\n",
      "[Epoch 47/200] [Batch 40/46] [D loss: -1.260205] [G loss: -0.842019]\n",
      "[Epoch 47/200] [Batch 45/46] [D loss: -2.035278] [G loss: -2.337746]\n",
      "[Epoch 48/200] [Batch 0/46] [D loss: -1.774839] [G loss: -2.457002]\n",
      "[Epoch 48/200] [Batch 5/46] [D loss: -1.930664] [G loss: -2.974597]\n",
      "[Epoch 48/200] [Batch 10/46] [D loss: -1.400569] [G loss: -2.157185]\n",
      "[Epoch 48/200] [Batch 15/46] [D loss: -1.654131] [G loss: -1.415177]\n",
      "[Epoch 48/200] [Batch 20/46] [D loss: -2.366305] [G loss: 0.649417]\n",
      "[Epoch 48/200] [Batch 25/46] [D loss: -1.776706] [G loss: 0.471692]\n",
      "[Epoch 48/200] [Batch 30/46] [D loss: -1.627185] [G loss: -2.033526]\n",
      "[Epoch 48/200] [Batch 35/46] [D loss: -0.859045] [G loss: -1.355283]\n",
      "[Epoch 48/200] [Batch 40/46] [D loss: -1.448366] [G loss: 0.279196]\n",
      "[Epoch 48/200] [Batch 45/46] [D loss: -2.815821] [G loss: -2.790497]\n",
      "[Epoch 49/200] [Batch 0/46] [D loss: -0.458726] [G loss: -2.334572]\n",
      "[Epoch 49/200] [Batch 5/46] [D loss: -1.737230] [G loss: -4.356840]\n",
      "[Epoch 49/200] [Batch 10/46] [D loss: -0.092943] [G loss: -5.021110]\n",
      "[Epoch 49/200] [Batch 15/46] [D loss: -3.446555] [G loss: -3.635677]\n",
      "[Epoch 49/200] [Batch 20/46] [D loss: -1.913488] [G loss: -3.539839]\n",
      "[Epoch 49/200] [Batch 25/46] [D loss: -1.381290] [G loss: -5.720543]\n",
      "[Epoch 49/200] [Batch 30/46] [D loss: -1.794066] [G loss: -7.158929]\n",
      "[Epoch 49/200] [Batch 35/46] [D loss: -3.222626] [G loss: -10.727970]\n",
      "[Epoch 49/200] [Batch 40/46] [D loss: -0.784515] [G loss: -10.105583]\n",
      "[Epoch 49/200] [Batch 45/46] [D loss: -1.385549] [G loss: -9.831347]\n",
      "[Epoch 50/200] [Batch 0/46] [D loss: -1.718950] [G loss: -9.638288]\n",
      "[Epoch 50/200] [Batch 5/46] [D loss: -2.240938] [G loss: -7.395583]\n",
      "[Epoch 50/200] [Batch 10/46] [D loss: -2.076342] [G loss: -8.369215]\n",
      "[Epoch 50/200] [Batch 15/46] [D loss: -0.137800] [G loss: -3.971452]\n",
      "[Epoch 50/200] [Batch 20/46] [D loss: -1.594397] [G loss: -1.137310]\n",
      "[Epoch 50/200] [Batch 25/46] [D loss: -2.760776] [G loss: 0.754550]\n",
      "[Epoch 50/200] [Batch 30/46] [D loss: -1.150000] [G loss: 3.145496]\n",
      "[Epoch 50/200] [Batch 35/46] [D loss: -1.287905] [G loss: 3.720042]\n",
      "[Epoch 50/200] [Batch 40/46] [D loss: -3.184826] [G loss: 0.308221]\n",
      "[Epoch 50/200] [Batch 45/46] [D loss: -2.992592] [G loss: -2.371244]\n",
      "[Epoch 51/200] [Batch 0/46] [D loss: -3.291735] [G loss: -3.265975]\n",
      "[Epoch 51/200] [Batch 5/46] [D loss: -1.509791] [G loss: -5.310652]\n",
      "[Epoch 51/200] [Batch 10/46] [D loss: -2.678317] [G loss: -6.416528]\n",
      "[Epoch 51/200] [Batch 15/46] [D loss: -1.297084] [G loss: -8.960201]\n",
      "[Epoch 51/200] [Batch 20/46] [D loss: -3.253442] [G loss: -10.863933]\n",
      "[Epoch 51/200] [Batch 25/46] [D loss: -0.805116] [G loss: -12.980270]\n",
      "[Epoch 51/200] [Batch 30/46] [D loss: -4.312841] [G loss: -12.889686]\n",
      "[Epoch 51/200] [Batch 35/46] [D loss: -3.818048] [G loss: -9.953249]\n",
      "[Epoch 51/200] [Batch 40/46] [D loss: -0.027514] [G loss: -4.165865]\n",
      "[Epoch 51/200] [Batch 45/46] [D loss: -1.924250] [G loss: -1.845585]\n",
      "[Epoch 52/200] [Batch 0/46] [D loss: -1.177974] [G loss: -0.695042]\n",
      "[Epoch 52/200] [Batch 5/46] [D loss: -2.671721] [G loss: 2.522594]\n",
      "[Epoch 52/200] [Batch 10/46] [D loss: -3.416366] [G loss: 4.701184]\n",
      "[Epoch 52/200] [Batch 15/46] [D loss: -2.925256] [G loss: 5.805377]\n",
      "[Epoch 52/200] [Batch 20/46] [D loss: -1.993608] [G loss: 3.629125]\n",
      "[Epoch 52/200] [Batch 25/46] [D loss: -3.763682] [G loss: 3.543755]\n",
      "[Epoch 52/200] [Batch 30/46] [D loss: -1.267805] [G loss: -1.350082]\n",
      "[Epoch 52/200] [Batch 35/46] [D loss: -1.378872] [G loss: -6.329595]\n",
      "[Epoch 52/200] [Batch 40/46] [D loss: -0.558994] [G loss: -7.801269]\n",
      "[Epoch 52/200] [Batch 45/46] [D loss: -3.191966] [G loss: -9.581118]\n",
      "[Epoch 53/200] [Batch 0/46] [D loss: -0.924082] [G loss: -7.611694]\n",
      "[Epoch 53/200] [Batch 5/46] [D loss: 0.735957] [G loss: -11.880615]\n",
      "[Epoch 53/200] [Batch 10/46] [D loss: -3.240246] [G loss: -10.670060]\n",
      "[Epoch 53/200] [Batch 15/46] [D loss: -1.422708] [G loss: -9.511144]\n",
      "[Epoch 53/200] [Batch 20/46] [D loss: -0.785477] [G loss: -4.572119]\n",
      "[Epoch 53/200] [Batch 25/46] [D loss: -0.594814] [G loss: 2.289678]\n",
      "[Epoch 53/200] [Batch 30/46] [D loss: -0.753722] [G loss: 4.097105]\n",
      "[Epoch 53/200] [Batch 35/46] [D loss: -2.637421] [G loss: 6.088430]\n",
      "[Epoch 53/200] [Batch 40/46] [D loss: -2.304895] [G loss: 8.266134]\n",
      "[Epoch 53/200] [Batch 45/46] [D loss: -1.379222] [G loss: 5.022165]\n",
      "[Epoch 54/200] [Batch 0/46] [D loss: -0.818943] [G loss: 3.835464]\n",
      "[Epoch 54/200] [Batch 5/46] [D loss: -1.123466] [G loss: 0.059625]\n",
      "[Epoch 54/200] [Batch 10/46] [D loss: -2.176085] [G loss: -6.076465]\n",
      "[Epoch 54/200] [Batch 15/46] [D loss: -1.250169] [G loss: -9.391430]\n",
      "[Epoch 54/200] [Batch 20/46] [D loss: -1.437469] [G loss: -12.361073]\n",
      "[Epoch 54/200] [Batch 25/46] [D loss: -2.597110] [G loss: -13.647600]\n",
      "[Epoch 54/200] [Batch 30/46] [D loss: -2.109516] [G loss: -14.150932]\n",
      "[Epoch 54/200] [Batch 35/46] [D loss: -1.364705] [G loss: -12.390836]\n",
      "[Epoch 54/200] [Batch 40/46] [D loss: -1.166859] [G loss: -8.015296]\n",
      "[Epoch 54/200] [Batch 45/46] [D loss: -1.526572] [G loss: -5.506878]\n",
      "[Epoch 55/200] [Batch 0/46] [D loss: -1.180545] [G loss: -3.522683]\n",
      "[Epoch 55/200] [Batch 5/46] [D loss: -0.691801] [G loss: 0.504411]\n",
      "[Epoch 55/200] [Batch 10/46] [D loss: -3.516819] [G loss: 5.578718]\n",
      "[Epoch 55/200] [Batch 15/46] [D loss: -1.582110] [G loss: 7.693626]\n",
      "[Epoch 55/200] [Batch 20/46] [D loss: -1.223378] [G loss: 9.421432]\n",
      "[Epoch 55/200] [Batch 25/46] [D loss: -0.327326] [G loss: 7.705167]\n",
      "[Epoch 55/200] [Batch 30/46] [D loss: 0.484042] [G loss: 2.729210]\n",
      "[Epoch 55/200] [Batch 35/46] [D loss: -1.363176] [G loss: -1.957593]\n",
      "[Epoch 55/200] [Batch 40/46] [D loss: -0.519272] [G loss: -6.845998]\n",
      "[Epoch 55/200] [Batch 45/46] [D loss: -1.193216] [G loss: -11.299614]\n",
      "[Epoch 56/200] [Batch 0/46] [D loss: -1.976325] [G loss: -12.570411]\n",
      "[Epoch 56/200] [Batch 5/46] [D loss: -1.486375] [G loss: -14.002352]\n",
      "[Epoch 56/200] [Batch 10/46] [D loss: -2.038521] [G loss: -16.294102]\n",
      "[Epoch 56/200] [Batch 15/46] [D loss: -1.397899] [G loss: -17.825586]\n",
      "[Epoch 56/200] [Batch 20/46] [D loss: 0.305774] [G loss: -17.568287]\n",
      "[Epoch 56/200] [Batch 25/46] [D loss: -1.104252] [G loss: -13.532140]\n",
      "[Epoch 56/200] [Batch 30/46] [D loss: -0.869000] [G loss: -6.661931]\n",
      "[Epoch 56/200] [Batch 35/46] [D loss: -2.365105] [G loss: -1.394585]\n",
      "[Epoch 56/200] [Batch 40/46] [D loss: -0.156246] [G loss: 4.588311]\n",
      "[Epoch 56/200] [Batch 45/46] [D loss: -3.758636] [G loss: 7.514863]\n",
      "[Epoch 57/200] [Batch 0/46] [D loss: -2.091876] [G loss: 6.198829]\n",
      "[Epoch 57/200] [Batch 5/46] [D loss: -2.005261] [G loss: 7.630525]\n",
      "[Epoch 57/200] [Batch 10/46] [D loss: 0.460097] [G loss: 1.301696]\n",
      "[Epoch 57/200] [Batch 15/46] [D loss: -1.075891] [G loss: -1.758610]\n",
      "[Epoch 57/200] [Batch 20/46] [D loss: -1.989467] [G loss: -5.060014]\n",
      "[Epoch 57/200] [Batch 25/46] [D loss: -0.783573] [G loss: -8.622871]\n",
      "[Epoch 57/200] [Batch 30/46] [D loss: -2.386403] [G loss: -11.131867]\n",
      "[Epoch 57/200] [Batch 35/46] [D loss: -2.521785] [G loss: -11.718460]\n",
      "[Epoch 57/200] [Batch 40/46] [D loss: -1.549527] [G loss: -11.679832]\n",
      "[Epoch 57/200] [Batch 45/46] [D loss: -1.391506] [G loss: -11.208866]\n",
      "[Epoch 58/200] [Batch 0/46] [D loss: -0.609869] [G loss: -10.782780]\n",
      "[Epoch 58/200] [Batch 5/46] [D loss: -1.392926] [G loss: -6.428312]\n",
      "[Epoch 58/200] [Batch 10/46] [D loss: -2.497993] [G loss: -1.438409]\n",
      "[Epoch 58/200] [Batch 15/46] [D loss: -1.183733] [G loss: 3.114765]\n",
      "[Epoch 58/200] [Batch 20/46] [D loss: -1.298739] [G loss: 7.356847]\n",
      "[Epoch 58/200] [Batch 25/46] [D loss: -2.649030] [G loss: 7.886406]\n",
      "[Epoch 58/200] [Batch 30/46] [D loss: -0.384122] [G loss: 6.479236]\n",
      "[Epoch 58/200] [Batch 35/46] [D loss: -1.837646] [G loss: 5.995451]\n",
      "[Epoch 58/200] [Batch 40/46] [D loss: -1.513710] [G loss: 3.371949]\n",
      "[Epoch 58/200] [Batch 45/46] [D loss: -2.124139] [G loss: -3.058305]\n",
      "[Epoch 59/200] [Batch 0/46] [D loss: -1.002555] [G loss: -3.169549]\n",
      "[Epoch 59/200] [Batch 5/46] [D loss: -1.925856] [G loss: -6.312713]\n",
      "[Epoch 59/200] [Batch 10/46] [D loss: -1.725041] [G loss: -9.618805]\n",
      "[Epoch 59/200] [Batch 15/46] [D loss: -1.650390] [G loss: -11.944854]\n",
      "[Epoch 59/200] [Batch 20/46] [D loss: -1.243198] [G loss: -12.201749]\n",
      "[Epoch 59/200] [Batch 25/46] [D loss: -1.344720] [G loss: -13.059572]\n",
      "[Epoch 59/200] [Batch 30/46] [D loss: -0.374078] [G loss: -7.379358]\n",
      "[Epoch 59/200] [Batch 35/46] [D loss: -0.845286] [G loss: -2.668597]\n",
      "[Epoch 59/200] [Batch 40/46] [D loss: -2.263394] [G loss: 3.039438]\n",
      "[Epoch 59/200] [Batch 45/46] [D loss: -2.761557] [G loss: 4.601943]\n",
      "[Epoch 60/200] [Batch 0/46] [D loss: -2.594065] [G loss: 5.147872]\n",
      "[Epoch 60/200] [Batch 5/46] [D loss: -1.399322] [G loss: 5.200279]\n",
      "[Epoch 60/200] [Batch 10/46] [D loss: -0.993435] [G loss: 1.144479]\n",
      "[Epoch 60/200] [Batch 15/46] [D loss: -0.454485] [G loss: 0.066404]\n",
      "[Epoch 60/200] [Batch 20/46] [D loss: -1.779194] [G loss: -1.448075]\n",
      "[Epoch 60/200] [Batch 25/46] [D loss: -3.407954] [G loss: -5.048429]\n",
      "[Epoch 60/200] [Batch 30/46] [D loss: -0.571105] [G loss: -7.070981]\n",
      "[Epoch 60/200] [Batch 35/46] [D loss: -1.320040] [G loss: -8.325472]\n",
      "[Epoch 60/200] [Batch 40/46] [D loss: -0.329181] [G loss: -8.243805]\n",
      "[Epoch 60/200] [Batch 45/46] [D loss: -1.055106] [G loss: -6.800655]\n",
      "[Epoch 61/200] [Batch 0/46] [D loss: -1.256557] [G loss: -6.563855]\n",
      "[Epoch 61/200] [Batch 5/46] [D loss: 1.302220] [G loss: -4.310543]\n",
      "[Epoch 61/200] [Batch 10/46] [D loss: -2.125616] [G loss: -2.635684]\n",
      "[Epoch 61/200] [Batch 15/46] [D loss: -2.988315] [G loss: -0.136787]\n",
      "[Epoch 61/200] [Batch 20/46] [D loss: -2.351763] [G loss: -1.195667]\n",
      "[Epoch 61/200] [Batch 25/46] [D loss: -2.794517] [G loss: -2.330166]\n",
      "[Epoch 61/200] [Batch 30/46] [D loss: -1.628933] [G loss: -3.294865]\n",
      "[Epoch 61/200] [Batch 35/46] [D loss: 0.725483] [G loss: -8.299046]\n",
      "[Epoch 61/200] [Batch 40/46] [D loss: -0.678185] [G loss: -10.182653]\n",
      "[Epoch 61/200] [Batch 45/46] [D loss: -2.058012] [G loss: -9.735591]\n",
      "[Epoch 62/200] [Batch 0/46] [D loss: -2.335840] [G loss: -9.102345]\n",
      "[Epoch 62/200] [Batch 5/46] [D loss: -1.152651] [G loss: -9.575031]\n",
      "[Epoch 62/200] [Batch 10/46] [D loss: -2.068101] [G loss: -8.441774]\n",
      "[Epoch 62/200] [Batch 15/46] [D loss: -0.856275] [G loss: -6.178857]\n",
      "[Epoch 62/200] [Batch 20/46] [D loss: -0.758247] [G loss: -5.128141]\n",
      "[Epoch 62/200] [Batch 25/46] [D loss: 0.292857] [G loss: -3.282168]\n",
      "[Epoch 62/200] [Batch 30/46] [D loss: -0.687230] [G loss: 0.830747]\n",
      "[Epoch 62/200] [Batch 35/46] [D loss: -2.181047] [G loss: 4.590106]\n",
      "[Epoch 62/200] [Batch 40/46] [D loss: -2.640418] [G loss: 3.788883]\n",
      "[Epoch 62/200] [Batch 45/46] [D loss: -2.197091] [G loss: 3.935576]\n",
      "[Epoch 63/200] [Batch 0/46] [D loss: -1.760623] [G loss: 4.934669]\n",
      "[Epoch 63/200] [Batch 5/46] [D loss: -2.820320] [G loss: 3.716112]\n",
      "[Epoch 63/200] [Batch 10/46] [D loss: -0.615416] [G loss: -0.431494]\n",
      "[Epoch 63/200] [Batch 15/46] [D loss: -1.138582] [G loss: -4.255500]\n",
      "[Epoch 63/200] [Batch 20/46] [D loss: -1.411660] [G loss: -5.051064]\n",
      "[Epoch 63/200] [Batch 25/46] [D loss: -0.849159] [G loss: -6.736906]\n",
      "[Epoch 63/200] [Batch 30/46] [D loss: -2.739448] [G loss: -7.353647]\n",
      "[Epoch 63/200] [Batch 35/46] [D loss: -2.167269] [G loss: -8.448783]\n",
      "[Epoch 63/200] [Batch 40/46] [D loss: -3.066647] [G loss: -6.578510]\n",
      "[Epoch 63/200] [Batch 45/46] [D loss: -1.658907] [G loss: -6.447326]\n",
      "[Epoch 64/200] [Batch 0/46] [D loss: 0.705236] [G loss: -6.668587]\n",
      "[Epoch 64/200] [Batch 5/46] [D loss: -1.041543] [G loss: -4.099631]\n",
      "[Epoch 64/200] [Batch 10/46] [D loss: -0.979780] [G loss: 0.371423]\n",
      "[Epoch 64/200] [Batch 15/46] [D loss: -2.491222] [G loss: 3.307717]\n",
      "[Epoch 64/200] [Batch 20/46] [D loss: -2.004269] [G loss: 4.446778]\n",
      "[Epoch 64/200] [Batch 25/46] [D loss: -0.828263] [G loss: 5.337646]\n",
      "[Epoch 64/200] [Batch 30/46] [D loss: 0.502405] [G loss: 1.153502]\n",
      "[Epoch 64/200] [Batch 35/46] [D loss: 1.119524] [G loss: -4.725588]\n",
      "[Epoch 64/200] [Batch 40/46] [D loss: -2.307889] [G loss: -6.839436]\n",
      "[Epoch 64/200] [Batch 45/46] [D loss: -2.142190] [G loss: -10.459579]\n",
      "[Epoch 65/200] [Batch 0/46] [D loss: -1.951618] [G loss: -10.965055]\n",
      "[Epoch 65/200] [Batch 5/46] [D loss: -1.817546] [G loss: -12.428331]\n",
      "[Epoch 65/200] [Batch 10/46] [D loss: -1.220956] [G loss: -13.378111]\n",
      "[Epoch 65/200] [Batch 15/46] [D loss: -2.042535] [G loss: -13.448954]\n",
      "[Epoch 65/200] [Batch 20/46] [D loss: -2.241255] [G loss: -10.806278]\n",
      "[Epoch 65/200] [Batch 25/46] [D loss: -1.788770] [G loss: -7.949895]\n",
      "[Epoch 65/200] [Batch 30/46] [D loss: -0.859631] [G loss: -5.788692]\n",
      "[Epoch 65/200] [Batch 35/46] [D loss: 0.069140] [G loss: -2.460146]\n",
      "[Epoch 65/200] [Batch 40/46] [D loss: -0.953283] [G loss: 1.942425]\n",
      "[Epoch 65/200] [Batch 45/46] [D loss: -1.821192] [G loss: 3.908855]\n",
      "[Epoch 66/200] [Batch 0/46] [D loss: -3.084908] [G loss: 4.586991]\n",
      "[Epoch 66/200] [Batch 5/46] [D loss: -1.727918] [G loss: 6.386024]\n",
      "[Epoch 66/200] [Batch 10/46] [D loss: -1.713219] [G loss: 4.863867]\n",
      "[Epoch 66/200] [Batch 15/46] [D loss: 0.100458] [G loss: 2.040583]\n",
      "[Epoch 66/200] [Batch 20/46] [D loss: -1.439569] [G loss: -1.169213]\n",
      "[Epoch 66/200] [Batch 25/46] [D loss: -1.391966] [G loss: -3.666263]\n",
      "[Epoch 66/200] [Batch 30/46] [D loss: -2.548258] [G loss: -5.704321]\n",
      "[Epoch 66/200] [Batch 35/46] [D loss: -3.441391] [G loss: -7.722716]\n",
      "[Epoch 66/200] [Batch 40/46] [D loss: -1.491381] [G loss: -8.983072]\n",
      "[Epoch 66/200] [Batch 45/46] [D loss: -0.997483] [G loss: -10.819072]\n",
      "[Epoch 67/200] [Batch 0/46] [D loss: -1.003402] [G loss: -10.495816]\n",
      "[Epoch 67/200] [Batch 5/46] [D loss: -1.042306] [G loss: -10.711034]\n",
      "[Epoch 67/200] [Batch 10/46] [D loss: -1.267929] [G loss: -5.420276]\n",
      "[Epoch 67/200] [Batch 15/46] [D loss: -1.143004] [G loss: -2.833193]\n",
      "[Epoch 67/200] [Batch 20/46] [D loss: -1.785097] [G loss: 0.464669]\n",
      "[Epoch 67/200] [Batch 25/46] [D loss: -2.098764] [G loss: 3.534895]\n",
      "[Epoch 67/200] [Batch 30/46] [D loss: -0.909795] [G loss: 5.254730]\n",
      "[Epoch 67/200] [Batch 35/46] [D loss: -1.839552] [G loss: 4.031593]\n",
      "[Epoch 67/200] [Batch 40/46] [D loss: -1.628897] [G loss: 1.468162]\n",
      "[Epoch 67/200] [Batch 45/46] [D loss: -2.076870] [G loss: -0.007125]\n",
      "[Epoch 68/200] [Batch 0/46] [D loss: -2.095351] [G loss: -1.416026]\n",
      "[Epoch 68/200] [Batch 5/46] [D loss: -0.923189] [G loss: -2.046034]\n",
      "[Epoch 68/200] [Batch 10/46] [D loss: -2.002211] [G loss: -4.228164]\n",
      "[Epoch 68/200] [Batch 15/46] [D loss: -2.341378] [G loss: -4.468052]\n",
      "[Epoch 68/200] [Batch 20/46] [D loss: -1.369677] [G loss: -4.297635]\n",
      "[Epoch 68/200] [Batch 25/46] [D loss: -2.743088] [G loss: -4.076130]\n",
      "[Epoch 68/200] [Batch 30/46] [D loss: -0.538904] [G loss: -3.949210]\n",
      "[Epoch 68/200] [Batch 35/46] [D loss: -0.130403] [G loss: -4.031282]\n",
      "[Epoch 68/200] [Batch 40/46] [D loss: -1.611345] [G loss: -3.314391]\n",
      "[Epoch 68/200] [Batch 45/46] [D loss: -1.637854] [G loss: -2.895534]\n",
      "[Epoch 69/200] [Batch 0/46] [D loss: -0.893127] [G loss: -1.970920]\n",
      "[Epoch 69/200] [Batch 5/46] [D loss: -2.675530] [G loss: -1.250947]\n",
      "[Epoch 69/200] [Batch 10/46] [D loss: -0.621584] [G loss: -0.048365]\n",
      "[Epoch 69/200] [Batch 15/46] [D loss: -1.111203] [G loss: -2.633658]\n",
      "[Epoch 69/200] [Batch 20/46] [D loss: -2.058794] [G loss: -2.247846]\n",
      "[Epoch 69/200] [Batch 25/46] [D loss: -1.424601] [G loss: -4.995552]\n",
      "[Epoch 69/200] [Batch 30/46] [D loss: -0.809769] [G loss: -3.366012]\n",
      "[Epoch 69/200] [Batch 35/46] [D loss: -0.780768] [G loss: -3.144838]\n",
      "[Epoch 69/200] [Batch 40/46] [D loss: -0.981911] [G loss: -2.953973]\n",
      "[Epoch 69/200] [Batch 45/46] [D loss: -2.296866] [G loss: -2.279868]\n",
      "[Epoch 70/200] [Batch 0/46] [D loss: -0.991948] [G loss: -3.341442]\n",
      "[Epoch 70/200] [Batch 5/46] [D loss: -1.219121] [G loss: -2.420974]\n",
      "[Epoch 70/200] [Batch 10/46] [D loss: 0.287386] [G loss: -0.853944]\n",
      "[Epoch 70/200] [Batch 15/46] [D loss: -1.314288] [G loss: 3.601834]\n",
      "[Epoch 70/200] [Batch 20/46] [D loss: -1.083359] [G loss: 4.699046]\n",
      "[Epoch 70/200] [Batch 25/46] [D loss: -1.216830] [G loss: 2.118073]\n",
      "[Epoch 70/200] [Batch 30/46] [D loss: -1.114230] [G loss: 0.427718]\n",
      "[Epoch 70/200] [Batch 35/46] [D loss: -0.664036] [G loss: -1.011073]\n",
      "[Epoch 70/200] [Batch 40/46] [D loss: -1.247726] [G loss: -4.216435]\n",
      "[Epoch 70/200] [Batch 45/46] [D loss: -1.255697] [G loss: -5.547370]\n",
      "[Epoch 71/200] [Batch 0/46] [D loss: -1.330193] [G loss: -6.135157]\n",
      "[Epoch 71/200] [Batch 5/46] [D loss: -2.048615] [G loss: -7.813923]\n",
      "[Epoch 71/200] [Batch 10/46] [D loss: -0.808845] [G loss: -5.956803]\n",
      "[Epoch 71/200] [Batch 15/46] [D loss: -2.073165] [G loss: -6.978570]\n",
      "[Epoch 71/200] [Batch 20/46] [D loss: -1.124027] [G loss: -6.332841]\n",
      "[Epoch 71/200] [Batch 25/46] [D loss: -2.233244] [G loss: -5.831056]\n",
      "[Epoch 71/200] [Batch 30/46] [D loss: -1.957634] [G loss: -3.340288]\n",
      "[Epoch 71/200] [Batch 35/46] [D loss: -1.370255] [G loss: -2.370863]\n",
      "[Epoch 71/200] [Batch 40/46] [D loss: -1.651725] [G loss: 1.693799]\n",
      "[Epoch 71/200] [Batch 45/46] [D loss: -2.280780] [G loss: 3.323124]\n",
      "[Epoch 72/200] [Batch 0/46] [D loss: -2.262097] [G loss: 2.559493]\n",
      "[Epoch 72/200] [Batch 5/46] [D loss: -1.212274] [G loss: -0.032497]\n",
      "[Epoch 72/200] [Batch 10/46] [D loss: -0.301340] [G loss: -0.235891]\n",
      "[Epoch 72/200] [Batch 15/46] [D loss: -0.843526] [G loss: -2.094401]\n",
      "[Epoch 72/200] [Batch 20/46] [D loss: -1.722562] [G loss: -1.779843]\n",
      "[Epoch 72/200] [Batch 25/46] [D loss: -1.884550] [G loss: -1.565926]\n",
      "[Epoch 72/200] [Batch 30/46] [D loss: -2.230098] [G loss: -3.353760]\n",
      "[Epoch 72/200] [Batch 35/46] [D loss: -1.392898] [G loss: -5.091791]\n",
      "[Epoch 72/200] [Batch 40/46] [D loss: -1.324781] [G loss: -3.604968]\n",
      "[Epoch 72/200] [Batch 45/46] [D loss: 0.425581] [G loss: -3.055235]\n",
      "[Epoch 73/200] [Batch 0/46] [D loss: -0.886810] [G loss: -2.859701]\n",
      "[Epoch 73/200] [Batch 5/46] [D loss: -0.301129] [G loss: -2.847168]\n",
      "[Epoch 73/200] [Batch 10/46] [D loss: -1.671472] [G loss: -1.017051]\n",
      "[Epoch 73/200] [Batch 15/46] [D loss: -1.426491] [G loss: 0.401499]\n",
      "[Epoch 73/200] [Batch 20/46] [D loss: -2.657455] [G loss: -0.600083]\n",
      "[Epoch 73/200] [Batch 25/46] [D loss: -1.046994] [G loss: 0.356715]\n",
      "[Epoch 73/200] [Batch 30/46] [D loss: -1.808393] [G loss: -1.140169]\n",
      "[Epoch 73/200] [Batch 35/46] [D loss: -1.114206] [G loss: -2.921957]\n",
      "[Epoch 73/200] [Batch 40/46] [D loss: -3.844054] [G loss: -2.669537]\n",
      "[Epoch 73/200] [Batch 45/46] [D loss: -1.444546] [G loss: -3.241201]\n",
      "[Epoch 74/200] [Batch 0/46] [D loss: -1.320642] [G loss: -4.303567]\n",
      "[Epoch 74/200] [Batch 5/46] [D loss: -2.249993] [G loss: -5.444705]\n",
      "[Epoch 74/200] [Batch 10/46] [D loss: -2.116753] [G loss: -6.677002]\n",
      "[Epoch 74/200] [Batch 15/46] [D loss: -0.864291] [G loss: -9.316494]\n",
      "[Epoch 74/200] [Batch 20/46] [D loss: -2.321835] [G loss: -8.011784]\n",
      "[Epoch 74/200] [Batch 25/46] [D loss: -0.780081] [G loss: -8.279450]\n",
      "[Epoch 74/200] [Batch 30/46] [D loss: -1.220389] [G loss: -5.025625]\n",
      "[Epoch 74/200] [Batch 35/46] [D loss: -1.333694] [G loss: -1.137332]\n",
      "[Epoch 74/200] [Batch 40/46] [D loss: -2.106295] [G loss: 1.272494]\n",
      "[Epoch 74/200] [Batch 45/46] [D loss: -1.725075] [G loss: 3.575255]\n",
      "[Epoch 75/200] [Batch 0/46] [D loss: -1.718457] [G loss: 1.969110]\n",
      "[Epoch 75/200] [Batch 5/46] [D loss: -1.877954] [G loss: 1.686123]\n",
      "[Epoch 75/200] [Batch 10/46] [D loss: -0.687641] [G loss: -0.870604]\n",
      "[Epoch 75/200] [Batch 15/46] [D loss: -1.661638] [G loss: -1.839023]\n",
      "[Epoch 75/200] [Batch 20/46] [D loss: -0.712925] [G loss: -2.007214]\n",
      "[Epoch 75/200] [Batch 25/46] [D loss: -1.131292] [G loss: -3.499197]\n",
      "[Epoch 75/200] [Batch 30/46] [D loss: -2.397461] [G loss: -6.025486]\n",
      "[Epoch 75/200] [Batch 35/46] [D loss: -2.308299] [G loss: -3.734220]\n",
      "[Epoch 75/200] [Batch 40/46] [D loss: -1.420460] [G loss: -4.571190]\n",
      "[Epoch 75/200] [Batch 45/46] [D loss: -2.049341] [G loss: -4.513936]\n",
      "[Epoch 76/200] [Batch 0/46] [D loss: 0.164593] [G loss: -5.102541]\n",
      "[Epoch 76/200] [Batch 5/46] [D loss: -0.628511] [G loss: -2.806561]\n",
      "[Epoch 76/200] [Batch 10/46] [D loss: -1.228138] [G loss: -0.753041]\n",
      "[Epoch 76/200] [Batch 15/46] [D loss: -1.458583] [G loss: 1.999863]\n",
      "[Epoch 76/200] [Batch 20/46] [D loss: -2.682649] [G loss: 3.628741]\n",
      "[Epoch 76/200] [Batch 25/46] [D loss: -1.015565] [G loss: 1.894058]\n",
      "[Epoch 76/200] [Batch 30/46] [D loss: -2.041462] [G loss: -2.296671]\n",
      "[Epoch 76/200] [Batch 35/46] [D loss: -0.731985] [G loss: -5.164921]\n",
      "[Epoch 76/200] [Batch 40/46] [D loss: -1.856648] [G loss: -5.929266]\n",
      "[Epoch 76/200] [Batch 45/46] [D loss: -3.768747] [G loss: -7.122731]\n",
      "[Epoch 77/200] [Batch 0/46] [D loss: -2.625969] [G loss: -7.020301]\n",
      "[Epoch 77/200] [Batch 5/46] [D loss: -1.589064] [G loss: -6.501559]\n",
      "[Epoch 77/200] [Batch 10/46] [D loss: -2.167117] [G loss: -6.515555]\n",
      "[Epoch 77/200] [Batch 15/46] [D loss: -0.489571] [G loss: -6.951584]\n",
      "[Epoch 77/200] [Batch 20/46] [D loss: -0.695049] [G loss: -4.814758]\n",
      "[Epoch 77/200] [Batch 25/46] [D loss: -0.184704] [G loss: -2.380006]\n",
      "[Epoch 77/200] [Batch 30/46] [D loss: -1.583519] [G loss: 0.371183]\n",
      "[Epoch 77/200] [Batch 35/46] [D loss: -1.701736] [G loss: 0.927743]\n",
      "[Epoch 77/200] [Batch 40/46] [D loss: -1.584432] [G loss: 5.993308]\n",
      "[Epoch 77/200] [Batch 45/46] [D loss: -2.081430] [G loss: 5.531204]\n",
      "[Epoch 78/200] [Batch 0/46] [D loss: -3.343858] [G loss: 5.133611]\n",
      "[Epoch 78/200] [Batch 5/46] [D loss: -1.296560] [G loss: 3.865956]\n",
      "[Epoch 78/200] [Batch 10/46] [D loss: -1.723566] [G loss: 0.748220]\n",
      "[Epoch 78/200] [Batch 15/46] [D loss: -1.494950] [G loss: -3.128469]\n",
      "[Epoch 78/200] [Batch 20/46] [D loss: -2.395301] [G loss: -4.951380]\n",
      "[Epoch 78/200] [Batch 25/46] [D loss: -2.075301] [G loss: -8.080658]\n",
      "[Epoch 78/200] [Batch 30/46] [D loss: -2.863977] [G loss: -8.527960]\n",
      "[Epoch 78/200] [Batch 35/46] [D loss: -1.278045] [G loss: -8.948975]\n",
      "[Epoch 78/200] [Batch 40/46] [D loss: -2.056054] [G loss: -8.734464]\n",
      "[Epoch 78/200] [Batch 45/46] [D loss: -1.632524] [G loss: -7.403406]\n",
      "[Epoch 79/200] [Batch 0/46] [D loss: -1.781981] [G loss: -7.834494]\n",
      "[Epoch 79/200] [Batch 5/46] [D loss: -2.573134] [G loss: -6.247006]\n",
      "[Epoch 79/200] [Batch 10/46] [D loss: 0.823811] [G loss: -4.057046]\n",
      "[Epoch 79/200] [Batch 15/46] [D loss: -2.371707] [G loss: -1.101133]\n",
      "[Epoch 79/200] [Batch 20/46] [D loss: -0.695160] [G loss: 2.107946]\n",
      "[Epoch 79/200] [Batch 25/46] [D loss: 0.015933] [G loss: 5.199425]\n",
      "[Epoch 79/200] [Batch 30/46] [D loss: -1.610488] [G loss: 5.161301]\n",
      "[Epoch 79/200] [Batch 35/46] [D loss: -1.033655] [G loss: 5.488985]\n",
      "[Epoch 79/200] [Batch 40/46] [D loss: -1.292064] [G loss: 4.172528]\n",
      "[Epoch 79/200] [Batch 45/46] [D loss: -0.584341] [G loss: 2.095306]\n",
      "[Epoch 80/200] [Batch 0/46] [D loss: -2.503651] [G loss: 1.286303]\n",
      "[Epoch 80/200] [Batch 5/46] [D loss: -1.519133] [G loss: 0.171065]\n",
      "[Epoch 80/200] [Batch 10/46] [D loss: -1.174281] [G loss: -2.091844]\n",
      "[Epoch 80/200] [Batch 15/46] [D loss: -1.556864] [G loss: -6.464763]\n",
      "[Epoch 80/200] [Batch 20/46] [D loss: -2.155272] [G loss: -7.478547]\n",
      "[Epoch 80/200] [Batch 25/46] [D loss: -0.836644] [G loss: -9.229729]\n",
      "[Epoch 80/200] [Batch 30/46] [D loss: -0.904332] [G loss: -9.290065]\n",
      "[Epoch 80/200] [Batch 35/46] [D loss: -0.556180] [G loss: -8.766891]\n",
      "[Epoch 80/200] [Batch 40/46] [D loss: -2.332726] [G loss: -6.104320]\n",
      "[Epoch 80/200] [Batch 45/46] [D loss: -2.292758] [G loss: -1.894231]\n",
      "[Epoch 81/200] [Batch 0/46] [D loss: -0.898830] [G loss: -1.579182]\n",
      "[Epoch 81/200] [Batch 5/46] [D loss: -2.078821] [G loss: 1.968509]\n",
      "[Epoch 81/200] [Batch 10/46] [D loss: -1.087635] [G loss: 2.661465]\n",
      "[Epoch 81/200] [Batch 15/46] [D loss: -2.150859] [G loss: 1.929900]\n",
      "[Epoch 81/200] [Batch 20/46] [D loss: -0.616695] [G loss: 2.567263]\n",
      "[Epoch 81/200] [Batch 25/46] [D loss: -2.079079] [G loss: 3.938034]\n",
      "[Epoch 81/200] [Batch 30/46] [D loss: -2.149962] [G loss: 1.891457]\n",
      "[Epoch 81/200] [Batch 35/46] [D loss: -1.230685] [G loss: -0.243952]\n",
      "[Epoch 81/200] [Batch 40/46] [D loss: -1.884099] [G loss: -3.623463]\n",
      "[Epoch 81/200] [Batch 45/46] [D loss: -2.139096] [G loss: -7.624040]\n",
      "[Epoch 82/200] [Batch 0/46] [D loss: -1.320256] [G loss: -8.067381]\n",
      "[Epoch 82/200] [Batch 5/46] [D loss: -1.080878] [G loss: -9.138577]\n",
      "[Epoch 82/200] [Batch 10/46] [D loss: -1.588735] [G loss: -5.752456]\n",
      "[Epoch 82/200] [Batch 15/46] [D loss: -2.324981] [G loss: -3.835323]\n",
      "[Epoch 82/200] [Batch 20/46] [D loss: -2.046931] [G loss: -2.723879]\n",
      "[Epoch 82/200] [Batch 25/46] [D loss: -1.683905] [G loss: -1.726829]\n",
      "[Epoch 82/200] [Batch 30/46] [D loss: -1.718848] [G loss: -4.200519]\n",
      "[Epoch 82/200] [Batch 35/46] [D loss: -0.132300] [G loss: -2.642392]\n",
      "[Epoch 82/200] [Batch 40/46] [D loss: -1.211110] [G loss: -2.998558]\n",
      "[Epoch 82/200] [Batch 45/46] [D loss: -2.046839] [G loss: -1.518154]\n",
      "[Epoch 83/200] [Batch 0/46] [D loss: -2.671242] [G loss: -1.216914]\n",
      "[Epoch 83/200] [Batch 5/46] [D loss: -2.016646] [G loss: -1.228260]\n",
      "[Epoch 83/200] [Batch 10/46] [D loss: -2.150111] [G loss: -2.275123]\n",
      "[Epoch 83/200] [Batch 15/46] [D loss: -1.984592] [G loss: -2.695059]\n",
      "[Epoch 83/200] [Batch 20/46] [D loss: -3.101152] [G loss: -1.465317]\n",
      "[Epoch 83/200] [Batch 25/46] [D loss: -1.757779] [G loss: -2.231761]\n",
      "[Epoch 83/200] [Batch 30/46] [D loss: -2.414197] [G loss: -3.429303]\n",
      "[Epoch 83/200] [Batch 35/46] [D loss: -0.400337] [G loss: -3.353139]\n",
      "[Epoch 83/200] [Batch 40/46] [D loss: -1.248211] [G loss: -1.896351]\n",
      "[Epoch 83/200] [Batch 45/46] [D loss: -2.242927] [G loss: -2.967669]\n",
      "[Epoch 84/200] [Batch 0/46] [D loss: -1.882511] [G loss: -4.143182]\n",
      "[Epoch 84/200] [Batch 5/46] [D loss: -1.043326] [G loss: -2.397437]\n",
      "[Epoch 84/200] [Batch 10/46] [D loss: -0.456254] [G loss: -3.514745]\n",
      "[Epoch 84/200] [Batch 15/46] [D loss: -1.832468] [G loss: -2.962357]\n",
      "[Epoch 84/200] [Batch 20/46] [D loss: -1.430339] [G loss: -5.050073]\n",
      "[Epoch 84/200] [Batch 25/46] [D loss: -2.022881] [G loss: -4.127478]\n",
      "[Epoch 84/200] [Batch 30/46] [D loss: -2.110456] [G loss: -2.188921]\n",
      "[Epoch 84/200] [Batch 35/46] [D loss: -2.572684] [G loss: -1.492739]\n",
      "[Epoch 84/200] [Batch 40/46] [D loss: -1.148990] [G loss: 0.366862]\n",
      "[Epoch 84/200] [Batch 45/46] [D loss: -0.826173] [G loss: 1.150389]\n",
      "[Epoch 85/200] [Batch 0/46] [D loss: -0.473801] [G loss: 1.377072]\n",
      "[Epoch 85/200] [Batch 5/46] [D loss: -2.290809] [G loss: 0.320376]\n",
      "[Epoch 85/200] [Batch 10/46] [D loss: -0.815393] [G loss: 1.409108]\n",
      "[Epoch 85/200] [Batch 15/46] [D loss: -2.091049] [G loss: 1.283731]\n",
      "[Epoch 85/200] [Batch 20/46] [D loss: -1.805149] [G loss: 1.733174]\n",
      "[Epoch 85/200] [Batch 25/46] [D loss: -2.454111] [G loss: 1.869014]\n",
      "[Epoch 85/200] [Batch 30/46] [D loss: -1.975600] [G loss: 2.254604]\n",
      "[Epoch 85/200] [Batch 35/46] [D loss: -1.212402] [G loss: -2.150380]\n",
      "[Epoch 85/200] [Batch 40/46] [D loss: -1.322814] [G loss: -2.902526]\n",
      "[Epoch 85/200] [Batch 45/46] [D loss: -1.285410] [G loss: -2.993230]\n",
      "[Epoch 86/200] [Batch 0/46] [D loss: -1.965380] [G loss: -4.082701]\n",
      "[Epoch 86/200] [Batch 5/46] [D loss: -1.972356] [G loss: -6.099609]\n",
      "[Epoch 86/200] [Batch 10/46] [D loss: -1.665517] [G loss: -5.846376]\n",
      "[Epoch 86/200] [Batch 15/46] [D loss: -0.428326] [G loss: -5.348222]\n",
      "[Epoch 86/200] [Batch 20/46] [D loss: -1.677805] [G loss: -5.027421]\n",
      "[Epoch 86/200] [Batch 25/46] [D loss: -1.174393] [G loss: -6.496449]\n",
      "[Epoch 86/200] [Batch 30/46] [D loss: -0.618577] [G loss: -4.934471]\n",
      "[Epoch 86/200] [Batch 35/46] [D loss: -0.911483] [G loss: -3.751425]\n",
      "[Epoch 86/200] [Batch 40/46] [D loss: -1.683952] [G loss: -2.696264]\n",
      "[Epoch 86/200] [Batch 45/46] [D loss: -2.868019] [G loss: 2.402893]\n",
      "[Epoch 87/200] [Batch 0/46] [D loss: -1.951926] [G loss: 1.419677]\n",
      "[Epoch 87/200] [Batch 5/46] [D loss: -3.232195] [G loss: 2.851214]\n",
      "[Epoch 87/200] [Batch 10/46] [D loss: -1.478501] [G loss: 2.299536]\n",
      "[Epoch 87/200] [Batch 15/46] [D loss: -1.043237] [G loss: -0.932009]\n",
      "[Epoch 87/200] [Batch 20/46] [D loss: -2.185333] [G loss: -2.167363]\n",
      "[Epoch 87/200] [Batch 25/46] [D loss: -0.905869] [G loss: -3.730747]\n",
      "[Epoch 87/200] [Batch 30/46] [D loss: -1.844635] [G loss: -4.093799]\n",
      "[Epoch 87/200] [Batch 35/46] [D loss: -2.388536] [G loss: -5.046289]\n",
      "[Epoch 87/200] [Batch 40/46] [D loss: -2.374678] [G loss: -4.897153]\n",
      "[Epoch 87/200] [Batch 45/46] [D loss: -2.267866] [G loss: -5.465701]\n",
      "[Epoch 88/200] [Batch 0/46] [D loss: -1.917657] [G loss: -5.601418]\n",
      "[Epoch 88/200] [Batch 5/46] [D loss: -0.847928] [G loss: -6.557837]\n",
      "[Epoch 88/200] [Batch 10/46] [D loss: -0.554617] [G loss: -4.916885]\n",
      "[Epoch 88/200] [Batch 15/46] [D loss: -1.814001] [G loss: -3.164885]\n",
      "[Epoch 88/200] [Batch 20/46] [D loss: -1.475883] [G loss: -2.632529]\n",
      "[Epoch 88/200] [Batch 25/46] [D loss: -1.150591] [G loss: 0.057939]\n",
      "[Epoch 88/200] [Batch 30/46] [D loss: -2.126015] [G loss: 4.618834]\n",
      "[Epoch 88/200] [Batch 35/46] [D loss: -1.714832] [G loss: 6.275369]\n",
      "[Epoch 88/200] [Batch 40/46] [D loss: -2.466845] [G loss: 4.227321]\n",
      "[Epoch 88/200] [Batch 45/46] [D loss: -1.534308] [G loss: 2.489210]\n",
      "[Epoch 89/200] [Batch 0/46] [D loss: -1.288531] [G loss: 1.850444]\n",
      "[Epoch 89/200] [Batch 5/46] [D loss: -0.252659] [G loss: -0.265237]\n",
      "[Epoch 89/200] [Batch 10/46] [D loss: -2.001460] [G loss: -1.743198]\n",
      "[Epoch 89/200] [Batch 15/46] [D loss: -0.939678] [G loss: -3.234249]\n",
      "[Epoch 89/200] [Batch 20/46] [D loss: -2.743779] [G loss: -4.491722]\n",
      "[Epoch 89/200] [Batch 25/46] [D loss: -1.696549] [G loss: -5.301770]\n",
      "[Epoch 89/200] [Batch 30/46] [D loss: -3.255660] [G loss: -6.900666]\n",
      "[Epoch 89/200] [Batch 35/46] [D loss: -0.772718] [G loss: -7.594390]\n",
      "[Epoch 89/200] [Batch 40/46] [D loss: -1.882884] [G loss: -8.001514]\n",
      "[Epoch 89/200] [Batch 45/46] [D loss: -0.608334] [G loss: -6.658399]\n",
      "[Epoch 90/200] [Batch 0/46] [D loss: -0.984747] [G loss: -6.716699]\n",
      "[Epoch 90/200] [Batch 5/46] [D loss: -1.864654] [G loss: -3.759278]\n",
      "[Epoch 90/200] [Batch 10/46] [D loss: -2.162926] [G loss: -0.035776]\n",
      "[Epoch 90/200] [Batch 15/46] [D loss: -1.978859] [G loss: 2.469017]\n",
      "[Epoch 90/200] [Batch 20/46] [D loss: -3.764155] [G loss: 2.669471]\n",
      "[Epoch 90/200] [Batch 25/46] [D loss: -2.291939] [G loss: 2.214832]\n",
      "[Epoch 90/200] [Batch 30/46] [D loss: -0.697206] [G loss: 6.091100]\n",
      "[Epoch 90/200] [Batch 35/46] [D loss: -2.106469] [G loss: 6.340167]\n",
      "[Epoch 90/200] [Batch 40/46] [D loss: -1.488059] [G loss: 5.279792]\n",
      "[Epoch 90/200] [Batch 45/46] [D loss: -1.597569] [G loss: 2.423542]\n",
      "[Epoch 91/200] [Batch 0/46] [D loss: -1.797754] [G loss: 2.590677]\n",
      "[Epoch 91/200] [Batch 5/46] [D loss: -1.359402] [G loss: 1.292441]\n",
      "[Epoch 91/200] [Batch 10/46] [D loss: -2.711249] [G loss: -2.273896]\n",
      "[Epoch 91/200] [Batch 15/46] [D loss: -2.990836] [G loss: -5.493908]\n",
      "[Epoch 91/200] [Batch 20/46] [D loss: -2.752071] [G loss: -7.421403]\n",
      "[Epoch 91/200] [Batch 25/46] [D loss: -1.045873] [G loss: -9.441751]\n",
      "[Epoch 91/200] [Batch 30/46] [D loss: -1.225586] [G loss: -10.224523]\n",
      "[Epoch 91/200] [Batch 35/46] [D loss: -1.237342] [G loss: -9.771789]\n",
      "[Epoch 91/200] [Batch 40/46] [D loss: -1.448207] [G loss: -7.557904]\n",
      "[Epoch 91/200] [Batch 45/46] [D loss: -2.746831] [G loss: -3.721284]\n",
      "[Epoch 92/200] [Batch 0/46] [D loss: -2.699261] [G loss: -2.978101]\n",
      "[Epoch 92/200] [Batch 5/46] [D loss: -0.642734] [G loss: -2.565254]\n",
      "[Epoch 92/200] [Batch 10/46] [D loss: -1.100206] [G loss: -1.723373]\n",
      "[Epoch 92/200] [Batch 15/46] [D loss: -2.238992] [G loss: -1.048893]\n",
      "[Epoch 92/200] [Batch 20/46] [D loss: -1.394935] [G loss: -0.337586]\n",
      "[Epoch 92/200] [Batch 25/46] [D loss: -1.647617] [G loss: 1.358140]\n",
      "[Epoch 92/200] [Batch 30/46] [D loss: -2.917772] [G loss: 1.230053]\n",
      "[Epoch 92/200] [Batch 35/46] [D loss: -0.904362] [G loss: 2.388523]\n",
      "[Epoch 92/200] [Batch 40/46] [D loss: -1.967751] [G loss: 3.378067]\n",
      "[Epoch 92/200] [Batch 45/46] [D loss: -1.243526] [G loss: 0.974350]\n",
      "[Epoch 93/200] [Batch 0/46] [D loss: -0.362360] [G loss: 1.777690]\n",
      "[Epoch 93/200] [Batch 5/46] [D loss: -0.317570] [G loss: -0.712061]\n",
      "[Epoch 93/200] [Batch 10/46] [D loss: -1.546744] [G loss: -1.935437]\n",
      "[Epoch 93/200] [Batch 15/46] [D loss: -2.109657] [G loss: -1.467840]\n",
      "[Epoch 93/200] [Batch 20/46] [D loss: -3.073979] [G loss: -1.085705]\n",
      "[Epoch 93/200] [Batch 25/46] [D loss: -1.000029] [G loss: -1.687841]\n",
      "[Epoch 93/200] [Batch 30/46] [D loss: -2.042787] [G loss: -5.536014]\n",
      "[Epoch 93/200] [Batch 35/46] [D loss: -1.714599] [G loss: -5.438912]\n",
      "[Epoch 93/200] [Batch 40/46] [D loss: -2.090854] [G loss: -3.802999]\n",
      "[Epoch 93/200] [Batch 45/46] [D loss: -2.380349] [G loss: -3.749882]\n",
      "[Epoch 94/200] [Batch 0/46] [D loss: -1.788307] [G loss: -4.659159]\n",
      "[Epoch 94/200] [Batch 5/46] [D loss: -1.353798] [G loss: -4.840078]\n",
      "[Epoch 94/200] [Batch 10/46] [D loss: -0.058752] [G loss: -4.602137]\n",
      "[Epoch 94/200] [Batch 15/46] [D loss: -2.360749] [G loss: -0.184943]\n",
      "[Epoch 94/200] [Batch 20/46] [D loss: -1.129271] [G loss: -0.320756]\n",
      "[Epoch 94/200] [Batch 25/46] [D loss: -1.827113] [G loss: 0.465638]\n",
      "[Epoch 94/200] [Batch 30/46] [D loss: -1.517445] [G loss: -0.053825]\n",
      "[Epoch 94/200] [Batch 35/46] [D loss: -2.050016] [G loss: -0.227528]\n",
      "[Epoch 94/200] [Batch 40/46] [D loss: -2.085015] [G loss: -1.013316]\n",
      "[Epoch 94/200] [Batch 45/46] [D loss: -1.839498] [G loss: 0.074709]\n",
      "[Epoch 95/200] [Batch 0/46] [D loss: -2.459300] [G loss: 0.251172]\n",
      "[Epoch 95/200] [Batch 5/46] [D loss: -2.850614] [G loss: -0.199541]\n",
      "[Epoch 95/200] [Batch 10/46] [D loss: -2.167932] [G loss: -0.323656]\n",
      "[Epoch 95/200] [Batch 15/46] [D loss: -1.903295] [G loss: 0.522700]\n",
      "[Epoch 95/200] [Batch 20/46] [D loss: -2.263216] [G loss: 0.014714]\n",
      "[Epoch 95/200] [Batch 25/46] [D loss: -2.120844] [G loss: 0.906609]\n",
      "[Epoch 95/200] [Batch 30/46] [D loss: -2.418782] [G loss: -1.451897]\n",
      "[Epoch 95/200] [Batch 35/46] [D loss: -2.379590] [G loss: -3.048014]\n",
      "[Epoch 95/200] [Batch 40/46] [D loss: -0.812871] [G loss: -2.980460]\n",
      "[Epoch 95/200] [Batch 45/46] [D loss: -1.900910] [G loss: -4.996123]\n",
      "[Epoch 96/200] [Batch 0/46] [D loss: -0.100772] [G loss: -4.101538]\n",
      "[Epoch 96/200] [Batch 5/46] [D loss: -1.527645] [G loss: -4.514572]\n",
      "[Epoch 96/200] [Batch 10/46] [D loss: -1.578868] [G loss: -5.255224]\n",
      "[Epoch 96/200] [Batch 15/46] [D loss: -1.592203] [G loss: -5.615311]\n",
      "[Epoch 96/200] [Batch 20/46] [D loss: -1.473745] [G loss: -5.964888]\n",
      "[Epoch 96/200] [Batch 25/46] [D loss: -2.298993] [G loss: -4.372924]\n",
      "[Epoch 96/200] [Batch 30/46] [D loss: -0.347729] [G loss: -3.452539]\n",
      "[Epoch 96/200] [Batch 35/46] [D loss: -1.654933] [G loss: -3.442559]\n",
      "[Epoch 96/200] [Batch 40/46] [D loss: -0.859469] [G loss: -1.119464]\n",
      "[Epoch 96/200] [Batch 45/46] [D loss: -1.477011] [G loss: -1.640893]\n",
      "[Epoch 97/200] [Batch 0/46] [D loss: -0.396910] [G loss: -2.348130]\n",
      "[Epoch 97/200] [Batch 5/46] [D loss: -0.937419] [G loss: -2.081294]\n",
      "[Epoch 97/200] [Batch 10/46] [D loss: -1.664297] [G loss: -1.466177]\n",
      "[Epoch 97/200] [Batch 15/46] [D loss: -2.357132] [G loss: 0.219782]\n",
      "[Epoch 97/200] [Batch 20/46] [D loss: -2.107220] [G loss: 0.184402]\n",
      "[Epoch 97/200] [Batch 25/46] [D loss: -1.710558] [G loss: 0.970793]\n",
      "[Epoch 97/200] [Batch 30/46] [D loss: -1.341480] [G loss: -0.133991]\n",
      "[Epoch 97/200] [Batch 35/46] [D loss: -3.066644] [G loss: -0.792734]\n",
      "[Epoch 97/200] [Batch 40/46] [D loss: -3.724899] [G loss: -1.669277]\n",
      "[Epoch 97/200] [Batch 45/46] [D loss: -1.205133] [G loss: -3.947137]\n",
      "[Epoch 98/200] [Batch 0/46] [D loss: -0.064542] [G loss: -4.850301]\n",
      "[Epoch 98/200] [Batch 5/46] [D loss: -0.949380] [G loss: -6.041567]\n",
      "[Epoch 98/200] [Batch 10/46] [D loss: -2.362401] [G loss: -2.283615]\n",
      "[Epoch 98/200] [Batch 15/46] [D loss: -1.459890] [G loss: -1.812895]\n",
      "[Epoch 98/200] [Batch 20/46] [D loss: -3.351975] [G loss: -0.979609]\n",
      "[Epoch 98/200] [Batch 25/46] [D loss: -2.720244] [G loss: 1.088196]\n",
      "[Epoch 98/200] [Batch 30/46] [D loss: -1.167653] [G loss: 0.560905]\n",
      "[Epoch 98/200] [Batch 35/46] [D loss: -1.301469] [G loss: 0.647015]\n",
      "[Epoch 98/200] [Batch 40/46] [D loss: -3.115600] [G loss: -0.726549]\n",
      "[Epoch 98/200] [Batch 45/46] [D loss: -2.702201] [G loss: 0.270556]\n",
      "[Epoch 99/200] [Batch 0/46] [D loss: -2.499908] [G loss: -0.278364]\n",
      "[Epoch 99/200] [Batch 5/46] [D loss: -0.795517] [G loss: 0.544967]\n",
      "[Epoch 99/200] [Batch 10/46] [D loss: -1.313371] [G loss: 2.148209]\n",
      "[Epoch 99/200] [Batch 15/46] [D loss: -0.544485] [G loss: -0.250514]\n",
      "[Epoch 99/200] [Batch 20/46] [D loss: -2.844512] [G loss: 0.008804]\n",
      "[Epoch 99/200] [Batch 25/46] [D loss: -0.015397] [G loss: -5.214778]\n",
      "[Epoch 99/200] [Batch 30/46] [D loss: -1.275675] [G loss: -3.226495]\n",
      "[Epoch 99/200] [Batch 35/46] [D loss: -1.624676] [G loss: -3.451486]\n",
      "[Epoch 99/200] [Batch 40/46] [D loss: -1.461717] [G loss: -4.013151]\n",
      "[Epoch 99/200] [Batch 45/46] [D loss: -2.291371] [G loss: -3.411209]\n",
      "[Epoch 100/200] [Batch 0/46] [D loss: -0.829699] [G loss: -3.113506]\n",
      "[Epoch 100/200] [Batch 5/46] [D loss: -2.345436] [G loss: -2.483609]\n",
      "[Epoch 100/200] [Batch 10/46] [D loss: -1.245987] [G loss: -2.413006]\n",
      "[Epoch 100/200] [Batch 15/46] [D loss: -2.484087] [G loss: -1.284962]\n",
      "[Epoch 100/200] [Batch 20/46] [D loss: -1.574353] [G loss: -0.995687]\n",
      "[Epoch 100/200] [Batch 25/46] [D loss: -1.455223] [G loss: -1.346028]\n",
      "[Epoch 100/200] [Batch 30/46] [D loss: -1.614631] [G loss: 0.047983]\n",
      "[Epoch 100/200] [Batch 35/46] [D loss: -1.600752] [G loss: 1.201182]\n",
      "[Epoch 100/200] [Batch 40/46] [D loss: -1.567317] [G loss: 1.095014]\n",
      "[Epoch 100/200] [Batch 45/46] [D loss: -3.129780] [G loss: 0.618436]\n",
      "[Epoch 101/200] [Batch 0/46] [D loss: -0.088280] [G loss: 0.660455]\n",
      "[Epoch 101/200] [Batch 5/46] [D loss: -1.476203] [G loss: 1.464766]\n",
      "[Epoch 101/200] [Batch 10/46] [D loss: -2.331613] [G loss: 0.055178]\n",
      "[Epoch 101/200] [Batch 15/46] [D loss: -1.260569] [G loss: 0.472878]\n",
      "[Epoch 101/200] [Batch 20/46] [D loss: -0.395623] [G loss: -0.205989]\n",
      "[Epoch 101/200] [Batch 25/46] [D loss: -2.098967] [G loss: -0.281702]\n",
      "[Epoch 101/200] [Batch 30/46] [D loss: -1.130194] [G loss: -2.090430]\n",
      "[Epoch 101/200] [Batch 35/46] [D loss: -0.964227] [G loss: -3.154888]\n",
      "[Epoch 101/200] [Batch 40/46] [D loss: -1.181425] [G loss: -2.521483]\n",
      "[Epoch 101/200] [Batch 45/46] [D loss: -2.354124] [G loss: -4.940997]\n",
      "[Epoch 102/200] [Batch 0/46] [D loss: -1.044578] [G loss: -2.521430]\n",
      "[Epoch 102/200] [Batch 5/46] [D loss: -1.197241] [G loss: -4.044790]\n",
      "[Epoch 102/200] [Batch 10/46] [D loss: -0.238253] [G loss: -2.974431]\n",
      "[Epoch 102/200] [Batch 15/46] [D loss: 0.445099] [G loss: -2.903471]\n",
      "[Epoch 102/200] [Batch 20/46] [D loss: -1.936454] [G loss: -1.495749]\n",
      "[Epoch 102/200] [Batch 25/46] [D loss: -1.185901] [G loss: -0.761964]\n",
      "[Epoch 102/200] [Batch 30/46] [D loss: -2.214813] [G loss: 0.224016]\n",
      "[Epoch 102/200] [Batch 35/46] [D loss: -2.441594] [G loss: 1.147972]\n",
      "[Epoch 102/200] [Batch 40/46] [D loss: -2.770747] [G loss: 3.386283]\n",
      "[Epoch 102/200] [Batch 45/46] [D loss: -1.531144] [G loss: 1.477520]\n",
      "[Epoch 103/200] [Batch 0/46] [D loss: -1.491121] [G loss: 1.858904]\n",
      "[Epoch 103/200] [Batch 5/46] [D loss: -2.175134] [G loss: 0.170034]\n",
      "[Epoch 103/200] [Batch 10/46] [D loss: -1.021815] [G loss: -0.604744]\n",
      "[Epoch 103/200] [Batch 15/46] [D loss: -2.820021] [G loss: -2.085598]\n",
      "[Epoch 103/200] [Batch 20/46] [D loss: -2.955232] [G loss: -3.904562]\n",
      "[Epoch 103/200] [Batch 25/46] [D loss: -0.932590] [G loss: -3.588667]\n",
      "[Epoch 103/200] [Batch 30/46] [D loss: -1.793751] [G loss: -2.877702]\n",
      "[Epoch 103/200] [Batch 35/46] [D loss: -2.695231] [G loss: -2.175248]\n",
      "[Epoch 103/200] [Batch 40/46] [D loss: -0.760614] [G loss: -2.955676]\n",
      "[Epoch 103/200] [Batch 45/46] [D loss: -0.076894] [G loss: -2.879296]\n",
      "[Epoch 104/200] [Batch 0/46] [D loss: -0.474058] [G loss: -1.633953]\n",
      "[Epoch 104/200] [Batch 10/46] [D loss: 0.034168] [G loss: -2.022953]\n",
      "[Epoch 104/200] [Batch 15/46] [D loss: -1.383300] [G loss: -2.252486]\n",
      "[Epoch 104/200] [Batch 20/46] [D loss: -3.417060] [G loss: -1.243927]\n",
      "[Epoch 104/200] [Batch 25/46] [D loss: -1.582794] [G loss: -2.650754]\n",
      "[Epoch 104/200] [Batch 30/46] [D loss: -0.845242] [G loss: -3.210779]\n",
      "[Epoch 104/200] [Batch 35/46] [D loss: -1.444304] [G loss: -1.526842]\n",
      "[Epoch 104/200] [Batch 40/46] [D loss: -2.028085] [G loss: -0.633674]\n",
      "[Epoch 104/200] [Batch 45/46] [D loss: -2.844746] [G loss: -1.909467]\n",
      "[Epoch 105/200] [Batch 0/46] [D loss: -0.964219] [G loss: -1.514271]\n",
      "[Epoch 105/200] [Batch 5/46] [D loss: -1.217358] [G loss: -3.325158]\n",
      "[Epoch 105/200] [Batch 10/46] [D loss: -1.619289] [G loss: -1.929990]\n",
      "[Epoch 105/200] [Batch 15/46] [D loss: -0.529633] [G loss: -2.243505]\n",
      "[Epoch 105/200] [Batch 20/46] [D loss: -1.539680] [G loss: -3.368869]\n",
      "[Epoch 105/200] [Batch 25/46] [D loss: -2.359216] [G loss: -3.954002]\n",
      "[Epoch 105/200] [Batch 30/46] [D loss: -1.352746] [G loss: -4.887440]\n",
      "[Epoch 105/200] [Batch 35/46] [D loss: -0.138345] [G loss: -4.527843]\n",
      "[Epoch 105/200] [Batch 40/46] [D loss: -1.935281] [G loss: -3.527476]\n",
      "[Epoch 105/200] [Batch 45/46] [D loss: -1.596757] [G loss: -1.704075]\n",
      "[Epoch 106/200] [Batch 0/46] [D loss: -0.869822] [G loss: -0.858026]\n",
      "[Epoch 106/200] [Batch 5/46] [D loss: -1.281149] [G loss: 0.720341]\n",
      "[Epoch 106/200] [Batch 10/46] [D loss: -1.996538] [G loss: 0.973703]\n",
      "[Epoch 106/200] [Batch 15/46] [D loss: -1.331327] [G loss: 2.308347]\n",
      "[Epoch 106/200] [Batch 20/46] [D loss: -1.635256] [G loss: 0.944514]\n",
      "[Epoch 106/200] [Batch 25/46] [D loss: -2.129189] [G loss: 0.649397]\n",
      "[Epoch 106/200] [Batch 30/46] [D loss: -2.337487] [G loss: 0.029833]\n",
      "[Epoch 106/200] [Batch 35/46] [D loss: -1.905932] [G loss: -0.151826]\n",
      "[Epoch 106/200] [Batch 40/46] [D loss: -1.631355] [G loss: -0.067195]\n",
      "[Epoch 106/200] [Batch 45/46] [D loss: -0.772332] [G loss: -1.796770]\n",
      "[Epoch 107/200] [Batch 0/46] [D loss: -3.329563] [G loss: -1.129785]\n",
      "[Epoch 107/200] [Batch 5/46] [D loss: -2.205475] [G loss: -1.979753]\n",
      "[Epoch 107/200] [Batch 10/46] [D loss: -1.809670] [G loss: -3.601526]\n",
      "[Epoch 107/200] [Batch 15/46] [D loss: -1.541716] [G loss: -3.540911]\n",
      "[Epoch 107/200] [Batch 20/46] [D loss: -1.682619] [G loss: -3.666485]\n",
      "[Epoch 107/200] [Batch 25/46] [D loss: -1.415058] [G loss: -3.069962]\n",
      "[Epoch 107/200] [Batch 30/46] [D loss: -1.646518] [G loss: -1.078027]\n",
      "[Epoch 107/200] [Batch 35/46] [D loss: -2.363934] [G loss: -2.033324]\n",
      "[Epoch 107/200] [Batch 40/46] [D loss: -1.880563] [G loss: -0.750038]\n",
      "[Epoch 107/200] [Batch 45/46] [D loss: -2.142356] [G loss: 0.942533]\n",
      "[Epoch 108/200] [Batch 0/46] [D loss: -2.112378] [G loss: 0.813807]\n",
      "[Epoch 108/200] [Batch 5/46] [D loss: -0.997777] [G loss: 1.245073]\n",
      "[Epoch 108/200] [Batch 10/46] [D loss: -2.405513] [G loss: 0.461199]\n",
      "[Epoch 108/200] [Batch 15/46] [D loss: -1.809842] [G loss: -0.850945]\n",
      "[Epoch 108/200] [Batch 20/46] [D loss: -1.904608] [G loss: -0.229810]\n",
      "[Epoch 108/200] [Batch 25/46] [D loss: -2.139157] [G loss: -1.806963]\n",
      "[Epoch 108/200] [Batch 30/46] [D loss: -2.299053] [G loss: -2.229842]\n",
      "[Epoch 108/200] [Batch 35/46] [D loss: -3.740757] [G loss: -2.048122]\n",
      "[Epoch 108/200] [Batch 40/46] [D loss: -1.899666] [G loss: -2.431468]\n",
      "[Epoch 108/200] [Batch 45/46] [D loss: -1.919564] [G loss: -3.025416]\n",
      "[Epoch 109/200] [Batch 0/46] [D loss: -1.033253] [G loss: -3.566861]\n",
      "[Epoch 109/200] [Batch 5/46] [D loss: -1.077639] [G loss: -4.339134]\n",
      "[Epoch 109/200] [Batch 10/46] [D loss: -1.909810] [G loss: -2.091366]\n",
      "[Epoch 109/200] [Batch 15/46] [D loss: -2.045962] [G loss: 0.414001]\n",
      "[Epoch 109/200] [Batch 20/46] [D loss: -0.413197] [G loss: 1.540487]\n",
      "[Epoch 109/200] [Batch 25/46] [D loss: -2.242323] [G loss: 1.026806]\n",
      "[Epoch 109/200] [Batch 30/46] [D loss: -0.024862] [G loss: 0.656181]\n",
      "[Epoch 109/200] [Batch 35/46] [D loss: -0.400071] [G loss: -0.905173]\n",
      "[Epoch 109/200] [Batch 40/46] [D loss: -2.374108] [G loss: -0.676660]\n",
      "[Epoch 109/200] [Batch 45/46] [D loss: -1.627921] [G loss: -1.413293]\n",
      "[Epoch 110/200] [Batch 0/46] [D loss: -1.038720] [G loss: -1.999579]\n",
      "[Epoch 110/200] [Batch 5/46] [D loss: -0.911095] [G loss: -2.808420]\n",
      "[Epoch 110/200] [Batch 10/46] [D loss: -0.160241] [G loss: -1.815174]\n",
      "[Epoch 110/200] [Batch 15/46] [D loss: -1.866993] [G loss: -1.770780]\n",
      "[Epoch 110/200] [Batch 20/46] [D loss: -1.978848] [G loss: -3.227937]\n",
      "[Epoch 110/200] [Batch 25/46] [D loss: -2.345767] [G loss: -2.824497]\n",
      "[Epoch 110/200] [Batch 30/46] [D loss: -1.603402] [G loss: -1.867425]\n",
      "[Epoch 110/200] [Batch 35/46] [D loss: -2.800896] [G loss: 0.645546]\n",
      "[Epoch 110/200] [Batch 40/46] [D loss: -1.027301] [G loss: 1.303034]\n",
      "[Epoch 110/200] [Batch 45/46] [D loss: 0.126524] [G loss: 0.223440]\n",
      "[Epoch 111/200] [Batch 0/46] [D loss: -0.634789] [G loss: 0.402991]\n",
      "[Epoch 111/200] [Batch 5/46] [D loss: -1.574355] [G loss: -1.378358]\n",
      "[Epoch 111/200] [Batch 10/46] [D loss: -1.357278] [G loss: -1.419405]\n",
      "[Epoch 111/200] [Batch 15/46] [D loss: -1.589385] [G loss: -3.678772]\n",
      "[Epoch 111/200] [Batch 20/46] [D loss: -2.988137] [G loss: -3.250721]\n",
      "[Epoch 111/200] [Batch 25/46] [D loss: -0.660379] [G loss: -3.217484]\n",
      "[Epoch 111/200] [Batch 30/46] [D loss: -1.727110] [G loss: -2.148710]\n",
      "[Epoch 111/200] [Batch 35/46] [D loss: -2.800411] [G loss: -2.419173]\n",
      "[Epoch 111/200] [Batch 40/46] [D loss: -0.620112] [G loss: -1.300101]\n",
      "[Epoch 111/200] [Batch 45/46] [D loss: -0.189614] [G loss: -0.909402]\n",
      "[Epoch 112/200] [Batch 0/46] [D loss: -1.733791] [G loss: -2.807223]\n",
      "[Epoch 112/200] [Batch 5/46] [D loss: -1.941437] [G loss: -2.715896]\n",
      "[Epoch 112/200] [Batch 10/46] [D loss: -1.555884] [G loss: -3.550913]\n",
      "[Epoch 112/200] [Batch 15/46] [D loss: -3.187772] [G loss: -1.705244]\n",
      "[Epoch 112/200] [Batch 20/46] [D loss: -1.928429] [G loss: -1.371275]\n",
      "[Epoch 112/200] [Batch 25/46] [D loss: -1.917727] [G loss: -2.227788]\n",
      "[Epoch 112/200] [Batch 30/46] [D loss: -2.283615] [G loss: -2.426701]\n",
      "[Epoch 112/200] [Batch 35/46] [D loss: -1.598249] [G loss: -0.858905]\n",
      "[Epoch 112/200] [Batch 40/46] [D loss: -1.517153] [G loss: 1.028867]\n",
      "[Epoch 112/200] [Batch 45/46] [D loss: -2.363914] [G loss: 2.534610]\n",
      "[Epoch 113/200] [Batch 0/46] [D loss: -2.945428] [G loss: 2.866433]\n",
      "[Epoch 113/200] [Batch 5/46] [D loss: -1.867540] [G loss: 2.323035]\n",
      "[Epoch 113/200] [Batch 10/46] [D loss: -2.703372] [G loss: 1.326093]\n",
      "[Epoch 113/200] [Batch 15/46] [D loss: -1.570346] [G loss: -1.007582]\n",
      "[Epoch 113/200] [Batch 20/46] [D loss: -2.702129] [G loss: -1.049684]\n",
      "[Epoch 113/200] [Batch 25/46] [D loss: -2.082031] [G loss: -0.801149]\n",
      "[Epoch 113/200] [Batch 30/46] [D loss: -0.897601] [G loss: -0.220299]\n",
      "[Epoch 113/200] [Batch 35/46] [D loss: -2.682412] [G loss: -1.387851]\n",
      "[Epoch 113/200] [Batch 40/46] [D loss: -2.495405] [G loss: -2.887862]\n",
      "[Epoch 113/200] [Batch 45/46] [D loss: -2.396826] [G loss: -2.332478]\n",
      "[Epoch 114/200] [Batch 0/46] [D loss: -2.517152] [G loss: -1.808663]\n",
      "[Epoch 114/200] [Batch 5/46] [D loss: -0.652987] [G loss: -1.054027]\n",
      "[Epoch 114/200] [Batch 10/46] [D loss: -1.803118] [G loss: -1.101863]\n",
      "[Epoch 114/200] [Batch 15/46] [D loss: -1.161955] [G loss: -1.183320]\n",
      "[Epoch 114/200] [Batch 20/46] [D loss: -2.797774] [G loss: -2.073308]\n",
      "[Epoch 114/200] [Batch 25/46] [D loss: -0.965503] [G loss: -1.900469]\n",
      "[Epoch 114/200] [Batch 30/46] [D loss: -0.175377] [G loss: -1.669418]\n",
      "[Epoch 114/200] [Batch 35/46] [D loss: -2.279441] [G loss: -3.696906]\n",
      "[Epoch 114/200] [Batch 40/46] [D loss: -0.899136] [G loss: -1.836450]\n",
      "[Epoch 114/200] [Batch 45/46] [D loss: -1.824018] [G loss: 0.095296]\n",
      "[Epoch 115/200] [Batch 0/46] [D loss: -0.384424] [G loss: -1.526470]\n",
      "[Epoch 115/200] [Batch 5/46] [D loss: -1.658442] [G loss: -1.560528]\n",
      "[Epoch 115/200] [Batch 10/46] [D loss: -1.497149] [G loss: -0.599600]\n",
      "[Epoch 115/200] [Batch 15/46] [D loss: -2.810339] [G loss: -1.747095]\n",
      "[Epoch 115/200] [Batch 20/46] [D loss: -1.337037] [G loss: -1.419869]\n",
      "[Epoch 115/200] [Batch 25/46] [D loss: -0.577310] [G loss: -0.553475]\n",
      "[Epoch 115/200] [Batch 30/46] [D loss: -2.719516] [G loss: 0.062348]\n",
      "[Epoch 115/200] [Batch 35/46] [D loss: -1.825853] [G loss: 0.589991]\n",
      "[Epoch 115/200] [Batch 40/46] [D loss: -0.579728] [G loss: 0.728738]\n",
      "[Epoch 115/200] [Batch 45/46] [D loss: -0.744954] [G loss: -0.148467]\n",
      "[Epoch 116/200] [Batch 0/46] [D loss: -1.953365] [G loss: -0.089170]\n",
      "[Epoch 116/200] [Batch 5/46] [D loss: -1.774296] [G loss: -0.111378]\n",
      "[Epoch 116/200] [Batch 10/46] [D loss: -1.806194] [G loss: -1.277016]\n",
      "[Epoch 116/200] [Batch 15/46] [D loss: -1.892314] [G loss: -1.410047]\n",
      "[Epoch 116/200] [Batch 20/46] [D loss: -2.324140] [G loss: -0.220388]\n",
      "[Epoch 116/200] [Batch 25/46] [D loss: -2.018794] [G loss: -2.161576]\n",
      "[Epoch 116/200] [Batch 30/46] [D loss: -2.189808] [G loss: -3.459292]\n",
      "[Epoch 116/200] [Batch 35/46] [D loss: -1.553927] [G loss: -4.578843]\n",
      "[Epoch 116/200] [Batch 40/46] [D loss: -2.505510] [G loss: -2.504169]\n",
      "[Epoch 116/200] [Batch 45/46] [D loss: -2.358134] [G loss: -0.626997]\n",
      "[Epoch 117/200] [Batch 0/46] [D loss: -1.398988] [G loss: -1.380957]\n",
      "[Epoch 117/200] [Batch 5/46] [D loss: -1.946931] [G loss: -1.773296]\n",
      "[Epoch 117/200] [Batch 10/46] [D loss: -1.671922] [G loss: -1.800596]\n",
      "[Epoch 117/200] [Batch 15/46] [D loss: -2.120908] [G loss: -3.433388]\n",
      "[Epoch 117/200] [Batch 20/46] [D loss: -3.093747] [G loss: -2.353237]\n",
      "[Epoch 117/200] [Batch 25/46] [D loss: -1.739512] [G loss: -1.685697]\n",
      "[Epoch 117/200] [Batch 30/46] [D loss: -2.391389] [G loss: -1.471530]\n",
      "[Epoch 117/200] [Batch 35/46] [D loss: -3.107532] [G loss: -0.470313]\n",
      "[Epoch 117/200] [Batch 40/46] [D loss: -2.550313] [G loss: -1.172045]\n",
      "[Epoch 117/200] [Batch 45/46] [D loss: -3.929420] [G loss: -1.158479]\n",
      "[Epoch 118/200] [Batch 0/46] [D loss: -1.450552] [G loss: -1.267182]\n",
      "[Epoch 118/200] [Batch 5/46] [D loss: -0.082683] [G loss: -2.890399]\n",
      "[Epoch 118/200] [Batch 10/46] [D loss: -3.180186] [G loss: -3.131592]\n",
      "[Epoch 118/200] [Batch 15/46] [D loss: -1.861922] [G loss: -0.786529]\n",
      "[Epoch 118/200] [Batch 20/46] [D loss: -2.512977] [G loss: -0.887346]\n",
      "[Epoch 118/200] [Batch 25/46] [D loss: -3.412806] [G loss: -0.454390]\n",
      "[Epoch 118/200] [Batch 30/46] [D loss: -2.622741] [G loss: -3.222029]\n",
      "[Epoch 118/200] [Batch 35/46] [D loss: -1.618154] [G loss: -4.340005]\n",
      "[Epoch 118/200] [Batch 40/46] [D loss: -0.370961] [G loss: -4.332515]\n",
      "[Epoch 118/200] [Batch 45/46] [D loss: -1.068952] [G loss: -4.611707]\n",
      "[Epoch 119/200] [Batch 0/46] [D loss: -1.601847] [G loss: -3.801053]\n",
      "[Epoch 119/200] [Batch 5/46] [D loss: -2.861029] [G loss: -3.125814]\n",
      "[Epoch 119/200] [Batch 10/46] [D loss: -1.841019] [G loss: -2.101074]\n",
      "[Epoch 119/200] [Batch 15/46] [D loss: -3.464486] [G loss: -0.899297]\n",
      "[Epoch 119/200] [Batch 20/46] [D loss: -1.118809] [G loss: -0.200925]\n",
      "[Epoch 119/200] [Batch 25/46] [D loss: -1.887948] [G loss: -0.646200]\n",
      "[Epoch 119/200] [Batch 30/46] [D loss: -2.945038] [G loss: -0.260336]\n",
      "[Epoch 119/200] [Batch 35/46] [D loss: -1.153419] [G loss: -3.282114]\n",
      "[Epoch 119/200] [Batch 40/46] [D loss: -2.012787] [G loss: -3.709258]\n",
      "[Epoch 119/200] [Batch 45/46] [D loss: -2.551458] [G loss: -0.241241]\n",
      "[Epoch 120/200] [Batch 0/46] [D loss: -1.445630] [G loss: -0.419550]\n",
      "[Epoch 120/200] [Batch 5/46] [D loss: -2.290422] [G loss: 1.653872]\n",
      "[Epoch 120/200] [Batch 10/46] [D loss: -3.895968] [G loss: 3.474523]\n",
      "[Epoch 120/200] [Batch 15/46] [D loss: -2.468091] [G loss: 2.771907]\n",
      "[Epoch 120/200] [Batch 20/46] [D loss: -1.796888] [G loss: -0.443451]\n",
      "[Epoch 120/200] [Batch 25/46] [D loss: -1.470520] [G loss: -1.910977]\n",
      "[Epoch 120/200] [Batch 30/46] [D loss: -0.978875] [G loss: -4.056841]\n",
      "[Epoch 120/200] [Batch 35/46] [D loss: -1.925851] [G loss: -4.615657]\n",
      "[Epoch 120/200] [Batch 40/46] [D loss: -2.732728] [G loss: -3.434857]\n",
      "[Epoch 120/200] [Batch 45/46] [D loss: -3.562855] [G loss: -1.746802]\n",
      "[Epoch 121/200] [Batch 0/46] [D loss: -1.076051] [G loss: -3.002927]\n",
      "[Epoch 121/200] [Batch 5/46] [D loss: -2.634709] [G loss: -2.298478]\n",
      "[Epoch 121/200] [Batch 10/46] [D loss: -1.237841] [G loss: -3.655594]\n",
      "[Epoch 121/200] [Batch 15/46] [D loss: -1.134600] [G loss: -2.201143]\n",
      "[Epoch 121/200] [Batch 20/46] [D loss: -1.394042] [G loss: -4.483559]\n",
      "[Epoch 121/200] [Batch 25/46] [D loss: -1.958521] [G loss: -4.391261]\n",
      "[Epoch 121/200] [Batch 30/46] [D loss: -1.390965] [G loss: -2.940576]\n",
      "[Epoch 121/200] [Batch 35/46] [D loss: -1.519154] [G loss: -2.999878]\n",
      "[Epoch 121/200] [Batch 40/46] [D loss: -1.823383] [G loss: -1.702501]\n",
      "[Epoch 121/200] [Batch 45/46] [D loss: -1.311069] [G loss: -0.435079]\n",
      "[Epoch 122/200] [Batch 0/46] [D loss: 0.251841] [G loss: -0.890638]\n",
      "[Epoch 122/200] [Batch 5/46] [D loss: -0.969302] [G loss: -1.591739]\n",
      "[Epoch 122/200] [Batch 10/46] [D loss: -0.585844] [G loss: -2.797166]\n",
      "[Epoch 122/200] [Batch 15/46] [D loss: -2.919871] [G loss: -0.621615]\n",
      "[Epoch 122/200] [Batch 20/46] [D loss: -1.833030] [G loss: -1.003145]\n",
      "[Epoch 122/200] [Batch 25/46] [D loss: -0.166599] [G loss: -1.249467]\n",
      "[Epoch 122/200] [Batch 30/46] [D loss: -1.015595] [G loss: -0.000150]\n",
      "[Epoch 122/200] [Batch 35/46] [D loss: -1.489604] [G loss: 2.290662]\n",
      "[Epoch 122/200] [Batch 40/46] [D loss: -1.290612] [G loss: -0.267531]\n",
      "[Epoch 122/200] [Batch 45/46] [D loss: -1.388226] [G loss: -1.610344]\n",
      "[Epoch 123/200] [Batch 0/46] [D loss: -0.974521] [G loss: -2.579350]\n",
      "[Epoch 123/200] [Batch 5/46] [D loss: -1.209239] [G loss: -4.470011]\n",
      "[Epoch 123/200] [Batch 10/46] [D loss: -2.630629] [G loss: -6.021875]\n",
      "[Epoch 123/200] [Batch 15/46] [D loss: -0.969030] [G loss: -5.779180]\n",
      "[Epoch 123/200] [Batch 20/46] [D loss: -2.224701] [G loss: -3.009020]\n",
      "[Epoch 123/200] [Batch 25/46] [D loss: -2.242406] [G loss: -1.556570]\n",
      "[Epoch 123/200] [Batch 30/46] [D loss: -1.900953] [G loss: -0.670498]\n",
      "[Epoch 123/200] [Batch 35/46] [D loss: -1.603351] [G loss: 0.838070]\n",
      "[Epoch 123/200] [Batch 40/46] [D loss: -0.739118] [G loss: 0.219272]\n",
      "[Epoch 123/200] [Batch 45/46] [D loss: -2.199842] [G loss: -0.090907]\n",
      "[Epoch 124/200] [Batch 0/46] [D loss: -0.392084] [G loss: 0.933854]\n",
      "[Epoch 124/200] [Batch 5/46] [D loss: -1.782882] [G loss: 1.569271]\n",
      "[Epoch 124/200] [Batch 10/46] [D loss: -1.797248] [G loss: 1.273493]\n",
      "[Epoch 124/200] [Batch 15/46] [D loss: -0.821145] [G loss: 0.050588]\n",
      "[Epoch 124/200] [Batch 20/46] [D loss: -1.052483] [G loss: -0.081017]\n",
      "[Epoch 124/200] [Batch 25/46] [D loss: -1.754864] [G loss: 0.411607]\n",
      "[Epoch 124/200] [Batch 30/46] [D loss: -3.004845] [G loss: -0.823679]\n",
      "[Epoch 124/200] [Batch 35/46] [D loss: -1.751972] [G loss: -2.651575]\n",
      "[Epoch 124/200] [Batch 40/46] [D loss: -1.968128] [G loss: -3.384652]\n",
      "[Epoch 124/200] [Batch 45/46] [D loss: -2.312171] [G loss: -4.660722]\n",
      "[Epoch 125/200] [Batch 0/46] [D loss: -2.425858] [G loss: -5.099120]\n",
      "[Epoch 125/200] [Batch 5/46] [D loss: -0.999593] [G loss: -4.283191]\n",
      "[Epoch 125/200] [Batch 10/46] [D loss: -1.896277] [G loss: -3.814702]\n",
      "[Epoch 125/200] [Batch 15/46] [D loss: -2.556648] [G loss: -2.206810]\n",
      "[Epoch 125/200] [Batch 20/46] [D loss: -0.966119] [G loss: -2.143213]\n",
      "[Epoch 125/200] [Batch 25/46] [D loss: -1.656036] [G loss: -0.344025]\n",
      "[Epoch 125/200] [Batch 30/46] [D loss: -2.444850] [G loss: 0.030902]\n",
      "[Epoch 125/200] [Batch 35/46] [D loss: -1.952682] [G loss: -0.664508]\n",
      "[Epoch 125/200] [Batch 40/46] [D loss: -0.203838] [G loss: -0.333651]\n",
      "[Epoch 125/200] [Batch 45/46] [D loss: -1.600607] [G loss: 1.318849]\n",
      "[Epoch 126/200] [Batch 0/46] [D loss: -1.673234] [G loss: 0.855189]\n",
      "[Epoch 126/200] [Batch 5/46] [D loss: -0.813199] [G loss: 0.325050]\n",
      "[Epoch 126/200] [Batch 10/46] [D loss: -2.712783] [G loss: -1.182985]\n",
      "[Epoch 126/200] [Batch 15/46] [D loss: -1.433346] [G loss: -2.412294]\n",
      "[Epoch 126/200] [Batch 20/46] [D loss: -1.469741] [G loss: -3.400129]\n",
      "[Epoch 126/200] [Batch 25/46] [D loss: -3.854665] [G loss: -1.364001]\n",
      "[Epoch 126/200] [Batch 30/46] [D loss: -2.457950] [G loss: -1.616214]\n",
      "[Epoch 126/200] [Batch 35/46] [D loss: -1.858164] [G loss: -4.198272]\n",
      "[Epoch 126/200] [Batch 40/46] [D loss: -1.523585] [G loss: -4.603991]\n",
      "[Epoch 126/200] [Batch 45/46] [D loss: -1.162035] [G loss: -5.572892]\n",
      "[Epoch 127/200] [Batch 0/46] [D loss: -2.761233] [G loss: -4.942910]\n",
      "[Epoch 127/200] [Batch 5/46] [D loss: -1.237908] [G loss: -5.680748]\n",
      "[Epoch 127/200] [Batch 10/46] [D loss: -1.101404] [G loss: -5.007969]\n",
      "[Epoch 127/200] [Batch 15/46] [D loss: -2.668062] [G loss: -2.328756]\n",
      "[Epoch 127/200] [Batch 20/46] [D loss: -2.227160] [G loss: 0.418605]\n",
      "[Epoch 127/200] [Batch 25/46] [D loss: -3.558552] [G loss: -1.026103]\n",
      "[Epoch 127/200] [Batch 30/46] [D loss: -4.143686] [G loss: 2.203486]\n",
      "[Epoch 127/200] [Batch 35/46] [D loss: -0.886366] [G loss: 0.474434]\n",
      "[Epoch 127/200] [Batch 40/46] [D loss: -3.225391] [G loss: 1.820796]\n",
      "[Epoch 127/200] [Batch 45/46] [D loss: 0.055220] [G loss: 2.939712]\n",
      "[Epoch 128/200] [Batch 0/46] [D loss: -2.831873] [G loss: 4.259814]\n",
      "[Epoch 128/200] [Batch 5/46] [D loss: -2.275788] [G loss: 4.413841]\n",
      "[Epoch 128/200] [Batch 10/46] [D loss: -0.961227] [G loss: 1.781083]\n",
      "[Epoch 128/200] [Batch 15/46] [D loss: -2.494411] [G loss: 0.791074]\n",
      "[Epoch 128/200] [Batch 25/46] [D loss: -2.546656] [G loss: -0.643016]\n",
      "[Epoch 128/200] [Batch 30/46] [D loss: -1.704370] [G loss: -2.703699]\n",
      "[Epoch 128/200] [Batch 35/46] [D loss: -0.885742] [G loss: -2.612889]\n",
      "[Epoch 128/200] [Batch 40/46] [D loss: -1.406675] [G loss: -0.801585]\n",
      "[Epoch 128/200] [Batch 45/46] [D loss: -1.868826] [G loss: 1.118809]\n",
      "[Epoch 129/200] [Batch 0/46] [D loss: -0.963278] [G loss: 0.716913]\n",
      "[Epoch 129/200] [Batch 5/46] [D loss: -3.011191] [G loss: 0.201925]\n",
      "[Epoch 129/200] [Batch 10/46] [D loss: -3.211506] [G loss: 1.125156]\n",
      "[Epoch 129/200] [Batch 15/46] [D loss: -1.500201] [G loss: 1.087906]\n",
      "[Epoch 129/200] [Batch 20/46] [D loss: -1.027261] [G loss: -2.318942]\n",
      "[Epoch 129/200] [Batch 25/46] [D loss: -0.504881] [G loss: -2.659939]\n",
      "[Epoch 129/200] [Batch 30/46] [D loss: -2.180762] [G loss: -2.746055]\n",
      "[Epoch 129/200] [Batch 35/46] [D loss: -2.053535] [G loss: -2.959978]\n",
      "[Epoch 129/200] [Batch 40/46] [D loss: -1.651781] [G loss: -3.890863]\n",
      "[Epoch 129/200] [Batch 45/46] [D loss: -2.227640] [G loss: -4.470182]\n",
      "[Epoch 130/200] [Batch 0/46] [D loss: -0.929607] [G loss: -4.813289]\n",
      "[Epoch 130/200] [Batch 5/46] [D loss: -1.562457] [G loss: -6.126661]\n",
      "[Epoch 130/200] [Batch 10/46] [D loss: -1.162580] [G loss: -5.169433]\n",
      "[Epoch 130/200] [Batch 15/46] [D loss: -1.658281] [G loss: -3.215524]\n",
      "[Epoch 130/200] [Batch 20/46] [D loss: -0.815301] [G loss: -3.853503]\n",
      "[Epoch 130/200] [Batch 25/46] [D loss: -1.807589] [G loss: -2.393197]\n",
      "[Epoch 130/200] [Batch 30/46] [D loss: -1.319643] [G loss: -1.932352]\n",
      "[Epoch 130/200] [Batch 35/46] [D loss: -1.726235] [G loss: 0.302249]\n",
      "[Epoch 130/200] [Batch 40/46] [D loss: -2.173227] [G loss: 4.034781]\n",
      "[Epoch 130/200] [Batch 45/46] [D loss: -0.628245] [G loss: 3.536546]\n",
      "[Epoch 131/200] [Batch 0/46] [D loss: -2.596668] [G loss: 2.824420]\n",
      "[Epoch 131/200] [Batch 5/46] [D loss: -0.304360] [G loss: 3.865535]\n",
      "[Epoch 131/200] [Batch 10/46] [D loss: -1.400331] [G loss: 2.609591]\n",
      "[Epoch 131/200] [Batch 15/46] [D loss: -0.782572] [G loss: 2.634543]\n",
      "[Epoch 131/200] [Batch 20/46] [D loss: -1.534526] [G loss: -0.348533]\n",
      "[Epoch 131/200] [Batch 25/46] [D loss: 0.368903] [G loss: -2.141322]\n",
      "[Epoch 131/200] [Batch 30/46] [D loss: -0.953323] [G loss: -3.822577]\n",
      "[Epoch 131/200] [Batch 35/46] [D loss: -2.416338] [G loss: -2.894143]\n",
      "[Epoch 131/200] [Batch 40/46] [D loss: -1.701699] [G loss: -3.608543]\n",
      "[Epoch 131/200] [Batch 45/46] [D loss: -1.481191] [G loss: -4.350488]\n",
      "[Epoch 132/200] [Batch 0/46] [D loss: -0.741153] [G loss: -3.776617]\n",
      "[Epoch 132/200] [Batch 5/46] [D loss: -2.190838] [G loss: -3.638356]\n",
      "[Epoch 132/200] [Batch 10/46] [D loss: -1.565256] [G loss: -2.205722]\n",
      "[Epoch 132/200] [Batch 15/46] [D loss: -2.762878] [G loss: -1.663336]\n",
      "[Epoch 132/200] [Batch 20/46] [D loss: -0.980841] [G loss: -1.759332]\n",
      "[Epoch 132/200] [Batch 25/46] [D loss: -1.102537] [G loss: -1.659742]\n",
      "[Epoch 132/200] [Batch 30/46] [D loss: -2.170387] [G loss: -2.338817]\n",
      "[Epoch 132/200] [Batch 35/46] [D loss: -2.178916] [G loss: -0.768616]\n",
      "[Epoch 132/200] [Batch 40/46] [D loss: -2.078909] [G loss: -0.635831]\n",
      "[Epoch 132/200] [Batch 45/46] [D loss: -2.130911] [G loss: -2.016686]\n",
      "[Epoch 133/200] [Batch 0/46] [D loss: -1.236962] [G loss: -3.633319]\n",
      "[Epoch 133/200] [Batch 5/46] [D loss: -1.155397] [G loss: -4.540735]\n",
      "[Epoch 133/200] [Batch 10/46] [D loss: -0.675153] [G loss: -4.707179]\n",
      "[Epoch 133/200] [Batch 15/46] [D loss: -0.811768] [G loss: -4.522886]\n",
      "[Epoch 133/200] [Batch 20/46] [D loss: -1.638032] [G loss: -2.442907]\n",
      "[Epoch 133/200] [Batch 25/46] [D loss: -1.514935] [G loss: -0.301199]\n",
      "[Epoch 133/200] [Batch 30/46] [D loss: -0.945332] [G loss: 1.135684]\n",
      "[Epoch 133/200] [Batch 35/46] [D loss: -2.349377] [G loss: 1.184679]\n",
      "[Epoch 133/200] [Batch 40/46] [D loss: -1.515068] [G loss: 3.104714]\n",
      "[Epoch 133/200] [Batch 45/46] [D loss: -0.666016] [G loss: 3.791958]\n",
      "[Epoch 134/200] [Batch 0/46] [D loss: -0.328105] [G loss: 3.100584]\n",
      "[Epoch 134/200] [Batch 5/46] [D loss: -1.477800] [G loss: 2.815501]\n",
      "[Epoch 134/200] [Batch 10/46] [D loss: -1.613007] [G loss: 0.533151]\n",
      "[Epoch 134/200] [Batch 15/46] [D loss: -2.180673] [G loss: 0.478193]\n",
      "[Epoch 134/200] [Batch 20/46] [D loss: -2.138269] [G loss: -2.888878]\n",
      "[Epoch 134/200] [Batch 25/46] [D loss: -1.986671] [G loss: -3.792206]\n",
      "[Epoch 134/200] [Batch 30/46] [D loss: -0.138932] [G loss: -4.859630]\n",
      "[Epoch 134/200] [Batch 35/46] [D loss: -0.928910] [G loss: -5.079982]\n",
      "[Epoch 134/200] [Batch 40/46] [D loss: -1.604661] [G loss: -5.292990]\n",
      "[Epoch 134/200] [Batch 45/46] [D loss: -1.708741] [G loss: -5.722896]\n",
      "[Epoch 135/200] [Batch 0/46] [D loss: -2.405692] [G loss: -6.350293]\n",
      "[Epoch 135/200] [Batch 5/46] [D loss: -0.559498] [G loss: -5.777757]\n",
      "[Epoch 135/200] [Batch 10/46] [D loss: -2.540356] [G loss: -3.358353]\n",
      "[Epoch 135/200] [Batch 15/46] [D loss: -2.379762] [G loss: -0.694820]\n",
      "[Epoch 135/200] [Batch 20/46] [D loss: -1.692220] [G loss: -0.527829]\n",
      "[Epoch 135/200] [Batch 25/46] [D loss: -2.639600] [G loss: -0.834666]\n",
      "[Epoch 135/200] [Batch 30/46] [D loss: -2.210888] [G loss: -1.473440]\n",
      "[Epoch 135/200] [Batch 35/46] [D loss: -1.532824] [G loss: -1.514598]\n",
      "[Epoch 135/200] [Batch 40/46] [D loss: -1.596821] [G loss: 0.169742]\n",
      "[Epoch 135/200] [Batch 45/46] [D loss: -2.801440] [G loss: 0.159600]\n",
      "[Epoch 136/200] [Batch 0/46] [D loss: -1.665085] [G loss: -0.864229]\n",
      "[Epoch 136/200] [Batch 5/46] [D loss: -1.440514] [G loss: 0.244656]\n",
      "[Epoch 136/200] [Batch 10/46] [D loss: -0.651259] [G loss: 0.081899]\n",
      "[Epoch 136/200] [Batch 15/46] [D loss: -1.019426] [G loss: -0.864034]\n",
      "[Epoch 136/200] [Batch 20/46] [D loss: -1.932342] [G loss: 0.237401]\n",
      "[Epoch 136/200] [Batch 25/46] [D loss: -2.243346] [G loss: -0.697540]\n",
      "[Epoch 136/200] [Batch 30/46] [D loss: -0.832125] [G loss: -0.201387]\n",
      "[Epoch 136/200] [Batch 35/46] [D loss: -1.988844] [G loss: -3.293466]\n",
      "[Epoch 136/200] [Batch 40/46] [D loss: -1.770763] [G loss: -3.574858]\n",
      "[Epoch 136/200] [Batch 45/46] [D loss: -0.488532] [G loss: -2.646303]\n",
      "[Epoch 137/200] [Batch 0/46] [D loss: -1.487939] [G loss: -2.270754]\n",
      "[Epoch 137/200] [Batch 5/46] [D loss: -0.502746] [G loss: -1.759437]\n",
      "[Epoch 137/200] [Batch 10/46] [D loss: 0.184576] [G loss: -1.018755]\n",
      "[Epoch 137/200] [Batch 15/46] [D loss: -1.963433] [G loss: -1.676948]\n",
      "[Epoch 137/200] [Batch 20/46] [D loss: -1.439394] [G loss: 0.766077]\n",
      "[Epoch 137/200] [Batch 25/46] [D loss: -2.961394] [G loss: -0.878229]\n",
      "[Epoch 137/200] [Batch 30/46] [D loss: -1.405209] [G loss: -0.698993]\n",
      "[Epoch 137/200] [Batch 35/46] [D loss: -2.862590] [G loss: -1.722062]\n",
      "[Epoch 137/200] [Batch 40/46] [D loss: -1.331382] [G loss: -0.064972]\n",
      "[Epoch 137/200] [Batch 45/46] [D loss: -2.431905] [G loss: -1.772534]\n",
      "[Epoch 138/200] [Batch 0/46] [D loss: -1.181155] [G loss: -1.625264]\n",
      "[Epoch 138/200] [Batch 5/46] [D loss: -1.473553] [G loss: -0.696803]\n",
      "[Epoch 138/200] [Batch 10/46] [D loss: -2.821228] [G loss: -0.404742]\n",
      "[Epoch 138/200] [Batch 15/46] [D loss: -1.153477] [G loss: -0.699772]\n",
      "[Epoch 138/200] [Batch 20/46] [D loss: -3.322740] [G loss: 0.033891]\n",
      "[Epoch 138/200] [Batch 25/46] [D loss: -0.529601] [G loss: -0.480390]\n",
      "[Epoch 138/200] [Batch 30/46] [D loss: -0.599750] [G loss: -0.522762]\n",
      "[Epoch 138/200] [Batch 35/46] [D loss: -2.020908] [G loss: -3.568026]\n",
      "[Epoch 138/200] [Batch 40/46] [D loss: -2.040172] [G loss: -3.829919]\n",
      "[Epoch 138/200] [Batch 45/46] [D loss: 0.202213] [G loss: -5.236174]\n",
      "[Epoch 139/200] [Batch 0/46] [D loss: -1.603197] [G loss: -5.629152]\n",
      "[Epoch 139/200] [Batch 5/46] [D loss: -3.151420] [G loss: -3.599001]\n",
      "[Epoch 139/200] [Batch 10/46] [D loss: -2.095597] [G loss: -3.038197]\n",
      "[Epoch 139/200] [Batch 15/46] [D loss: -2.440187] [G loss: -2.676441]\n",
      "[Epoch 139/200] [Batch 20/46] [D loss: -0.760156] [G loss: -2.738123]\n",
      "[Epoch 139/200] [Batch 25/46] [D loss: -1.956363] [G loss: -2.907815]\n",
      "[Epoch 139/200] [Batch 30/46] [D loss: -1.324406] [G loss: -3.742602]\n",
      "[Epoch 139/200] [Batch 35/46] [D loss: -1.066246] [G loss: -3.745940]\n",
      "[Epoch 139/200] [Batch 40/46] [D loss: -2.296810] [G loss: -1.884271]\n",
      "[Epoch 139/200] [Batch 45/46] [D loss: -1.890009] [G loss: -2.111701]\n",
      "[Epoch 140/200] [Batch 0/46] [D loss: -0.341862] [G loss: -2.386447]\n",
      "[Epoch 140/200] [Batch 5/46] [D loss: -1.507516] [G loss: -1.443783]\n",
      "[Epoch 140/200] [Batch 10/46] [D loss: -1.787836] [G loss: -0.843066]\n",
      "[Epoch 140/200] [Batch 15/46] [D loss: -3.231542] [G loss: -0.262121]\n",
      "[Epoch 140/200] [Batch 20/46] [D loss: -1.519158] [G loss: -1.778269]\n",
      "[Epoch 140/200] [Batch 25/46] [D loss: -1.319813] [G loss: -2.433878]\n",
      "[Epoch 140/200] [Batch 30/46] [D loss: -2.275519] [G loss: -0.956149]\n",
      "[Epoch 140/200] [Batch 35/46] [D loss: -1.621385] [G loss: 0.446658]\n",
      "[Epoch 140/200] [Batch 40/46] [D loss: -1.079958] [G loss: -2.087071]\n",
      "[Epoch 140/200] [Batch 45/46] [D loss: -2.164617] [G loss: -1.572989]\n",
      "[Epoch 141/200] [Batch 0/46] [D loss: -1.129488] [G loss: -1.704959]\n",
      "[Epoch 141/200] [Batch 5/46] [D loss: -0.604440] [G loss: -2.930512]\n",
      "[Epoch 141/200] [Batch 10/46] [D loss: -1.403252] [G loss: -3.172659]\n",
      "[Epoch 141/200] [Batch 15/46] [D loss: -1.955144] [G loss: -0.959164]\n",
      "[Epoch 141/200] [Batch 20/46] [D loss: -1.236569] [G loss: -0.379348]\n",
      "[Epoch 141/200] [Batch 25/46] [D loss: -2.552340] [G loss: 1.053504]\n",
      "[Epoch 141/200] [Batch 30/46] [D loss: -2.857948] [G loss: 2.066734]\n",
      "[Epoch 141/200] [Batch 35/46] [D loss: -2.671453] [G loss: 0.495013]\n",
      "[Epoch 141/200] [Batch 40/46] [D loss: -0.196168] [G loss: 0.735920]\n",
      "[Epoch 141/200] [Batch 45/46] [D loss: -0.536045] [G loss: -0.001634]\n",
      "[Epoch 142/200] [Batch 0/46] [D loss: -1.100163] [G loss: -0.888254]\n",
      "[Epoch 142/200] [Batch 5/46] [D loss: -3.037712] [G loss: -2.212840]\n",
      "[Epoch 142/200] [Batch 10/46] [D loss: -2.706039] [G loss: -2.758937]\n",
      "[Epoch 142/200] [Batch 15/46] [D loss: -0.351835] [G loss: -2.681589]\n",
      "[Epoch 142/200] [Batch 20/46] [D loss: -0.671447] [G loss: -2.650457]\n",
      "[Epoch 142/200] [Batch 25/46] [D loss: -0.854361] [G loss: -0.838448]\n",
      "[Epoch 142/200] [Batch 30/46] [D loss: -2.513550] [G loss: -0.715692]\n",
      "[Epoch 142/200] [Batch 35/46] [D loss: -1.565966] [G loss: -0.865960]\n",
      "[Epoch 142/200] [Batch 40/46] [D loss: -1.755217] [G loss: -0.609990]\n",
      "[Epoch 142/200] [Batch 45/46] [D loss: -1.946520] [G loss: -0.725788]\n",
      "[Epoch 143/200] [Batch 0/46] [D loss: -1.825546] [G loss: -2.316410]\n",
      "[Epoch 143/200] [Batch 5/46] [D loss: -1.390726] [G loss: -0.913248]\n",
      "[Epoch 143/200] [Batch 10/46] [D loss: -2.095022] [G loss: -0.821606]\n",
      "[Epoch 143/200] [Batch 15/46] [D loss: -2.639421] [G loss: -1.736048]\n",
      "[Epoch 143/200] [Batch 20/46] [D loss: -1.790117] [G loss: -1.676575]\n",
      "[Epoch 143/200] [Batch 25/46] [D loss: -1.471082] [G loss: -1.573449]\n",
      "[Epoch 143/200] [Batch 30/46] [D loss: -0.542752] [G loss: -1.313234]\n",
      "[Epoch 143/200] [Batch 35/46] [D loss: -1.576729] [G loss: 0.427351]\n",
      "[Epoch 143/200] [Batch 40/46] [D loss: -2.511288] [G loss: -0.146645]\n",
      "[Epoch 143/200] [Batch 45/46] [D loss: 0.256986] [G loss: -1.149533]\n",
      "[Epoch 144/200] [Batch 0/46] [D loss: -1.803559] [G loss: -0.935577]\n",
      "[Epoch 144/200] [Batch 5/46] [D loss: -2.074760] [G loss: -2.414448]\n",
      "[Epoch 144/200] [Batch 10/46] [D loss: -2.895870] [G loss: -3.885489]\n",
      "[Epoch 144/200] [Batch 15/46] [D loss: -2.376591] [G loss: -2.235950]\n",
      "[Epoch 144/200] [Batch 20/46] [D loss: -1.184017] [G loss: -1.709250]\n",
      "[Epoch 144/200] [Batch 25/46] [D loss: 0.362255] [G loss: -0.148848]\n",
      "[Epoch 144/200] [Batch 30/46] [D loss: -2.665046] [G loss: 0.489130]\n",
      "[Epoch 144/200] [Batch 35/46] [D loss: -0.665930] [G loss: 0.918659]\n",
      "[Epoch 144/200] [Batch 40/46] [D loss: 0.497564] [G loss: 0.938548]\n",
      "[Epoch 144/200] [Batch 45/46] [D loss: -1.152522] [G loss: -0.273481]\n",
      "[Epoch 145/200] [Batch 0/46] [D loss: -2.834583] [G loss: -1.597735]\n",
      "[Epoch 145/200] [Batch 5/46] [D loss: -2.767427] [G loss: -2.524060]\n",
      "[Epoch 145/200] [Batch 10/46] [D loss: -2.101791] [G loss: -1.719492]\n",
      "[Epoch 145/200] [Batch 15/46] [D loss: -1.265897] [G loss: -3.479358]\n",
      "[Epoch 145/200] [Batch 20/46] [D loss: -0.130458] [G loss: -2.615971]\n",
      "[Epoch 145/200] [Batch 25/46] [D loss: -1.260091] [G loss: -2.430388]\n",
      "[Epoch 145/200] [Batch 30/46] [D loss: -1.975343] [G loss: -3.285108]\n",
      "[Epoch 145/200] [Batch 35/46] [D loss: -0.659308] [G loss: -4.416511]\n",
      "[Epoch 145/200] [Batch 40/46] [D loss: -0.884239] [G loss: -3.276605]\n",
      "[Epoch 145/200] [Batch 45/46] [D loss: -3.291253] [G loss: -2.637840]\n",
      "[Epoch 146/200] [Batch 0/46] [D loss: -1.497827] [G loss: -3.382856]\n",
      "[Epoch 146/200] [Batch 5/46] [D loss: -2.020345] [G loss: -2.384432]\n",
      "[Epoch 146/200] [Batch 10/46] [D loss: -0.279836] [G loss: -2.088230]\n",
      "[Epoch 146/200] [Batch 15/46] [D loss: -1.414240] [G loss: -0.285139]\n",
      "[Epoch 146/200] [Batch 20/46] [D loss: -1.624321] [G loss: 0.508673]\n",
      "[Epoch 146/200] [Batch 25/46] [D loss: -0.675142] [G loss: -0.969083]\n",
      "[Epoch 146/200] [Batch 30/46] [D loss: -0.780357] [G loss: -1.427751]\n",
      "[Epoch 146/200] [Batch 35/46] [D loss: -2.434378] [G loss: -1.664891]\n",
      "[Epoch 146/200] [Batch 40/46] [D loss: -0.672996] [G loss: -3.243861]\n",
      "[Epoch 146/200] [Batch 45/46] [D loss: -3.322896] [G loss: -2.038713]\n",
      "[Epoch 147/200] [Batch 0/46] [D loss: -2.219686] [G loss: -1.514726]\n",
      "[Epoch 147/200] [Batch 5/46] [D loss: -2.020010] [G loss: -0.212021]\n",
      "[Epoch 147/200] [Batch 10/46] [D loss: -0.831104] [G loss: -1.564731]\n",
      "[Epoch 147/200] [Batch 15/46] [D loss: -1.364449] [G loss: -0.897741]\n",
      "[Epoch 147/200] [Batch 20/46] [D loss: -0.637762] [G loss: 0.139314]\n",
      "[Epoch 147/200] [Batch 25/46] [D loss: -0.586074] [G loss: -0.253338]\n",
      "[Epoch 147/200] [Batch 30/46] [D loss: -2.352737] [G loss: -0.456065]\n",
      "[Epoch 147/200] [Batch 35/46] [D loss: -0.782238] [G loss: -0.416330]\n",
      "[Epoch 147/200] [Batch 40/46] [D loss: -1.334754] [G loss: 0.327012]\n",
      "[Epoch 147/200] [Batch 45/46] [D loss: -0.044658] [G loss: -0.618156]\n",
      "[Epoch 148/200] [Batch 0/46] [D loss: -2.150294] [G loss: -1.429152]\n",
      "[Epoch 148/200] [Batch 5/46] [D loss: -1.797220] [G loss: -2.040157]\n",
      "[Epoch 148/200] [Batch 10/46] [D loss: -0.810410] [G loss: -1.731500]\n",
      "[Epoch 148/200] [Batch 15/46] [D loss: -1.536726] [G loss: -1.680637]\n",
      "[Epoch 148/200] [Batch 20/46] [D loss: -0.256243] [G loss: -2.175757]\n",
      "[Epoch 148/200] [Batch 25/46] [D loss: -1.780399] [G loss: -1.942259]\n",
      "[Epoch 148/200] [Batch 30/46] [D loss: -1.238567] [G loss: -3.490731]\n",
      "[Epoch 148/200] [Batch 35/46] [D loss: -0.182318] [G loss: -3.742811]\n",
      "[Epoch 148/200] [Batch 40/46] [D loss: -3.737499] [G loss: -3.286308]\n",
      "[Epoch 148/200] [Batch 45/46] [D loss: -1.627598] [G loss: -3.440671]\n",
      "[Epoch 149/200] [Batch 0/46] [D loss: -1.136884] [G loss: -3.998605]\n",
      "[Epoch 149/200] [Batch 5/46] [D loss: -0.967443] [G loss: -2.201277]\n",
      "[Epoch 149/200] [Batch 10/46] [D loss: -1.813976] [G loss: -1.675654]\n",
      "[Epoch 149/200] [Batch 15/46] [D loss: -2.415221] [G loss: -0.956903]\n",
      "[Epoch 149/200] [Batch 20/46] [D loss: -0.860770] [G loss: -0.064974]\n",
      "[Epoch 149/200] [Batch 25/46] [D loss: -2.223049] [G loss: -0.648551]\n",
      "[Epoch 149/200] [Batch 30/46] [D loss: -1.622020] [G loss: -0.916495]\n",
      "[Epoch 149/200] [Batch 35/46] [D loss: -0.292575] [G loss: -0.805029]\n",
      "[Epoch 149/200] [Batch 40/46] [D loss: -0.472391] [G loss: -1.262151]\n",
      "[Epoch 149/200] [Batch 45/46] [D loss: -1.678500] [G loss: -3.258804]\n",
      "[Epoch 150/200] [Batch 5/46] [D loss: -0.789495] [G loss: -4.489069]\n",
      "[Epoch 150/200] [Batch 10/46] [D loss: -1.067001] [G loss: -2.184857]\n",
      "[Epoch 150/200] [Batch 15/46] [D loss: -2.576354] [G loss: -1.881520]\n",
      "[Epoch 150/200] [Batch 20/46] [D loss: -2.009513] [G loss: -0.106150]\n",
      "[Epoch 150/200] [Batch 25/46] [D loss: -2.315775] [G loss: 1.003027]\n",
      "[Epoch 150/200] [Batch 30/46] [D loss: -0.017141] [G loss: 2.131872]\n",
      "[Epoch 150/200] [Batch 35/46] [D loss: -0.497712] [G loss: 2.100192]\n",
      "[Epoch 150/200] [Batch 40/46] [D loss: -3.458066] [G loss: 1.258606]\n",
      "[Epoch 150/200] [Batch 45/46] [D loss: -1.299618] [G loss: 1.485915]\n",
      "[Epoch 151/200] [Batch 0/46] [D loss: -2.535774] [G loss: 0.771721]\n",
      "[Epoch 151/200] [Batch 5/46] [D loss: -1.825160] [G loss: -0.455418]\n",
      "[Epoch 151/200] [Batch 10/46] [D loss: -1.263102] [G loss: -2.015660]\n",
      "[Epoch 151/200] [Batch 15/46] [D loss: -1.057142] [G loss: -2.826180]\n",
      "[Epoch 151/200] [Batch 20/46] [D loss: -1.546682] [G loss: -4.624504]\n",
      "[Epoch 151/200] [Batch 25/46] [D loss: -2.136336] [G loss: -6.697295]\n",
      "[Epoch 151/200] [Batch 30/46] [D loss: -3.951056] [G loss: -5.713268]\n",
      "[Epoch 151/200] [Batch 35/46] [D loss: -1.860903] [G loss: -6.775402]\n",
      "[Epoch 151/200] [Batch 40/46] [D loss: -1.536754] [G loss: -5.429501]\n",
      "[Epoch 151/200] [Batch 45/46] [D loss: -1.175688] [G loss: -4.793453]\n",
      "[Epoch 152/200] [Batch 0/46] [D loss: -0.410044] [G loss: -4.474812]\n",
      "[Epoch 152/200] [Batch 5/46] [D loss: -2.273206] [G loss: -2.374810]\n",
      "[Epoch 152/200] [Batch 10/46] [D loss: -3.228058] [G loss: -2.162710]\n",
      "[Epoch 152/200] [Batch 15/46] [D loss: -1.542855] [G loss: -1.103105]\n",
      "[Epoch 152/200] [Batch 20/46] [D loss: -2.749446] [G loss: -0.305576]\n",
      "[Epoch 152/200] [Batch 25/46] [D loss: -0.864182] [G loss: 0.308886]\n",
      "[Epoch 152/200] [Batch 30/46] [D loss: -1.651734] [G loss: -0.398744]\n",
      "[Epoch 152/200] [Batch 35/46] [D loss: -0.439277] [G loss: -0.087743]\n",
      "[Epoch 152/200] [Batch 40/46] [D loss: -2.783813] [G loss: 1.805968]\n",
      "[Epoch 152/200] [Batch 45/46] [D loss: -1.622427] [G loss: 1.488024]\n",
      "[Epoch 153/200] [Batch 0/46] [D loss: -0.949860] [G loss: 1.345952]\n",
      "[Epoch 153/200] [Batch 5/46] [D loss: -2.231379] [G loss: 0.942713]\n",
      "[Epoch 153/200] [Batch 10/46] [D loss: -0.357213] [G loss: -0.627064]\n",
      "[Epoch 153/200] [Batch 15/46] [D loss: -1.781698] [G loss: -0.987820]\n",
      "[Epoch 153/200] [Batch 20/46] [D loss: -1.389172] [G loss: -2.064403]\n",
      "[Epoch 153/200] [Batch 25/46] [D loss: -2.211244] [G loss: -2.588743]\n",
      "[Epoch 153/200] [Batch 30/46] [D loss: -0.874658] [G loss: -2.986543]\n",
      "[Epoch 153/200] [Batch 35/46] [D loss: -1.605739] [G loss: -0.390874]\n",
      "[Epoch 153/200] [Batch 40/46] [D loss: -1.933282] [G loss: -0.361554]\n",
      "[Epoch 153/200] [Batch 45/46] [D loss: -2.145518] [G loss: 0.503013]\n",
      "[Epoch 154/200] [Batch 0/46] [D loss: -0.426107] [G loss: -0.087220]\n",
      "[Epoch 154/200] [Batch 5/46] [D loss: 0.560503] [G loss: 0.496792]\n",
      "[Epoch 154/200] [Batch 10/46] [D loss: -1.935812] [G loss: 1.021738]\n",
      "[Epoch 154/200] [Batch 15/46] [D loss: -1.894161] [G loss: 0.076075]\n",
      "[Epoch 154/200] [Batch 20/46] [D loss: -0.682485] [G loss: 0.984777]\n",
      "[Epoch 154/200] [Batch 25/46] [D loss: -3.418034] [G loss: -0.419536]\n",
      "[Epoch 154/200] [Batch 30/46] [D loss: -2.693021] [G loss: 0.545362]\n",
      "[Epoch 154/200] [Batch 35/46] [D loss: -0.507976] [G loss: 1.547908]\n",
      "[Epoch 154/200] [Batch 40/46] [D loss: -1.311826] [G loss: -0.562239]\n",
      "[Epoch 154/200] [Batch 45/46] [D loss: -1.327103] [G loss: -0.026157]\n",
      "[Epoch 155/200] [Batch 0/46] [D loss: -2.443582] [G loss: -0.155172]\n",
      "[Epoch 155/200] [Batch 5/46] [D loss: -1.415242] [G loss: -0.397383]\n",
      "[Epoch 155/200] [Batch 10/46] [D loss: -1.329078] [G loss: 0.730468]\n",
      "[Epoch 155/200] [Batch 15/46] [D loss: -1.229428] [G loss: 0.698941]\n",
      "[Epoch 155/200] [Batch 20/46] [D loss: -0.640733] [G loss: 1.289887]\n",
      "[Epoch 155/200] [Batch 25/46] [D loss: -1.347860] [G loss: -0.515865]\n",
      "[Epoch 155/200] [Batch 30/46] [D loss: 0.194282] [G loss: -0.304254]\n",
      "[Epoch 155/200] [Batch 35/46] [D loss: -2.074439] [G loss: -2.186755]\n",
      "[Epoch 155/200] [Batch 40/46] [D loss: -2.233505] [G loss: 0.093178]\n",
      "[Epoch 155/200] [Batch 45/46] [D loss: -1.209902] [G loss: 0.096176]\n",
      "[Epoch 156/200] [Batch 0/46] [D loss: -1.011532] [G loss: -0.712591]\n",
      "[Epoch 156/200] [Batch 5/46] [D loss: -2.426643] [G loss: -2.143683]\n",
      "[Epoch 156/200] [Batch 10/46] [D loss: -0.755082] [G loss: -1.400749]\n",
      "[Epoch 156/200] [Batch 15/46] [D loss: -1.289638] [G loss: -2.628556]\n",
      "[Epoch 156/200] [Batch 20/46] [D loss: -2.332272] [G loss: -2.649126]\n",
      "[Epoch 156/200] [Batch 25/46] [D loss: -2.296384] [G loss: -2.157025]\n",
      "[Epoch 156/200] [Batch 30/46] [D loss: -1.411820] [G loss: -2.859826]\n",
      "[Epoch 156/200] [Batch 35/46] [D loss: -2.077735] [G loss: -3.090478]\n",
      "[Epoch 156/200] [Batch 40/46] [D loss: -2.310179] [G loss: -1.732115]\n",
      "[Epoch 156/200] [Batch 45/46] [D loss: -0.578979] [G loss: -1.501673]\n",
      "[Epoch 157/200] [Batch 0/46] [D loss: -1.546982] [G loss: -0.256743]\n",
      "[Epoch 157/200] [Batch 5/46] [D loss: -1.220633] [G loss: 2.129715]\n",
      "[Epoch 157/200] [Batch 10/46] [D loss: -1.505913] [G loss: 3.874743]\n",
      "[Epoch 157/200] [Batch 15/46] [D loss: -2.084940] [G loss: 1.415625]\n",
      "[Epoch 157/200] [Batch 20/46] [D loss: -1.912873] [G loss: 1.156361]\n",
      "[Epoch 157/200] [Batch 25/46] [D loss: -2.666163] [G loss: 1.304483]\n",
      "[Epoch 157/200] [Batch 30/46] [D loss: -0.738981] [G loss: -0.058405]\n",
      "[Epoch 157/200] [Batch 35/46] [D loss: -0.733704] [G loss: -1.796097]\n",
      "[Epoch 157/200] [Batch 40/46] [D loss: -1.224864] [G loss: -0.801222]\n",
      "[Epoch 157/200] [Batch 45/46] [D loss: -1.640511] [G loss: -1.208301]\n",
      "[Epoch 158/200] [Batch 0/46] [D loss: -0.743762] [G loss: -0.307556]\n",
      "[Epoch 158/200] [Batch 5/46] [D loss: -3.213701] [G loss: -2.610937]\n",
      "[Epoch 158/200] [Batch 10/46] [D loss: -0.910532] [G loss: -2.232549]\n",
      "[Epoch 158/200] [Batch 15/46] [D loss: -1.046394] [G loss: -2.074427]\n",
      "[Epoch 158/200] [Batch 20/46] [D loss: -1.617055] [G loss: -2.944411]\n",
      "[Epoch 158/200] [Batch 25/46] [D loss: -1.705954] [G loss: -4.462200]\n",
      "[Epoch 158/200] [Batch 30/46] [D loss: -1.231801] [G loss: -3.818126]\n",
      "[Epoch 158/200] [Batch 35/46] [D loss: -1.090431] [G loss: -4.358780]\n",
      "[Epoch 158/200] [Batch 40/46] [D loss: -1.874059] [G loss: -3.253761]\n",
      "[Epoch 158/200] [Batch 45/46] [D loss: 0.019706] [G loss: -3.762969]\n",
      "[Epoch 159/200] [Batch 0/46] [D loss: -0.576523] [G loss: -2.353850]\n",
      "[Epoch 159/200] [Batch 5/46] [D loss: -0.673772] [G loss: -2.954584]\n",
      "[Epoch 159/200] [Batch 10/46] [D loss: -1.238695] [G loss: -3.164206]\n",
      "[Epoch 159/200] [Batch 15/46] [D loss: -2.096762] [G loss: -3.705659]\n",
      "[Epoch 159/200] [Batch 20/46] [D loss: -2.162388] [G loss: -0.976481]\n",
      "[Epoch 159/200] [Batch 25/46] [D loss: -1.641578] [G loss: 0.647269]\n",
      "[Epoch 159/200] [Batch 30/46] [D loss: -1.543132] [G loss: 1.325498]\n",
      "[Epoch 159/200] [Batch 35/46] [D loss: -2.060619] [G loss: 2.087086]\n",
      "[Epoch 159/200] [Batch 40/46] [D loss: -1.078595] [G loss: 2.973087]\n",
      "[Epoch 159/200] [Batch 45/46] [D loss: -1.668370] [G loss: 3.619730]\n",
      "[Epoch 160/200] [Batch 0/46] [D loss: -2.086161] [G loss: 3.031969]\n",
      "[Epoch 160/200] [Batch 5/46] [D loss: -1.479114] [G loss: 3.581805]\n",
      "[Epoch 160/200] [Batch 10/46] [D loss: -1.240443] [G loss: 4.243590]\n",
      "[Epoch 160/200] [Batch 15/46] [D loss: -2.077044] [G loss: 2.483014]\n",
      "[Epoch 160/200] [Batch 20/46] [D loss: -0.971737] [G loss: 1.796043]\n",
      "[Epoch 160/200] [Batch 25/46] [D loss: -1.765468] [G loss: 1.642733]\n",
      "[Epoch 160/200] [Batch 30/46] [D loss: -0.858581] [G loss: -0.287742]\n",
      "[Epoch 160/200] [Batch 35/46] [D loss: -1.235340] [G loss: -2.445463]\n",
      "[Epoch 160/200] [Batch 40/46] [D loss: -0.864210] [G loss: -2.011544]\n",
      "[Epoch 160/200] [Batch 45/46] [D loss: -1.460871] [G loss: -2.624280]\n",
      "[Epoch 161/200] [Batch 0/46] [D loss: -1.962482] [G loss: -2.341599]\n",
      "[Epoch 161/200] [Batch 5/46] [D loss: -2.301455] [G loss: -1.826539]\n",
      "[Epoch 161/200] [Batch 10/46] [D loss: 0.031273] [G loss: -2.128494]\n",
      "[Epoch 161/200] [Batch 15/46] [D loss: -2.700675] [G loss: -2.012310]\n",
      "[Epoch 161/200] [Batch 20/46] [D loss: -1.052524] [G loss: -3.503925]\n",
      "[Epoch 161/200] [Batch 25/46] [D loss: -1.515913] [G loss: -2.091767]\n",
      "[Epoch 161/200] [Batch 30/46] [D loss: -1.405613] [G loss: -2.141824]\n",
      "[Epoch 161/200] [Batch 35/46] [D loss: -1.405213] [G loss: -1.807321]\n",
      "[Epoch 161/200] [Batch 40/46] [D loss: -0.309153] [G loss: -1.910484]\n",
      "[Epoch 161/200] [Batch 45/46] [D loss: -2.462698] [G loss: -4.332663]\n",
      "[Epoch 162/200] [Batch 0/46] [D loss: -0.127527] [G loss: -4.250443]\n",
      "[Epoch 162/200] [Batch 5/46] [D loss: -0.816853] [G loss: -4.584442]\n",
      "[Epoch 162/200] [Batch 10/46] [D loss: -0.470399] [G loss: -3.123814]\n",
      "[Epoch 162/200] [Batch 15/46] [D loss: -2.329823] [G loss: -3.282942]\n",
      "[Epoch 162/200] [Batch 20/46] [D loss: 0.430824] [G loss: -4.471850]\n",
      "[Epoch 162/200] [Batch 25/46] [D loss: 0.033558] [G loss: -4.101645]\n",
      "[Epoch 162/200] [Batch 30/46] [D loss: -2.721314] [G loss: -2.934093]\n",
      "[Epoch 162/200] [Batch 35/46] [D loss: -0.324958] [G loss: -1.616981]\n",
      "[Epoch 162/200] [Batch 40/46] [D loss: -1.835360] [G loss: -2.887568]\n",
      "[Epoch 162/200] [Batch 45/46] [D loss: -0.766887] [G loss: -0.687670]\n",
      "[Epoch 163/200] [Batch 0/46] [D loss: -0.934174] [G loss: -0.605108]\n",
      "[Epoch 163/200] [Batch 5/46] [D loss: -0.172742] [G loss: -1.790654]\n",
      "[Epoch 163/200] [Batch 10/46] [D loss: -2.121385] [G loss: -1.500861]\n",
      "[Epoch 163/200] [Batch 15/46] [D loss: -1.480143] [G loss: -0.362858]\n",
      "[Epoch 163/200] [Batch 20/46] [D loss: -2.053368] [G loss: -0.116459]\n",
      "[Epoch 163/200] [Batch 25/46] [D loss: -2.316556] [G loss: 0.847449]\n",
      "[Epoch 163/200] [Batch 30/46] [D loss: -1.278455] [G loss: 0.523208]\n",
      "[Epoch 163/200] [Batch 35/46] [D loss: -1.816907] [G loss: 1.809756]\n",
      "[Epoch 163/200] [Batch 40/46] [D loss: -2.203731] [G loss: 1.757256]\n",
      "[Epoch 163/200] [Batch 45/46] [D loss: -1.036787] [G loss: 1.172869]\n",
      "[Epoch 164/200] [Batch 0/46] [D loss: -1.935427] [G loss: -0.074871]\n",
      "[Epoch 164/200] [Batch 5/46] [D loss: -0.942579] [G loss: 0.014423]\n",
      "[Epoch 164/200] [Batch 10/46] [D loss: -0.338055] [G loss: -1.837547]\n",
      "[Epoch 164/200] [Batch 15/46] [D loss: -1.541269] [G loss: -3.280442]\n",
      "[Epoch 164/200] [Batch 20/46] [D loss: -1.527721] [G loss: -1.013263]\n",
      "[Epoch 164/200] [Batch 25/46] [D loss: -0.719880] [G loss: -0.124590]\n",
      "[Epoch 164/200] [Batch 30/46] [D loss: -2.395823] [G loss: -1.708724]\n",
      "[Epoch 164/200] [Batch 35/46] [D loss: -0.163759] [G loss: -1.853365]\n",
      "[Epoch 164/200] [Batch 40/46] [D loss: -1.538171] [G loss: -4.055636]\n",
      "[Epoch 164/200] [Batch 45/46] [D loss: -1.986546] [G loss: -4.104299]\n",
      "[Epoch 165/200] [Batch 0/46] [D loss: -0.914927] [G loss: -3.239551]\n",
      "[Epoch 165/200] [Batch 5/46] [D loss: -1.925326] [G loss: -3.469706]\n",
      "[Epoch 165/200] [Batch 10/46] [D loss: -2.163791] [G loss: -3.012604]\n",
      "[Epoch 165/200] [Batch 15/46] [D loss: -1.149907] [G loss: -1.590767]\n",
      "[Epoch 165/200] [Batch 20/46] [D loss: -0.361956] [G loss: -1.659150]\n",
      "[Epoch 165/200] [Batch 25/46] [D loss: -2.085246] [G loss: 0.099409]\n",
      "[Epoch 165/200] [Batch 30/46] [D loss: -0.747486] [G loss: 0.002552]\n",
      "[Epoch 165/200] [Batch 35/46] [D loss: -0.872747] [G loss: 1.293454]\n",
      "[Epoch 165/200] [Batch 40/46] [D loss: -0.522636] [G loss: 2.847866]\n",
      "[Epoch 165/200] [Batch 45/46] [D loss: -1.514943] [G loss: 1.829586]\n",
      "[Epoch 166/200] [Batch 0/46] [D loss: -0.334575] [G loss: 1.375054]\n",
      "[Epoch 166/200] [Batch 5/46] [D loss: -1.055946] [G loss: 1.164508]\n",
      "[Epoch 166/200] [Batch 10/46] [D loss: -1.486723] [G loss: -0.128037]\n",
      "[Epoch 166/200] [Batch 15/46] [D loss: -1.351243] [G loss: -0.711914]\n",
      "[Epoch 166/200] [Batch 20/46] [D loss: -0.777190] [G loss: -0.799826]\n",
      "[Epoch 166/200] [Batch 25/46] [D loss: -0.602459] [G loss: -1.121390]\n",
      "[Epoch 166/200] [Batch 30/46] [D loss: -0.125555] [G loss: -2.378616]\n",
      "[Epoch 166/200] [Batch 35/46] [D loss: 0.300244] [G loss: -1.845670]\n",
      "[Epoch 166/200] [Batch 40/46] [D loss: -1.633210] [G loss: -2.083915]\n",
      "[Epoch 166/200] [Batch 45/46] [D loss: -0.375201] [G loss: 0.190537]\n",
      "[Epoch 167/200] [Batch 5/46] [D loss: -1.439648] [G loss: -0.465908]\n",
      "[Epoch 167/200] [Batch 10/46] [D loss: -1.600866] [G loss: 0.644249]\n",
      "[Epoch 167/200] [Batch 15/46] [D loss: -0.582767] [G loss: -0.194681]\n",
      "[Epoch 167/200] [Batch 20/46] [D loss: -2.247623] [G loss: 0.495536]\n",
      "[Epoch 167/200] [Batch 25/46] [D loss: -0.329360] [G loss: -0.336007]\n",
      "[Epoch 167/200] [Batch 30/46] [D loss: -1.527163] [G loss: 0.691494]\n",
      "[Epoch 167/200] [Batch 35/46] [D loss: -1.431371] [G loss: -0.205126]\n",
      "[Epoch 167/200] [Batch 40/46] [D loss: -1.995219] [G loss: 0.527345]\n",
      "[Epoch 167/200] [Batch 45/46] [D loss: -2.414478] [G loss: -0.100531]\n",
      "[Epoch 168/200] [Batch 0/46] [D loss: -1.679622] [G loss: -0.982161]\n",
      "[Epoch 168/200] [Batch 5/46] [D loss: -1.539251] [G loss: -1.390129]\n",
      "[Epoch 168/200] [Batch 10/46] [D loss: -1.805686] [G loss: -1.061779]\n",
      "[Epoch 168/200] [Batch 15/46] [D loss: -0.949564] [G loss: -0.967795]\n",
      "[Epoch 168/200] [Batch 20/46] [D loss: -2.127801] [G loss: -1.228316]\n",
      "[Epoch 168/200] [Batch 25/46] [D loss: -1.581746] [G loss: -0.263459]\n",
      "[Epoch 168/200] [Batch 30/46] [D loss: -1.985803] [G loss: -2.073088]\n",
      "[Epoch 168/200] [Batch 35/46] [D loss: -2.370672] [G loss: -2.659090]\n",
      "[Epoch 168/200] [Batch 40/46] [D loss: -1.148009] [G loss: -1.879156]\n",
      "[Epoch 168/200] [Batch 45/46] [D loss: -1.820250] [G loss: -2.103930]\n",
      "[Epoch 169/200] [Batch 0/46] [D loss: -2.214546] [G loss: -1.236879]\n",
      "[Epoch 169/200] [Batch 5/46] [D loss: -1.015904] [G loss: -3.248929]\n",
      "[Epoch 169/200] [Batch 10/46] [D loss: -1.425696] [G loss: -3.155709]\n",
      "[Epoch 169/200] [Batch 15/46] [D loss: -1.136672] [G loss: -2.913064]\n",
      "[Epoch 169/200] [Batch 20/46] [D loss: -1.233588] [G loss: -2.332121]\n",
      "[Epoch 169/200] [Batch 25/46] [D loss: -0.177308] [G loss: -2.009212]\n",
      "[Epoch 169/200] [Batch 30/46] [D loss: -1.100379] [G loss: -1.944821]\n",
      "[Epoch 169/200] [Batch 35/46] [D loss: -0.658712] [G loss: -1.950029]\n",
      "[Epoch 169/200] [Batch 40/46] [D loss: -0.549122] [G loss: -0.869775]\n",
      "[Epoch 169/200] [Batch 45/46] [D loss: -1.909106] [G loss: -1.661669]\n",
      "[Epoch 170/200] [Batch 0/46] [D loss: -0.657131] [G loss: -1.692710]\n",
      "[Epoch 170/200] [Batch 5/46] [D loss: -2.020661] [G loss: -1.009498]\n",
      "[Epoch 170/200] [Batch 10/46] [D loss: -1.903706] [G loss: 0.800713]\n",
      "[Epoch 170/200] [Batch 15/46] [D loss: -1.264489] [G loss: 0.104615]\n",
      "[Epoch 170/200] [Batch 20/46] [D loss: -1.237512] [G loss: -0.350597]\n",
      "[Epoch 170/200] [Batch 25/46] [D loss: 0.187292] [G loss: -0.964394]\n",
      "[Epoch 170/200] [Batch 30/46] [D loss: -1.832018] [G loss: -1.543917]\n",
      "[Epoch 170/200] [Batch 35/46] [D loss: -2.627530] [G loss: -1.580549]\n",
      "[Epoch 170/200] [Batch 40/46] [D loss: -2.422366] [G loss: -2.528403]\n",
      "[Epoch 170/200] [Batch 45/46] [D loss: -1.887670] [G loss: -3.188768]\n",
      "[Epoch 171/200] [Batch 0/46] [D loss: -1.772490] [G loss: -5.037489]\n",
      "[Epoch 171/200] [Batch 5/46] [D loss: -0.800798] [G loss: -1.574594]\n",
      "[Epoch 171/200] [Batch 10/46] [D loss: -0.607798] [G loss: -0.436982]\n",
      "[Epoch 171/200] [Batch 15/46] [D loss: -1.036296] [G loss: 1.333353]\n",
      "[Epoch 171/200] [Batch 20/46] [D loss: -1.517360] [G loss: 1.039710]\n",
      "[Epoch 171/200] [Batch 25/46] [D loss: -1.063181] [G loss: 1.614920]\n",
      "[Epoch 171/200] [Batch 30/46] [D loss: -1.449321] [G loss: 0.235880]\n",
      "[Epoch 171/200] [Batch 35/46] [D loss: -0.942902] [G loss: -0.558367]\n",
      "[Epoch 171/200] [Batch 40/46] [D loss: -0.796250] [G loss: -1.251366]\n",
      "[Epoch 171/200] [Batch 45/46] [D loss: 0.745461] [G loss: -1.483712]\n",
      "[Epoch 172/200] [Batch 0/46] [D loss: -2.169239] [G loss: -2.091402]\n",
      "[Epoch 172/200] [Batch 5/46] [D loss: -0.871128] [G loss: -1.206104]\n",
      "[Epoch 172/200] [Batch 10/46] [D loss: -1.843144] [G loss: 0.448404]\n",
      "[Epoch 172/200] [Batch 15/46] [D loss: -2.730309] [G loss: 0.941756]\n",
      "[Epoch 172/200] [Batch 20/46] [D loss: -0.996632] [G loss: 1.294342]\n",
      "[Epoch 172/200] [Batch 25/46] [D loss: -1.572266] [G loss: 1.169107]\n",
      "[Epoch 172/200] [Batch 30/46] [D loss: -0.771563] [G loss: 2.640475]\n",
      "[Epoch 172/200] [Batch 35/46] [D loss: -0.832752] [G loss: 1.976188]\n",
      "[Epoch 172/200] [Batch 40/46] [D loss: -1.258721] [G loss: 2.190952]\n",
      "[Epoch 172/200] [Batch 45/46] [D loss: -0.374603] [G loss: 0.522048]\n",
      "[Epoch 173/200] [Batch 0/46] [D loss: -2.089078] [G loss: 0.603388]\n",
      "[Epoch 173/200] [Batch 5/46] [D loss: -1.160421] [G loss: 0.949606]\n",
      "[Epoch 173/200] [Batch 10/46] [D loss: -2.033683] [G loss: 0.784283]\n",
      "[Epoch 173/200] [Batch 15/46] [D loss: -1.006563] [G loss: 0.343199]\n",
      "[Epoch 173/200] [Batch 20/46] [D loss: -2.125959] [G loss: -1.724751]\n",
      "[Epoch 173/200] [Batch 25/46] [D loss: -1.652549] [G loss: -1.875347]\n",
      "[Epoch 173/200] [Batch 30/46] [D loss: -1.730255] [G loss: -2.236914]\n",
      "[Epoch 173/200] [Batch 35/46] [D loss: 0.034594] [G loss: -2.817504]\n",
      "[Epoch 173/200] [Batch 40/46] [D loss: -0.991156] [G loss: -2.440952]\n",
      "[Epoch 173/200] [Batch 45/46] [D loss: -1.119589] [G loss: -2.858762]\n",
      "[Epoch 174/200] [Batch 0/46] [D loss: -1.606459] [G loss: -4.338336]\n",
      "[Epoch 174/200] [Batch 5/46] [D loss: -1.353173] [G loss: -3.471535]\n",
      "[Epoch 174/200] [Batch 10/46] [D loss: -1.320945] [G loss: -3.225774]\n",
      "[Epoch 174/200] [Batch 15/46] [D loss: -0.640246] [G loss: -2.828138]\n",
      "[Epoch 174/200] [Batch 20/46] [D loss: -1.755973] [G loss: -2.442659]\n",
      "[Epoch 174/200] [Batch 25/46] [D loss: -1.625647] [G loss: -0.431419]\n",
      "[Epoch 174/200] [Batch 30/46] [D loss: -1.007938] [G loss: 1.041266]\n",
      "[Epoch 174/200] [Batch 35/46] [D loss: -1.394760] [G loss: 1.881766]\n",
      "[Epoch 174/200] [Batch 40/46] [D loss: 0.460266] [G loss: 2.609026]\n",
      "[Epoch 174/200] [Batch 45/46] [D loss: -0.242562] [G loss: 1.646350]\n",
      "[Epoch 175/200] [Batch 0/46] [D loss: -0.675749] [G loss: 1.835385]\n",
      "[Epoch 175/200] [Batch 5/46] [D loss: -2.331354] [G loss: 0.746699]\n",
      "[Epoch 175/200] [Batch 10/46] [D loss: -0.251874] [G loss: 1.196103]\n",
      "[Epoch 175/200] [Batch 15/46] [D loss: -2.421341] [G loss: 0.538426]\n",
      "[Epoch 175/200] [Batch 20/46] [D loss: -2.218786] [G loss: 0.596585]\n",
      "[Epoch 175/200] [Batch 25/46] [D loss: -1.368630] [G loss: 0.615458]\n",
      "[Epoch 175/200] [Batch 30/46] [D loss: -2.545516] [G loss: -0.563083]\n",
      "[Epoch 175/200] [Batch 35/46] [D loss: -1.343045] [G loss: 0.099484]\n",
      "[Epoch 175/200] [Batch 40/46] [D loss: -2.613465] [G loss: -1.972280]\n",
      "[Epoch 175/200] [Batch 45/46] [D loss: -2.044579] [G loss: -0.360619]\n",
      "[Epoch 176/200] [Batch 0/46] [D loss: -0.907369] [G loss: -0.843428]\n",
      "[Epoch 176/200] [Batch 5/46] [D loss: -0.844968] [G loss: -1.456178]\n",
      "[Epoch 176/200] [Batch 10/46] [D loss: -0.270848] [G loss: -0.456867]\n",
      "[Epoch 176/200] [Batch 15/46] [D loss: -1.638698] [G loss: -2.501493]\n",
      "[Epoch 176/200] [Batch 20/46] [D loss: -1.004701] [G loss: -0.346839]\n",
      "[Epoch 176/200] [Batch 25/46] [D loss: -0.751225] [G loss: -0.492445]\n",
      "[Epoch 176/200] [Batch 30/46] [D loss: -1.376231] [G loss: 0.201191]\n",
      "[Epoch 176/200] [Batch 35/46] [D loss: -2.887824] [G loss: -0.733343]\n",
      "[Epoch 176/200] [Batch 40/46] [D loss: -2.291681] [G loss: 0.773201]\n",
      "[Epoch 176/200] [Batch 45/46] [D loss: -2.074221] [G loss: 0.881551]\n",
      "[Epoch 177/200] [Batch 0/46] [D loss: -1.705189] [G loss: 0.212499]\n",
      "[Epoch 177/200] [Batch 5/46] [D loss: -0.275397] [G loss: 0.834978]\n",
      "[Epoch 177/200] [Batch 10/46] [D loss: -1.141338] [G loss: 0.389259]\n",
      "[Epoch 177/200] [Batch 15/46] [D loss: -1.653060] [G loss: 1.420325]\n",
      "[Epoch 177/200] [Batch 20/46] [D loss: -0.827897] [G loss: -0.099148]\n",
      "[Epoch 177/200] [Batch 25/46] [D loss: -1.473510] [G loss: 0.508199]\n",
      "[Epoch 177/200] [Batch 30/46] [D loss: -1.470511] [G loss: 0.421285]\n",
      "[Epoch 177/200] [Batch 35/46] [D loss: -0.162347] [G loss: -0.941100]\n",
      "[Epoch 177/200] [Batch 40/46] [D loss: -0.379751] [G loss: -2.649276]\n",
      "[Epoch 177/200] [Batch 45/46] [D loss: -1.455621] [G loss: -0.959242]\n",
      "[Epoch 178/200] [Batch 0/46] [D loss: -2.382890] [G loss: -0.044229]\n",
      "[Epoch 178/200] [Batch 5/46] [D loss: -0.464463] [G loss: -0.628691]\n",
      "[Epoch 178/200] [Batch 10/46] [D loss: -2.025929] [G loss: 1.190570]\n",
      "[Epoch 178/200] [Batch 15/46] [D loss: -0.950583] [G loss: -1.774548]\n",
      "[Epoch 178/200] [Batch 20/46] [D loss: -1.702936] [G loss: -3.110296]\n",
      "[Epoch 178/200] [Batch 25/46] [D loss: 0.172024] [G loss: -2.382967]\n",
      "[Epoch 178/200] [Batch 30/46] [D loss: -0.406837] [G loss: -2.896836]\n",
      "[Epoch 178/200] [Batch 35/46] [D loss: -1.352643] [G loss: 0.062484]\n",
      "[Epoch 178/200] [Batch 40/46] [D loss: -1.683371] [G loss: 0.837852]\n",
      "[Epoch 178/200] [Batch 45/46] [D loss: -1.498474] [G loss: 1.845986]\n",
      "[Epoch 179/200] [Batch 0/46] [D loss: -1.144104] [G loss: 0.583458]\n",
      "[Epoch 179/200] [Batch 5/46] [D loss: -0.710145] [G loss: 1.413200]\n",
      "[Epoch 179/200] [Batch 10/46] [D loss: -1.522176] [G loss: 1.768161]\n",
      "[Epoch 179/200] [Batch 15/46] [D loss: -2.957630] [G loss: -0.121239]\n",
      "[Epoch 179/200] [Batch 20/46] [D loss: -0.303758] [G loss: -1.060589]\n",
      "[Epoch 179/200] [Batch 25/46] [D loss: 0.633353] [G loss: -1.234934]\n",
      "[Epoch 179/200] [Batch 30/46] [D loss: -0.077999] [G loss: 0.371648]\n",
      "[Epoch 179/200] [Batch 35/46] [D loss: -1.284060] [G loss: -1.948968]\n",
      "[Epoch 179/200] [Batch 40/46] [D loss: -1.054812] [G loss: -1.694751]\n",
      "[Epoch 179/200] [Batch 45/46] [D loss: -2.942772] [G loss: -1.351519]\n",
      "[Epoch 180/200] [Batch 0/46] [D loss: -0.789502] [G loss: -2.093544]\n",
      "[Epoch 180/200] [Batch 5/46] [D loss: -1.566802] [G loss: -2.679733]\n",
      "[Epoch 180/200] [Batch 10/46] [D loss: -1.178144] [G loss: 0.147769]\n",
      "[Epoch 180/200] [Batch 15/46] [D loss: -0.923870] [G loss: -0.192593]\n",
      "[Epoch 180/200] [Batch 20/46] [D loss: -2.104752] [G loss: -0.279186]\n",
      "[Epoch 180/200] [Batch 25/46] [D loss: 0.346857] [G loss: -1.119354]\n",
      "[Epoch 180/200] [Batch 30/46] [D loss: -0.419017] [G loss: 1.096232]\n",
      "[Epoch 180/200] [Batch 35/46] [D loss: -1.071424] [G loss: 1.585346]\n",
      "[Epoch 180/200] [Batch 40/46] [D loss: -1.276240] [G loss: 2.486707]\n",
      "[Epoch 180/200] [Batch 45/46] [D loss: -2.341659] [G loss: 0.932487]\n",
      "[Epoch 181/200] [Batch 0/46] [D loss: -0.590967] [G loss: 2.823154]\n",
      "[Epoch 181/200] [Batch 5/46] [D loss: -1.543507] [G loss: 1.962625]\n",
      "[Epoch 181/200] [Batch 10/46] [D loss: -2.137533] [G loss: 0.571386]\n",
      "[Epoch 181/200] [Batch 15/46] [D loss: -1.021050] [G loss: 0.331087]\n",
      "[Epoch 181/200] [Batch 20/46] [D loss: -2.356682] [G loss: 0.244192]\n",
      "[Epoch 181/200] [Batch 25/46] [D loss: -1.632468] [G loss: 0.010082]\n",
      "[Epoch 181/200] [Batch 30/46] [D loss: -1.639065] [G loss: -1.256420]\n",
      "[Epoch 181/200] [Batch 35/46] [D loss: -2.158463] [G loss: -1.601890]\n",
      "[Epoch 181/200] [Batch 40/46] [D loss: -1.282679] [G loss: -3.031467]\n",
      "[Epoch 181/200] [Batch 45/46] [D loss: -0.855047] [G loss: -3.749440]\n",
      "[Epoch 182/200] [Batch 0/46] [D loss: -1.619812] [G loss: -3.439993]\n",
      "[Epoch 182/200] [Batch 5/46] [D loss: -2.384843] [G loss: -3.525587]\n",
      "[Epoch 182/200] [Batch 10/46] [D loss: -1.971069] [G loss: -4.215964]\n",
      "[Epoch 182/200] [Batch 15/46] [D loss: -1.313525] [G loss: -3.704005]\n",
      "[Epoch 182/200] [Batch 20/46] [D loss: -1.870764] [G loss: -1.923832]\n",
      "[Epoch 182/200] [Batch 25/46] [D loss: -0.808799] [G loss: -0.996619]\n",
      "[Epoch 182/200] [Batch 30/46] [D loss: -1.612884] [G loss: -0.970784]\n",
      "[Epoch 182/200] [Batch 35/46] [D loss: -1.385161] [G loss: 0.382223]\n",
      "[Epoch 182/200] [Batch 40/46] [D loss: 0.725686] [G loss: 1.703063]\n",
      "[Epoch 182/200] [Batch 45/46] [D loss: -0.314289] [G loss: 2.017559]\n",
      "[Epoch 183/200] [Batch 0/46] [D loss: -1.020465] [G loss: 1.339190]\n",
      "[Epoch 183/200] [Batch 5/46] [D loss: -0.457340] [G loss: 1.668440]\n",
      "[Epoch 183/200] [Batch 10/46] [D loss: -1.316017] [G loss: 1.211865]\n",
      "[Epoch 183/200] [Batch 15/46] [D loss: -2.472642] [G loss: -0.577075]\n",
      "[Epoch 183/200] [Batch 20/46] [D loss: -1.769650] [G loss: -1.000270]\n",
      "[Epoch 183/200] [Batch 25/46] [D loss: -0.826497] [G loss: -1.830073]\n",
      "[Epoch 183/200] [Batch 30/46] [D loss: -1.926730] [G loss: -0.938802]\n",
      "[Epoch 183/200] [Batch 35/46] [D loss: -3.258724] [G loss: -0.855241]\n",
      "[Epoch 183/200] [Batch 40/46] [D loss: -1.936609] [G loss: -0.769838]\n",
      "[Epoch 183/200] [Batch 45/46] [D loss: -1.685460] [G loss: -1.221332]\n",
      "[Epoch 184/200] [Batch 0/46] [D loss: -1.413833] [G loss: -0.138886]\n",
      "[Epoch 184/200] [Batch 5/46] [D loss: -1.291947] [G loss: 0.249802]\n",
      "[Epoch 184/200] [Batch 10/46] [D loss: -2.111262] [G loss: -0.649570]\n",
      "[Epoch 184/200] [Batch 15/46] [D loss: -3.138407] [G loss: -0.012554]\n",
      "[Epoch 184/200] [Batch 20/46] [D loss: -2.072149] [G loss: -1.908720]\n",
      "[Epoch 184/200] [Batch 25/46] [D loss: -0.454931] [G loss: -1.303095]\n",
      "[Epoch 184/200] [Batch 30/46] [D loss: -0.834472] [G loss: -0.871011]\n",
      "[Epoch 184/200] [Batch 35/46] [D loss: -1.053018] [G loss: -0.417419]\n",
      "[Epoch 184/200] [Batch 40/46] [D loss: -2.204845] [G loss: -1.012598]\n",
      "[Epoch 184/200] [Batch 45/46] [D loss: -2.653955] [G loss: -1.787738]\n",
      "[Epoch 185/200] [Batch 0/46] [D loss: -0.868186] [G loss: -2.082241]\n",
      "[Epoch 185/200] [Batch 5/46] [D loss: -1.052416] [G loss: -0.847737]\n",
      "[Epoch 185/200] [Batch 10/46] [D loss: -1.332338] [G loss: -1.114363]\n",
      "[Epoch 185/200] [Batch 15/46] [D loss: -0.331980] [G loss: 0.965837]\n",
      "[Epoch 185/200] [Batch 20/46] [D loss: -1.416900] [G loss: 0.702712]\n",
      "[Epoch 185/200] [Batch 25/46] [D loss: -0.871703] [G loss: 0.342150]\n",
      "[Epoch 185/200] [Batch 30/46] [D loss: -0.918008] [G loss: 1.109403]\n",
      "[Epoch 185/200] [Batch 35/46] [D loss: -1.226204] [G loss: 0.838030]\n",
      "[Epoch 185/200] [Batch 40/46] [D loss: -1.206671] [G loss: 0.891502]\n",
      "[Epoch 185/200] [Batch 45/46] [D loss: 0.058149] [G loss: -0.973170]\n",
      "[Epoch 186/200] [Batch 0/46] [D loss: -1.266391] [G loss: -0.856769]\n",
      "[Epoch 186/200] [Batch 5/46] [D loss: -0.916034] [G loss: -1.511595]\n",
      "[Epoch 186/200] [Batch 10/46] [D loss: -2.079769] [G loss: -2.122383]\n",
      "[Epoch 186/200] [Batch 15/46] [D loss: -1.118334] [G loss: -3.096879]\n",
      "[Epoch 186/200] [Batch 20/46] [D loss: -1.002913] [G loss: -1.589886]\n",
      "[Epoch 186/200] [Batch 25/46] [D loss: -2.902319] [G loss: -3.180285]\n",
      "[Epoch 186/200] [Batch 30/46] [D loss: -0.813788] [G loss: -1.369051]\n",
      "[Epoch 186/200] [Batch 35/46] [D loss: -0.955322] [G loss: -0.134544]\n",
      "[Epoch 186/200] [Batch 40/46] [D loss: -1.080926] [G loss: -0.312896]\n",
      "[Epoch 186/200] [Batch 45/46] [D loss: -1.294462] [G loss: -0.777068]\n",
      "[Epoch 187/200] [Batch 0/46] [D loss: -0.169656] [G loss: 0.064557]\n",
      "[Epoch 187/200] [Batch 5/46] [D loss: -2.277143] [G loss: -0.487747]\n",
      "[Epoch 187/200] [Batch 10/46] [D loss: -1.038687] [G loss: -1.166552]\n",
      "[Epoch 187/200] [Batch 15/46] [D loss: -1.923329] [G loss: -0.639376]\n",
      "[Epoch 187/200] [Batch 20/46] [D loss: -0.453650] [G loss: 0.532302]\n",
      "[Epoch 187/200] [Batch 25/46] [D loss: -2.800505] [G loss: 1.286760]\n",
      "[Epoch 187/200] [Batch 30/46] [D loss: -0.831437] [G loss: 1.572175]\n",
      "[Epoch 187/200] [Batch 35/46] [D loss: -2.102656] [G loss: 1.096095]\n",
      "[Epoch 187/200] [Batch 40/46] [D loss: -1.215735] [G loss: 0.674589]\n",
      "[Epoch 187/200] [Batch 45/46] [D loss: -1.429254] [G loss: 0.392294]\n",
      "[Epoch 188/200] [Batch 0/46] [D loss: -0.699793] [G loss: 0.489962]\n",
      "[Epoch 188/200] [Batch 5/46] [D loss: -1.854483] [G loss: 0.893986]\n",
      "[Epoch 188/200] [Batch 10/46] [D loss: -2.250896] [G loss: 0.629179]\n",
      "[Epoch 188/200] [Batch 15/46] [D loss: -1.293885] [G loss: 0.386794]\n",
      "[Epoch 188/200] [Batch 20/46] [D loss: -1.343380] [G loss: -0.519011]\n",
      "[Epoch 188/200] [Batch 25/46] [D loss: -0.846267] [G loss: 0.180962]\n",
      "[Epoch 188/200] [Batch 30/46] [D loss: -1.191473] [G loss: -1.851601]\n",
      "[Epoch 188/200] [Batch 35/46] [D loss: -1.314248] [G loss: -2.165497]\n",
      "[Epoch 188/200] [Batch 40/46] [D loss: -1.538451] [G loss: -2.592699]\n",
      "[Epoch 188/200] [Batch 45/46] [D loss: -0.862736] [G loss: -0.955640]\n",
      "[Epoch 189/200] [Batch 0/46] [D loss: -1.154523] [G loss: -1.242134]\n",
      "[Epoch 189/200] [Batch 5/46] [D loss: -1.453800] [G loss: -0.812983]\n",
      "[Epoch 189/200] [Batch 10/46] [D loss: -1.141308] [G loss: -1.129213]\n",
      "[Epoch 189/200] [Batch 15/46] [D loss: -1.202049] [G loss: -1.052105]\n",
      "[Epoch 189/200] [Batch 20/46] [D loss: -3.470093] [G loss: -2.325508]\n",
      "[Epoch 189/200] [Batch 25/46] [D loss: -3.165478] [G loss: -3.233371]\n",
      "[Epoch 189/200] [Batch 30/46] [D loss: -0.073324] [G loss: 0.367875]\n",
      "[Epoch 189/200] [Batch 35/46] [D loss: -0.674603] [G loss: 0.711588]\n",
      "[Epoch 189/200] [Batch 40/46] [D loss: -0.919325] [G loss: 2.561820]\n",
      "[Epoch 189/200] [Batch 45/46] [D loss: -0.512443] [G loss: 2.451407]\n",
      "[Epoch 190/200] [Batch 0/46] [D loss: -1.598227] [G loss: 2.723448]\n",
      "[Epoch 190/200] [Batch 5/46] [D loss: -1.447113] [G loss: 2.575301]\n",
      "[Epoch 190/200] [Batch 10/46] [D loss: -1.731272] [G loss: 1.919373]\n",
      "[Epoch 190/200] [Batch 15/46] [D loss: -3.103953] [G loss: 1.922965]\n",
      "[Epoch 190/200] [Batch 20/46] [D loss: -0.886699] [G loss: 2.150756]\n",
      "[Epoch 190/200] [Batch 25/46] [D loss: -0.732525] [G loss: -0.287903]\n",
      "[Epoch 190/200] [Batch 30/46] [D loss: -2.149119] [G loss: -1.196273]\n",
      "[Epoch 190/200] [Batch 35/46] [D loss: -2.776739] [G loss: -3.551208]\n",
      "[Epoch 190/200] [Batch 40/46] [D loss: -1.720858] [G loss: -4.774456]\n",
      "[Epoch 190/200] [Batch 45/46] [D loss: -0.990764] [G loss: -1.531929]\n",
      "[Epoch 191/200] [Batch 0/46] [D loss: -1.688142] [G loss: -2.878407]\n",
      "[Epoch 191/200] [Batch 5/46] [D loss: -1.887675] [G loss: -0.678900]\n",
      "[Epoch 191/200] [Batch 10/46] [D loss: -1.848231] [G loss: -0.204266]\n",
      "[Epoch 191/200] [Batch 15/46] [D loss: -1.340315] [G loss: 0.301982]\n",
      "[Epoch 191/200] [Batch 20/46] [D loss: -0.715007] [G loss: -0.996290]\n",
      "[Epoch 191/200] [Batch 25/46] [D loss: -1.772655] [G loss: 1.716069]\n",
      "[Epoch 191/200] [Batch 30/46] [D loss: -2.231583] [G loss: 3.531734]\n",
      "[Epoch 191/200] [Batch 35/46] [D loss: -0.153518] [G loss: 3.238022]\n",
      "[Epoch 191/200] [Batch 40/46] [D loss: -0.543562] [G loss: 3.493586]\n",
      "[Epoch 191/200] [Batch 45/46] [D loss: -3.188948] [G loss: 2.147607]\n",
      "[Epoch 192/200] [Batch 0/46] [D loss: 0.932437] [G loss: 3.540557]\n",
      "[Epoch 192/200] [Batch 5/46] [D loss: -2.616352] [G loss: 1.594520]\n",
      "[Epoch 192/200] [Batch 10/46] [D loss: -1.864053] [G loss: -1.112290]\n",
      "[Epoch 192/200] [Batch 15/46] [D loss: -0.153456] [G loss: -2.073729]\n",
      "[Epoch 192/200] [Batch 20/46] [D loss: -1.498922] [G loss: -2.614372]\n",
      "[Epoch 192/200] [Batch 25/46] [D loss: -2.021724] [G loss: -2.464993]\n",
      "[Epoch 192/200] [Batch 30/46] [D loss: -1.148588] [G loss: -1.576548]\n",
      "[Epoch 192/200] [Batch 35/46] [D loss: -0.662065] [G loss: -0.143079]\n",
      "[Epoch 192/200] [Batch 40/46] [D loss: -1.869632] [G loss: 0.818755]\n",
      "[Epoch 192/200] [Batch 45/46] [D loss: 0.270898] [G loss: 2.988410]\n",
      "[Epoch 193/200] [Batch 0/46] [D loss: -1.189638] [G loss: 1.510231]\n",
      "[Epoch 193/200] [Batch 5/46] [D loss: -1.419212] [G loss: 0.332075]\n",
      "[Epoch 193/200] [Batch 10/46] [D loss: -1.567149] [G loss: 1.103067]\n",
      "[Epoch 193/200] [Batch 15/46] [D loss: -1.371896] [G loss: 1.506765]\n",
      "[Epoch 193/200] [Batch 20/46] [D loss: -0.803315] [G loss: 0.083501]\n",
      "[Epoch 193/200] [Batch 25/46] [D loss: -1.638120] [G loss: 0.385939]\n",
      "[Epoch 193/200] [Batch 30/46] [D loss: -0.854207] [G loss: 0.921692]\n",
      "[Epoch 193/200] [Batch 35/46] [D loss: -1.591038] [G loss: 0.875320]\n",
      "[Epoch 193/200] [Batch 40/46] [D loss: -0.571894] [G loss: 1.251594]\n",
      "[Epoch 193/200] [Batch 45/46] [D loss: -1.321135] [G loss: -0.247280]\n",
      "[Epoch 194/200] [Batch 0/46] [D loss: -0.928052] [G loss: -0.862585]\n",
      "[Epoch 194/200] [Batch 5/46] [D loss: 0.576661] [G loss: 0.417854]\n",
      "[Epoch 194/200] [Batch 10/46] [D loss: -1.358930] [G loss: 1.320321]\n",
      "[Epoch 194/200] [Batch 15/46] [D loss: -0.703133] [G loss: 0.800363]\n",
      "[Epoch 194/200] [Batch 20/46] [D loss: 0.373893] [G loss: 1.630743]\n",
      "[Epoch 194/200] [Batch 25/46] [D loss: -0.830402] [G loss: 1.197099]\n",
      "[Epoch 194/200] [Batch 30/46] [D loss: -1.612980] [G loss: 1.796482]\n",
      "[Epoch 194/200] [Batch 35/46] [D loss: -1.958288] [G loss: 1.213680]\n",
      "[Epoch 194/200] [Batch 40/46] [D loss: -1.486481] [G loss: -0.055946]\n",
      "[Epoch 194/200] [Batch 45/46] [D loss: 1.110296] [G loss: 1.052063]\n",
      "[Epoch 195/200] [Batch 0/46] [D loss: -1.149231] [G loss: 1.297663]\n",
      "[Epoch 195/200] [Batch 5/46] [D loss: -1.997425] [G loss: 1.629178]\n",
      "[Epoch 195/200] [Batch 10/46] [D loss: -2.097232] [G loss: 1.629754]\n",
      "[Epoch 195/200] [Batch 15/46] [D loss: -1.529986] [G loss: -0.415929]\n",
      "[Epoch 195/200] [Batch 20/46] [D loss: -0.942729] [G loss: -1.120643]\n",
      "[Epoch 195/200] [Batch 25/46] [D loss: -1.610324] [G loss: -0.352974]\n",
      "[Epoch 195/200] [Batch 30/46] [D loss: -1.041897] [G loss: -2.329566]\n",
      "[Epoch 195/200] [Batch 35/46] [D loss: -2.173970] [G loss: -2.241444]\n",
      "[Epoch 195/200] [Batch 40/46] [D loss: -0.849623] [G loss: -3.240976]\n",
      "[Epoch 195/200] [Batch 45/46] [D loss: -0.845924] [G loss: -2.189324]\n",
      "[Epoch 196/200] [Batch 0/46] [D loss: 0.123609] [G loss: -3.278460]\n",
      "[Epoch 196/200] [Batch 5/46] [D loss: -2.058228] [G loss: -2.144011]\n",
      "[Epoch 196/200] [Batch 10/46] [D loss: -1.634346] [G loss: -0.817599]\n",
      "[Epoch 196/200] [Batch 15/46] [D loss: 0.373117] [G loss: -1.403716]\n",
      "[Epoch 196/200] [Batch 20/46] [D loss: -0.271561] [G loss: -0.143628]\n",
      "[Epoch 196/200] [Batch 25/46] [D loss: -1.256292] [G loss: 1.097439]\n",
      "[Epoch 196/200] [Batch 30/46] [D loss: -1.049981] [G loss: -0.433338]\n",
      "[Epoch 196/200] [Batch 35/46] [D loss: -1.676656] [G loss: -0.508259]\n",
      "[Epoch 196/200] [Batch 40/46] [D loss: -1.655109] [G loss: 1.027761]\n",
      "[Epoch 196/200] [Batch 45/46] [D loss: -0.443868] [G loss: 0.696768]\n",
      "[Epoch 197/200] [Batch 0/46] [D loss: -2.189153] [G loss: 0.971416]\n",
      "[Epoch 197/200] [Batch 5/46] [D loss: -1.261202] [G loss: 1.700910]\n",
      "[Epoch 197/200] [Batch 10/46] [D loss: -1.002175] [G loss: -0.740061]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the Generator and Discriminator classes (as shown above)\n",
    "# from models import Generator, Discriminator\n",
    "\n",
    "class Opt:\n",
    "    n_epochs = 200\n",
    "    batch_size = 128\n",
    "    lr = 0.0002\n",
    "    b1 = 0.5\n",
    "    b2 = 0.999\n",
    "    n_cpu = 8\n",
    "    latent_dim = 100\n",
    "    img_size = 128\n",
    "    channels = 1\n",
    "    sample_interval = 600\n",
    "\n",
    "opt = Opt()\n",
    "\n",
    "os.makedirs('WGANGP_images_ch', exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(f\"Using CUDA: {cuda}\")\n",
    "\n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(opt)\n",
    "discriminator = Discriminator(opt)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = ChestDataset.get_data_loader('/home/e/emandan/ml/datasets/chestXray', opt.batch_size, opt.img_size)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Training loop\n",
    "batches_done = 0\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        fake_imgs = generator(z)\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_imgs = generator(z)\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(fake_imgs.data[:25], 'WGANGP_images_ch/%d.png' % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "        batches_done += 1\n",
    "\n",
    "torch.save(generator.state_dict(), 'gans/wganc_generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'gans/wganc_discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747ac50-9b2c-4a49-99ed-e25cdfaf3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n",
    "\n",
    "\"\"\"\n",
    "Generate images from WGAN-GP for the chestXray Dataset\n",
    "\"\"\"\n",
    "class Opt:\n",
    "    n_epochs = 100\n",
    "    batch_size = 128\n",
    "    lr = 0.0002\n",
    "    b1 = 0.5\n",
    "    b2 = 0.999\n",
    "    n_cpu = 8\n",
    "    latent_dim = 100\n",
    "    img_size = 128\n",
    "    channels = 1\n",
    "    sample_interval = 600\n",
    "    critic_iterations = 10 \n",
    "\n",
    "opt = Opt()\n",
    "\n",
    "generator = Generator(opt)\n",
    "discriminator = Discriminator(opt)\n",
    "\n",
    "# Load the state dictionaries\n",
    "generator.load_state_dict(torch.load('/home/e/emandan/ml/gans/wganc_generator.pth'))\n",
    "discriminator.load_state_dict(torch.load('/home/e/emandan/ml/gans/wganc_discriminator.pth'))\n",
    "\n",
    "# Set the models to evaluation mode\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "# If using CUDA\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "num_images = 5856\n",
    "latent_dim = opt.latent_dim\n",
    "batch_size = opt.batch_size\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# Create a directory to save the generated images\n",
    "os.makedirs('generated_images/WGANGP_ch', exist_ok=True)\n",
    "\n",
    "# Generate images in batches\n",
    "for i in range(0, num_images, batch_size):\n",
    "    # Generate random latent vectors\n",
    "    z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "\n",
    "    # Generate images\n",
    "    gen_imgs = generator(z)\n",
    "\n",
    "    # Save images\n",
    "    for j in range(gen_imgs.size(0)):\n",
    "        save_image(gen_imgs[j], f'generated_images/WGANGP_ch/image_{i+j}.png', normalize=True)\n",
    "\n",
    "print(f\"Generated {num_images} images and saved them to the 'generated_images' directory.\")\n",
    "\n",
    "print(f\"Evaluating images with FID\")\n",
    "\n",
    "from cleanfid import fid\n",
    "score = fid.compute_fid(\"/home/e/emandan/ml/datasets/chestXray\", \"/home/e/emandan/ml/generated_images/WGANGP_ch\")\n",
    "\n",
    "\"\"\"\n",
    "KID\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Calculates the Kernel Inception Distance (KID) to evalulate GANs\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    # If not tqdm is not available, provide a mock version of it\n",
    "    def tqdm(x): return x\n",
    "\n",
    "from models.inception import InceptionV3\n",
    "from models.lenet import LeNet5\n",
    "\n",
    "def get_activations(files, model, batch_size=50, dims=2048,\n",
    "                    cuda=False, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number\n",
    "                     of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    is_numpy = True if type(files[0]) == np.ndarray else False\n",
    "\n",
    "    if len(files) % batch_size != 0:\n",
    "        print(('Warning: number of images is not a multiple of the '\n",
    "               'batch size. Some samples are going to be ignored.'))\n",
    "    if batch_size > len(files):\n",
    "        print(('Warning: batch size is bigger than the data size. '\n",
    "               'Setting batch size to data size'))\n",
    "        batch_size = len(files)\n",
    "\n",
    "    n_batches = len(files) // batch_size\n",
    "    n_used_imgs = n_batches * batch_size\n",
    "\n",
    "    pred_arr = np.empty((n_used_imgs, dims))\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if is_numpy:\n",
    "            images = np.copy(files[start:end]) + 1\n",
    "            images /= 2.\n",
    "        else:\n",
    "            images = [np.array(Image.open(str(f))) for f in files[start:end]]\n",
    "            images = np.stack(images).astype(np.float32) / 255.\n",
    "            # Reshape to (n_images, 3, height, width)\n",
    "            images = images.transpose((0, 3, 1, 2))\n",
    "\n",
    "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "        if pred.shape[2] != 1 or pred.shape[3] != 1:\n",
    "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n",
    "\n",
    "    if verbose:\n",
    "        print('done', np.min(images))\n",
    "\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "def extract_lenet_features(imgs, net):\n",
    "    net.eval()\n",
    "    feats = []\n",
    "    imgs = imgs.reshape([-1, 100] + list(imgs.shape[1:]))\n",
    "    if imgs[0].min() < -0.001:\n",
    "      imgs = (imgs + 1)/2.0\n",
    "    print(imgs.shape, imgs.min(), imgs.max())\n",
    "    imgs = torch.from_numpy(imgs)\n",
    "    for i, images in enumerate(imgs):\n",
    "        feats.append(net.extract_features(images).detach().cpu().numpy())\n",
    "    feats = np.vstack(feats)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def _compute_activations(path, model, batch_size, dims, cuda, model_type):\n",
    "    if not type(path) == np.ndarray:\n",
    "        import glob\n",
    "        jpg = os.path.join(path, '*.jpg')\n",
    "        png = os.path.join(path, '*.png')\n",
    "        path = glob.glob(jpg) + glob.glob(png)\n",
    "        if len(path) > 50000:\n",
    "            import random\n",
    "            random.shuffle(path)\n",
    "            path = path[:50000]\n",
    "    if model_type == 'inception':\n",
    "        act = get_activations(path, model, batch_size, dims, cuda)\n",
    "    elif model_type == 'lenet':\n",
    "        act = extract_lenet_features(path, model)\n",
    "    return act\n",
    "\n",
    "\n",
    "def calculate_kid_given_paths(paths, batch_size, cuda, dims, model_type='inception'):\n",
    "    \"\"\"Calculates the KID of two paths\"\"\"\n",
    "    pths = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            raise RuntimeError('Invalid path: %s' % p)\n",
    "        if os.path.isdir(p):\n",
    "            pths.append(p)\n",
    "        elif p.endswith('.npy'):\n",
    "            np_imgs = np.load(p)\n",
    "            if np_imgs.shape[0] > 50000: np_imgs = np_imgs[np.random.permutation(np.arange(np_imgs.shape[0]))][:50000]\n",
    "            pths.append(np_imgs)\n",
    "\n",
    "    if model_type == 'inception':\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "        model = InceptionV3([block_idx])\n",
    "    elif model_type == 'lenet':\n",
    "        model = LeNet5()\n",
    "        model.load_state_dict(torch.load('/home/e/emandan/ml/models/lenet.pth'))\n",
    "    if cuda:\n",
    "       model.cuda()\n",
    "\n",
    "    act_true = _compute_activations(pths[0], model, batch_size, dims, cuda, model_type)\n",
    "    pths = pths[1:]\n",
    "    results = []\n",
    "    for j, pth in enumerate(pths):\n",
    "        print(paths[j+1])\n",
    "        actj = _compute_activations(pth, model, batch_size, dims, cuda, model_type)\n",
    "        kid_values = polynomial_mmd_averages(act_true, actj, n_subsets=100)\n",
    "        results.append((paths[j+1], kid_values[0].mean(), kid_values[0].std()))\n",
    "    return results\n",
    "\n",
    "def _sqn(arr):\n",
    "    flat = np.ravel(arr)\n",
    "    return flat.dot(flat)\n",
    "\n",
    "\n",
    "def polynomial_mmd_averages(codes_g, codes_r, n_subsets=50, subset_size=1000,\n",
    "                            ret_var=True, output=sys.stdout, **kernel_args):\n",
    "    m = min(codes_g.shape[0], codes_r.shape[0])\n",
    "    mmds = np.zeros(n_subsets)\n",
    "    if ret_var:\n",
    "        vars = np.zeros(n_subsets)\n",
    "    choice = np.random.choice\n",
    "\n",
    "    with tqdm(range(n_subsets), desc='MMD', file=output) as bar:\n",
    "        for i in bar:\n",
    "            g = codes_g[choice(len(codes_g), subset_size, replace=False)]\n",
    "            r = codes_r[choice(len(codes_r), subset_size, replace=False)]\n",
    "            o = polynomial_mmd(g, r, **kernel_args, var_at_m=m, ret_var=ret_var)\n",
    "            if ret_var:\n",
    "                mmds[i], vars[i] = o\n",
    "            else:\n",
    "                mmds[i] = o\n",
    "            bar.set_postfix({'mean': mmds[:i+1].mean()})\n",
    "    return (mmds, vars) if ret_var else mmds\n",
    "\n",
    "\n",
    "def polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1,\n",
    "                   var_at_m=None, ret_var=True):\n",
    "    # use  k(x, y) = (gamma <x, y> + coef0)^degree\n",
    "    # default gamma is 1 / dim\n",
    "    X = codes_g\n",
    "    Y = codes_r\n",
    "\n",
    "    K_XX = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n",
    "    K_YY = polynomial_kernel(Y, degree=degree, gamma=gamma, coef0=coef0)\n",
    "    K_XY = polynomial_kernel(X, Y, degree=degree, gamma=gamma, coef0=coef0)\n",
    "\n",
    "    return _mmd2_and_variance(K_XX, K_XY, K_YY,\n",
    "                              var_at_m=var_at_m, ret_var=ret_var)\n",
    "\n",
    "def _mmd2_and_variance(K_XX, K_XY, K_YY, unit_diagonal=False,\n",
    "                       mmd_est='unbiased', block_size=1024,\n",
    "                       var_at_m=None, ret_var=True):\n",
    "    # based on\n",
    "    # https://github.com/dougalsutherland/opt-mmd/blob/master/two_sample/mmd.py\n",
    "    # but changed to not compute the full kernel matrix at once\n",
    "    m = K_XX.shape[0]\n",
    "    assert K_XX.shape == (m, m)\n",
    "    assert K_XY.shape == (m, m)\n",
    "    assert K_YY.shape == (m, m)\n",
    "    if var_at_m is None:\n",
    "        var_at_m = m\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if unit_diagonal:\n",
    "        diag_X = diag_Y = 1\n",
    "        sum_diag_X = sum_diag_Y = m\n",
    "        sum_diag2_X = sum_diag2_Y = m\n",
    "    else:\n",
    "        diag_X = np.diagonal(K_XX)\n",
    "        diag_Y = np.diagonal(K_YY)\n",
    "\n",
    "        sum_diag_X = diag_X.sum()\n",
    "        sum_diag_Y = diag_Y.sum()\n",
    "\n",
    "        sum_diag2_X = _sqn(diag_X)\n",
    "        sum_diag2_Y = _sqn(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(axis=1) - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(axis=1) - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(axis=0)\n",
    "    K_XY_sums_1 = K_XY.sum(axis=1)\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()\n",
    "    K_XY_sum = K_XY_sums_0.sum()\n",
    "\n",
    "    if mmd_est == 'biased':\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "                + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "                - 2 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        assert mmd_est in {'unbiased', 'u-statistic'}\n",
    "        mmd2 = (Kt_XX_sum + Kt_YY_sum) / (m * (m-1))\n",
    "        if mmd_est == 'unbiased':\n",
    "            mmd2 -= 2 * K_XY_sum / (m * m)\n",
    "        else:\n",
    "            mmd2 -= 2 * (K_XY_sum - np.trace(K_XY)) / (m * (m-1))\n",
    "\n",
    "    if not ret_var:\n",
    "        return mmd2\n",
    "\n",
    "    Kt_XX_2_sum = _sqn(K_XX) - sum_diag2_X\n",
    "    Kt_YY_2_sum = _sqn(K_YY) - sum_diag2_Y\n",
    "    K_XY_2_sum = _sqn(K_XY)\n",
    "\n",
    "    dot_XX_XY = Kt_XX_sums.dot(K_XY_sums_1)\n",
    "    dot_YY_YX = Kt_YY_sums.dot(K_XY_sums_0)\n",
    "\n",
    "    m1 = m - 1\n",
    "    m2 = m - 2\n",
    "    zeta1_est = (\n",
    "        1 / (m * m1 * m2) * (\n",
    "            _sqn(Kt_XX_sums) - Kt_XX_2_sum + _sqn(Kt_YY_sums) - Kt_YY_2_sum)\n",
    "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 1 / (m * m * m1) * (\n",
    "            _sqn(K_XY_sums_1) + _sqn(K_XY_sums_0) - 2 * K_XY_2_sum)\n",
    "        - 2 / m**4 * K_XY_sum**2\n",
    "        - 2 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
    "        + 2 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "    )\n",
    "    zeta2_est = (\n",
    "        1 / (m * m1) * (Kt_XX_2_sum + Kt_YY_2_sum)\n",
    "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 2 / (m * m) * K_XY_2_sum\n",
    "        - 2 / m**4 * K_XY_sum**2\n",
    "        - 4 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
    "        + 4 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "    )\n",
    "    var_est = (4 * (var_at_m - 2) / (var_at_m * (var_at_m - 1)) * zeta1_est\n",
    "               + 2 / (var_at_m * (var_at_m - 1)) * zeta2_est)\n",
    "\n",
    "    return mmd2, var_est\n",
    "\n",
    "from argparse import ArgumentDefaultsHelpFormatter\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "true_path = '/home/e/emandan/ml/datasets/chestXray'  # Path to the true images\n",
    "fake_paths = ['/home/e/emandan/ml/generated_images/WGANGP_ch']  # Path to the generated images\n",
    "batch_size = 50  # Batch size to use\n",
    "dims = 2048  # Dimensionality of Inception features to use\n",
    "gpu = '0'  # GPU to use (leave blank for CPU only)\n",
    "model = 'inception'  # Model type: 'inception' or 'lenet'\n",
    "\n",
    "# Print the arguments to verify them\n",
    "print(f\"True Path: {true_path}\")\n",
    "print(f\"Fake Paths: {fake_paths}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Dims: {dims}\")\n",
    "print(f\"GPU: {gpu}\")\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "# Combine the paths\n",
    "paths = [true_path] + fake_paths\n",
    "\n",
    "# Debugging: Check if paths contain images\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "    else:\n",
    "        images = os.listdir(path)\n",
    "        if not images:\n",
    "            print(f\"No images found in path: {path}\")\n",
    "        else:\n",
    "            print(f\"Found {len(images)} images in path: {path}\")\n",
    "\n",
    "# Check if TensorFlow is using a GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Calculate KID\n",
    "results = calculate_kid_given_paths(paths, batch_size, gpu != '', dims, model_type=model)\n",
    "\n",
    "# Print the results\n",
    "for p, m, s in results:\n",
    "    print('KID (%s): %.3f (%.3f)' % (p, m, s))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"Defining functions\")\n",
    "def inception_score(images, batch_size=32, splits=10):\n",
    "    N = len(images)\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Load the InceptionV3 model\n",
    "    inception_model = models.inception_v3(pretrained=True, transform_input=False).eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear')\n",
    "\n",
    "    def get_pred(x):\n",
    "        x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return torch.nn.functional.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    # Preprocess images\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch = images[i:i + batch_size]\n",
    "        batch = torch.stack([preprocess(Image.open(img_path)) for img_path in batch], dim=0)\n",
    "        with torch.no_grad():\n",
    "            preds[i:i + batch_size] = get_pred(batch)\n",
    "\n",
    "    # Compute the Inception Score\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k + 1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in tqdm(range(part.shape[0])):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(np.sum(pyx * np.log(pyx / py)))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# Define paths to generated images\n",
    "fake_paths = ['/home/e/emandan/ml/generated_images/WGANGP_ch']\n",
    "\n",
    "print(f\"Gathering image paths\")\n",
    "# Gather all image paths\n",
    "images = []\n",
    "for path in fake_paths:\n",
    "    for img_file in os.listdir(path):\n",
    "        images.append(os.path.join(path, img_file))\n",
    "print(f\"Gathered all image paths..\")\n",
    "\n",
    "# Calculate Inception Score\n",
    "print(f\"Starting the calculation of IS\")\n",
    "mean_score, std_score = inception_score(images)\n",
    "print(f\"Inception Score: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "\n",
    "\n",
    "# Define paths to generated images\n",
    "fake_paths = ['/home/e/emandan/ml/generated_images/WGAN']\n",
    "\n",
    "print(f\"Gathering image paths for WGAN\")\n",
    "# Gather all image paths\n",
    "images = []\n",
    "for path in fake_paths:\n",
    "    for img_file in os.listdir(path):\n",
    "        images.append(os.path.join(path, img_file))\n",
    "print(f\"Gathered all image paths..\")\n",
    "\n",
    "# Calculate Inception Score\n",
    "print(f\"Starting the calculation of IS\")\n",
    "mean_score, std_score = inception_score(images)\n",
    "print(f\"Inception Score: {mean_score:.3f} ± {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e50cb-0377-4546-aeed-699479d5874f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My ML Environment",
   "language": "python",
   "name": "mymlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
