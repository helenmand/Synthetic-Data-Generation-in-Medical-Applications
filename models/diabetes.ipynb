{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2797d8-0d15-48e4-a115-e684d00e4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.sequential import PARSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.evaluation.single_table import evaluate_quality, get_column_plot\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e7360c-1e75-4f52-913b-9027668020e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\",\n",
      "    \"sequence_key\": \"patient_id\",\n",
      "    \"columns\": {\n",
      "        \"datetime\": {\n",
      "            \"sdtype\": \"datetime\"\n",
      "        },\n",
      "        \"patient_id\": {\n",
      "            \"sdtype\": \"id\"\n",
      "        },\n",
      "        \"code\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"value\": {\n",
      "            \"sdtype\": \"categorical\"\n",
      "        }\n",
      "    },\n",
      "    \"sequence_index\": \"datetime\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "metadata = SingleTableMetadata()\n",
    "\n",
    "# Detect metadata from CSV file\n",
    "metadata.detect_from_csv(filepath='datasets/diabetes.csv')\n",
    "\n",
    "metadata.update_column(\n",
    "    column_name='patient_id',\n",
    "    sdtype='id',\n",
    "    #pii='False'\n",
    ")\n",
    "\n",
    "metadata.update_column(\n",
    "    column_name='datetime',\n",
    "    sdtype='datetime',\n",
    ")\n",
    "\n",
    "metadata.set_sequence_key(column_name='patient_id')\n",
    "metadata.set_sequence_index(column_name='datetime')\n",
    "\n",
    "metadata.validate()\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71131e6-0115-4592-acbf-44987dc78de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  1%|▏         | 1/70 [00:00<00:23,  2.88it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  3%|▎         | 2/70 [00:02<01:30,  1.32s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  4%|▍         | 3/70 [00:03<01:23,  1.25s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  6%|▌         | 4/70 [00:04<01:17,  1.17s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  7%|▋         | 5/70 [00:04<00:57,  1.13it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  9%|▊         | 6/70 [00:05<00:57,  1.11it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 10%|█         | 7/70 [00:06<00:58,  1.07it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 11%|█▏        | 8/70 [00:08<01:07,  1.09s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 13%|█▎        | 9/70 [00:08<00:51,  1.18it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 14%|█▍        | 10/70 [00:09<00:48,  1.24it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 16%|█▌        | 11/70 [00:11<01:12,  1.22s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 17%|█▋        | 12/70 [00:11<00:54,  1.07it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 19%|█▊        | 13/70 [00:12<00:48,  1.17it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 20%|██        | 14/70 [00:12<00:42,  1.33it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 21%|██▏       | 15/70 [00:14<00:48,  1.13it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 23%|██▎       | 16/70 [00:14<00:40,  1.32it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 24%|██▍       | 17/70 [00:15<00:38,  1.38it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 26%|██▌       | 18/70 [00:15<00:32,  1.61it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 27%|██▋       | 19/70 [00:16<00:42,  1.21it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 29%|██▊       | 20/70 [00:17<00:40,  1.25it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 30%|███       | 21/70 [00:21<01:17,  1.58s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 31%|███▏      | 22/70 [00:22<01:10,  1.47s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 33%|███▎      | 23/70 [00:23<00:58,  1.25s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 34%|███▍      | 24/70 [00:23<00:48,  1.06s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 36%|███▌      | 25/70 [00:24<00:44,  1.02it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 37%|███▋      | 26/70 [00:25<00:47,  1.08s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 39%|███▊      | 27/70 [00:26<00:48,  1.12s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 40%|████      | 28/70 [00:28<00:49,  1.18s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 41%|████▏     | 29/70 [00:29<00:44,  1.08s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 43%|████▎     | 30/70 [00:30<00:43,  1.08s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 44%|████▍     | 31/70 [00:30<00:38,  1.01it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 46%|████▌     | 32/70 [00:31<00:36,  1.04it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 47%|████▋     | 33/70 [00:35<01:00,  1.63s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 49%|████▊     | 34/70 [00:35<00:45,  1.25s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 50%|█████     | 35/70 [00:35<00:36,  1.04s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 51%|█████▏    | 36/70 [00:37<00:36,  1.08s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 53%|█████▎    | 37/70 [00:37<00:32,  1.01it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 54%|█████▍    | 38/70 [00:39<00:33,  1.05s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 56%|█████▌    | 39/70 [00:39<00:28,  1.10it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 57%|█████▋    | 40/70 [00:41<00:32,  1.07s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 59%|█████▊    | 41/70 [00:41<00:28,  1.00it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 60%|██████    | 42/70 [00:42<00:22,  1.26it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 61%|██████▏   | 43/70 [00:44<00:32,  1.19s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 63%|██████▎   | 44/70 [00:44<00:25,  1.04it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 64%|██████▍   | 45/70 [00:45<00:19,  1.29it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 66%|██████▌   | 46/70 [00:45<00:17,  1.41it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 67%|██████▋   | 47/70 [00:47<00:22,  1.03it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 69%|██████▊   | 48/70 [00:47<00:18,  1.17it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 70%|███████   | 49/70 [00:48<00:16,  1.25it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 71%|███████▏  | 50/70 [00:50<00:20,  1.01s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 73%|███████▎  | 51/70 [00:51<00:19,  1.05s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 74%|███████▍  | 52/70 [00:53<00:24,  1.36s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 76%|███████▌  | 53/70 [00:55<00:25,  1.51s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 77%|███████▋  | 54/70 [00:56<00:22,  1.39s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 79%|███████▊  | 55/70 [00:57<00:18,  1.20s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 80%|████████  | 56/70 [00:57<00:14,  1.00s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 81%|████████▏ | 57/70 [00:59<00:14,  1.13s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 83%|████████▎ | 58/70 [00:59<00:11,  1.09it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 84%|████████▍ | 59/70 [01:00<00:10,  1.05it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 86%|████████▌ | 60/70 [01:02<00:12,  1.29s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 87%|████████▋ | 61/70 [01:03<00:11,  1.32s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 89%|████████▊ | 62/70 [01:05<00:10,  1.33s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 90%|█████████ | 63/70 [01:05<00:07,  1.02s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 91%|█████████▏| 64/70 [01:06<00:05,  1.11it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 93%|█████████▎| 65/70 [01:07<00:04,  1.02it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 94%|█████████▍| 66/70 [01:08<00:04,  1.02s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 96%|█████████▌| 67/70 [01:08<00:02,  1.20it/s]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 97%|█████████▋| 68/70 [01:10<00:02,  1.01s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 99%|█████████▊| 69/70 [01:13<00:01,  1.57s/it]/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "100%|██████████| 70/70 [01:13<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/diabetes.csv\", delimiter=',')\n",
    "\n",
    "data.head()# Create the synthesizer\n",
    "synthesizer = PARSynthesizer(\n",
    "    metadata, # required\n",
    "    enforce_min_max_values=False,\n",
    "    enforce_rounding=False,\n",
    "    epochs=180,\n",
    "    verbose=True,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "# Train the synthesizer\n",
    "#synthesizer.fit(data)\n",
    "#synthesizer.save(filepath='synthesizers/diabetes.pkl')\n",
    "\n",
    "# or load one\n",
    "synthesizer = PARSynthesizer.load(filepath='synthesizers/diabetes.pkl')\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = synthesizer.sample(num_sequences=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9663988-f285-4725-8e03-e8568db0d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.to_csv('synthetic/PARSyntdiabetes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My ML Environment",
   "language": "python",
   "name": "mymlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
