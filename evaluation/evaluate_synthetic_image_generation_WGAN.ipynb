{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e83c01-b76e-4038-9a67-ce0f4a469290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7828 images in the folder /home/e/emandan/ml/datasets/kneeXray/trainf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID trainf : 100%|██████████| 245/245 [01:24<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10112 images in the folder /home/e/emandan/ml/generated_images/WGAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID WGAN : 100%|██████████| 316/316 [01:02<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from cleanfid import fid\n",
    "score = fid.compute_fid(\"/home/e/emandan/ml/datasets/kneeXray/trainf\", \"/home/e/emandan/ml/generated_images/WGAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae1d86a-22e8-44cf-8eec-f74b7eab9d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439.2944827791756"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fae9d4-a454-4352-a61f-25ffbe9bb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Calculates the Kernel Inception Distance (KID) to evalulate GANs\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    # If not tqdm is not available, provide a mock version of it\n",
    "    def tqdm(x): return x\n",
    "\n",
    "from models.inception import InceptionV3\n",
    "from models.lenet import LeNet5\n",
    "\n",
    "def get_activations(files, model, batch_size=50, dims=2048,\n",
    "                    cuda=False, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number\n",
    "                     of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    is_numpy = True if type(files[0]) == np.ndarray else False\n",
    "\n",
    "    if len(files) % batch_size != 0:\n",
    "        print(('Warning: number of images is not a multiple of the '\n",
    "               'batch size. Some samples are going to be ignored.'))\n",
    "    if batch_size > len(files):\n",
    "        print(('Warning: batch size is bigger than the data size. '\n",
    "               'Setting batch size to data size'))\n",
    "        batch_size = len(files)\n",
    "\n",
    "    n_batches = len(files) // batch_size\n",
    "    n_used_imgs = n_batches * batch_size\n",
    "\n",
    "    pred_arr = np.empty((n_used_imgs, dims))\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if is_numpy:\n",
    "            images = np.copy(files[start:end]) + 1\n",
    "            images /= 2.\n",
    "        else:\n",
    "            images = [np.array(Image.open(str(f))) for f in files[start:end]]\n",
    "            images = np.stack(images).astype(np.float32) / 255.\n",
    "            # Reshape to (n_images, 3, height, width)\n",
    "            images = images.transpose((0, 3, 1, 2))\n",
    "\n",
    "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "        if pred.shape[2] != 1 or pred.shape[3] != 1:\n",
    "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n",
    "\n",
    "    if verbose:\n",
    "        print('done', np.min(images))\n",
    "\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "def extract_lenet_features(imgs, net):\n",
    "    net.eval()\n",
    "    feats = []\n",
    "    imgs = imgs.reshape([-1, 100] + list(imgs.shape[1:]))\n",
    "    if imgs[0].min() < -0.001:\n",
    "      imgs = (imgs + 1)/2.0\n",
    "    print(imgs.shape, imgs.min(), imgs.max())\n",
    "    imgs = torch.from_numpy(imgs)\n",
    "    for i, images in enumerate(imgs):\n",
    "        feats.append(net.extract_features(images).detach().cpu().numpy())\n",
    "    feats = np.vstack(feats)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def _compute_activations(path, model, batch_size, dims, cuda, model_type):\n",
    "    if not type(path) == np.ndarray:\n",
    "        import glob\n",
    "        jpg = os.path.join(path, '*.jpg')\n",
    "        png = os.path.join(path, '*.png')\n",
    "        path = glob.glob(jpg) + glob.glob(png)\n",
    "        if len(path) > 50000:\n",
    "            import random\n",
    "            random.shuffle(path)\n",
    "            path = path[:50000]\n",
    "    if model_type == 'inception':\n",
    "        act = get_activations(path, model, batch_size, dims, cuda)\n",
    "    elif model_type == 'lenet':\n",
    "        act = extract_lenet_features(path, model)\n",
    "    return act\n",
    "\n",
    "\n",
    "def calculate_kid_given_paths(paths, batch_size, cuda, dims, model_type='inception'):\n",
    "    \"\"\"Calculates the KID of two paths\"\"\"\n",
    "    pths = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            raise RuntimeError('Invalid path: %s' % p)\n",
    "        if os.path.isdir(p):\n",
    "            pths.append(p)\n",
    "        elif p.endswith('.npy'):\n",
    "            np_imgs = np.load(p)\n",
    "            if np_imgs.shape[0] > 50000: np_imgs = np_imgs[np.random.permutation(np.arange(np_imgs.shape[0]))][:50000]\n",
    "            pths.append(np_imgs)\n",
    "\n",
    "    if model_type == 'inception':\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "        model = InceptionV3([block_idx])\n",
    "    elif model_type == 'lenet':\n",
    "        model = LeNet5()\n",
    "        model.load_state_dict(torch.load('./models/lenet.pth'))\n",
    "    if cuda:\n",
    "       model.cuda()\n",
    "\n",
    "    act_true = _compute_activations(pths[0], model, batch_size, dims, cuda, model_type)\n",
    "    pths = pths[1:]\n",
    "    results = []\n",
    "    for j, pth in enumerate(pths):\n",
    "        print(paths[j+1])\n",
    "        actj = _compute_activations(pth, model, batch_size, dims, cuda, model_type)\n",
    "        kid_values = polynomial_mmd_averages(act_true, actj, n_subsets=100)\n",
    "        results.append((paths[j+1], kid_values[0].mean(), kid_values[0].std()))\n",
    "    return results\n",
    "\n",
    "def _sqn(arr):\n",
    "    flat = np.ravel(arr)\n",
    "    return flat.dot(flat)\n",
    "\n",
    "\n",
    "def polynomial_mmd_averages(codes_g, codes_r, n_subsets=50, subset_size=1000,\n",
    "                            ret_var=True, output=sys.stdout, **kernel_args):\n",
    "    m = min(codes_g.shape[0], codes_r.shape[0])\n",
    "    mmds = np.zeros(n_subsets)\n",
    "    if ret_var:\n",
    "        vars = np.zeros(n_subsets)\n",
    "    choice = np.random.choice\n",
    "\n",
    "    with tqdm(range(n_subsets), desc='MMD', file=output) as bar:\n",
    "        for i in bar:\n",
    "            g = codes_g[choice(len(codes_g), subset_size, replace=False)]\n",
    "            r = codes_r[choice(len(codes_r), subset_size, replace=False)]\n",
    "            o = polynomial_mmd(g, r, **kernel_args, var_at_m=m, ret_var=ret_var)\n",
    "            if ret_var:\n",
    "                mmds[i], vars[i] = o\n",
    "            else:\n",
    "                mmds[i] = o\n",
    "            bar.set_postfix({'mean': mmds[:i+1].mean()})\n",
    "    return (mmds, vars) if ret_var else mmds\n",
    "\n",
    "\n",
    "def polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1,\n",
    "                   var_at_m=None, ret_var=True):\n",
    "    # use  k(x, y) = (gamma <x, y> + coef0)^degree\n",
    "    # default gamma is 1 / dim\n",
    "    X = codes_g\n",
    "    Y = codes_r\n",
    "\n",
    "    K_XX = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n",
    "    K_YY = polynomial_kernel(Y, degree=degree, gamma=gamma, coef0=coef0)\n",
    "    K_XY = polynomial_kernel(X, Y, degree=degree, gamma=gamma, coef0=coef0)\n",
    "\n",
    "    return _mmd2_and_variance(K_XX, K_XY, K_YY,\n",
    "                              var_at_m=var_at_m, ret_var=ret_var)\n",
    "\n",
    "def _mmd2_and_variance(K_XX, K_XY, K_YY, unit_diagonal=False,\n",
    "                       mmd_est='unbiased', block_size=1024,\n",
    "                       var_at_m=None, ret_var=True):\n",
    "    # based on\n",
    "    # https://github.com/dougalsutherland/opt-mmd/blob/master/two_sample/mmd.py\n",
    "    # but changed to not compute the full kernel matrix at once\n",
    "    m = K_XX.shape[0]\n",
    "    assert K_XX.shape == (m, m)\n",
    "    assert K_XY.shape == (m, m)\n",
    "    assert K_YY.shape == (m, m)\n",
    "    if var_at_m is None:\n",
    "        var_at_m = m\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if unit_diagonal:\n",
    "        diag_X = diag_Y = 1\n",
    "        sum_diag_X = sum_diag_Y = m\n",
    "        sum_diag2_X = sum_diag2_Y = m\n",
    "    else:\n",
    "        diag_X = np.diagonal(K_XX)\n",
    "        diag_Y = np.diagonal(K_YY)\n",
    "\n",
    "        sum_diag_X = diag_X.sum()\n",
    "        sum_diag_Y = diag_Y.sum()\n",
    "\n",
    "        sum_diag2_X = _sqn(diag_X)\n",
    "        sum_diag2_Y = _sqn(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(axis=1) - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(axis=1) - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(axis=0)\n",
    "    K_XY_sums_1 = K_XY.sum(axis=1)\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()\n",
    "    K_XY_sum = K_XY_sums_0.sum()\n",
    "\n",
    "    if mmd_est == 'biased':\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "                + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "                - 2 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        assert mmd_est in {'unbiased', 'u-statistic'}\n",
    "        mmd2 = (Kt_XX_sum + Kt_YY_sum) / (m * (m-1))\n",
    "        if mmd_est == 'unbiased':\n",
    "            mmd2 -= 2 * K_XY_sum / (m * m)\n",
    "        else:\n",
    "            mmd2 -= 2 * (K_XY_sum - np.trace(K_XY)) / (m * (m-1))\n",
    "\n",
    "    if not ret_var:\n",
    "        return mmd2\n",
    "\n",
    "    Kt_XX_2_sum = _sqn(K_XX) - sum_diag2_X\n",
    "    Kt_YY_2_sum = _sqn(K_YY) - sum_diag2_Y\n",
    "    K_XY_2_sum = _sqn(K_XY)\n",
    "\n",
    "    dot_XX_XY = Kt_XX_sums.dot(K_XY_sums_1)\n",
    "    dot_YY_YX = Kt_YY_sums.dot(K_XY_sums_0)\n",
    "\n",
    "    m1 = m - 1\n",
    "    m2 = m - 2\n",
    "    zeta1_est = (\n",
    "        1 / (m * m1 * m2) * (\n",
    "            _sqn(Kt_XX_sums) - Kt_XX_2_sum + _sqn(Kt_YY_sums) - Kt_YY_2_sum)\n",
    "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 1 / (m * m * m1) * (\n",
    "            _sqn(K_XY_sums_1) + _sqn(K_XY_sums_0) - 2 * K_XY_2_sum)\n",
    "        - 2 / m**4 * K_XY_sum**2\n",
    "        - 2 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
    "        + 2 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "    )\n",
    "    zeta2_est = (\n",
    "        1 / (m * m1) * (Kt_XX_2_sum + Kt_YY_2_sum)\n",
    "        - 1 / (m * m1)**2 * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 2 / (m * m) * K_XY_2_sum\n",
    "        - 2 / m**4 * K_XY_sum**2\n",
    "        - 4 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n",
    "        + 4 / (m**3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "    )\n",
    "    var_est = (4 * (var_at_m - 2) / (var_at_m * (var_at_m - 1)) * zeta1_est\n",
    "               + 2 / (var_at_m * (var_at_m - 1)) * zeta2_est)\n",
    "\n",
    "    return mmd2, var_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be45b58-d932-4b81-bbcc-70590e95003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Path: /home/e/emandan/ml/datasets/kneeXray/trainf/train\n",
      "Fake Paths: ['/home/e/emandan/ml/generated_images/WGAN']\n",
      "Batch Size: 50\n",
      "Dims: 2048\n",
      "GPU: \n",
      "Model: inception\n",
      "Found 7828 images in path: /home/e/emandan/ml/datasets/kneeXray/trainf/train\n",
      "Found 10112 images in path: /home/e/emandan/ml/generated_images/WGAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of images is not a multiple of the batch size. Some samples are going to be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [22:12<00:00,  8.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/ml/generated_images/WGAN\n",
      "Warning: number of images is not a multiple of the batch size. Some samples are going to be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [28:10<00:00,  8.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD: 100%|██████████| 100/100 [00:33<00:00,  2.98it/s, mean=0.652]\n",
      "KID (/home/e/emandan/ml/generated_images/WGAN): 0.652 (0.002)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from argparse import ArgumentDefaultsHelpFormatter\n",
    "import os\n",
    "\n",
    "# Define the necessary variables\n",
    "true_path = '/home/e/emandan/ml/datasets/kneeXray/trainf/train'  # Path to the true images\n",
    "fake_paths = ['/home/e/emandan/ml/generated_images/WGAN']  # Path to the generated images\n",
    "batch_size = 50  # Batch size to use\n",
    "dims = 2048  # Dimensionality of Inception features to use\n",
    "gpu = ''  # GPU to use (leave blank for CPU only)\n",
    "model = 'inception'  # Model type: 'inception' or 'lenet'\n",
    "\n",
    "# Print the arguments to verify them\n",
    "print(f\"True Path: {true_path}\")\n",
    "print(f\"Fake Paths: {fake_paths}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Dims: {dims}\")\n",
    "print(f\"GPU: {gpu}\")\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "# Combine the paths\n",
    "paths = [true_path] + fake_paths\n",
    "\n",
    "# Debugging: Check if paths contain images\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "    else:\n",
    "        images = os.listdir(path)\n",
    "        if not images:\n",
    "            print(f\"No images found in path: {path}\")\n",
    "        else:\n",
    "            print(f\"Found {len(images)} images in path: {path}\")\n",
    "\n",
    "# Calculate KID\n",
    "results = calculate_kid_given_paths(paths, batch_size, gpu != '', dims, model_type=model)\n",
    "\n",
    "# Print the results\n",
    "for p, m, s in results:\n",
    "    print('KID (%s): %.3f (%.3f)' % (p, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75509ab9-c64d-417a-a7fd-8fe6261ae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Calculates the Frechet Inception Distance (FID) to evalulate GANs\n",
    "\n",
    "The FID metric calculates the distance between two distributions of images.\n",
    "Typically, we have summary statistics (mean & covariance matrix) of one\n",
    "of these distributions, while the 2nd distribution is given by a GAN.\n",
    "\n",
    "When run as a stand-alone program, it compares the distribution of\n",
    "images that are stored as PNG/JPEG at a specified location with a\n",
    "distribution given by summary statistics (in pickle format).\n",
    "\n",
    "The FID is calculated by assuming that X_1 and X_2 are the activations of\n",
    "the pool_3 layer of the inception net for generated samples and real world\n",
    "samples respectively.\n",
    "\n",
    "See --help to see further details.\n",
    "\n",
    "Code apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\n",
    "of Tensorflow\n",
    "\n",
    "Copyright 2018 Institute of Bioinformatics, JKU Linz\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    # If not tqdm is not available, provide a mock version of it\n",
    "    def tqdm(x): return x\n",
    "#from models import lenet\n",
    "from models.inception import InceptionV3\n",
    "from models.lenet import LeNet5\n",
    "\n",
    "\n",
    "def get_activations(files, model, batch_size=50, dims=2048,\n",
    "                    cuda=False, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number\n",
    "                     of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    is_numpy = True if type(files[0]) == np.ndarray else False\n",
    "\n",
    "    if len(files) % batch_size != 0:\n",
    "        print(('Warning: number of images is not a multiple of the '\n",
    "               'batch size. Some samples are going to be ignored.'))\n",
    "    if batch_size > len(files):\n",
    "        print(('Warning: batch size is bigger than the data size. '\n",
    "               'Setting batch size to data size'))\n",
    "        batch_size = len(files)\n",
    "\n",
    "    n_batches = len(files) // batch_size\n",
    "    n_used_imgs = n_batches * batch_size\n",
    "\n",
    "    pred_arr = np.empty((n_used_imgs, dims))\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if is_numpy:\n",
    "            images = np.copy(files[start:end]) + 1\n",
    "            images /= 2.\n",
    "        else:\n",
    "            images = [np.array(Image.open(str(f))) for f in files[start:end]]\n",
    "            images = np.stack(images).astype(np.float32) / 255.\n",
    "            # Reshape to (n_images, 3, height, width)\n",
    "            images = images.transpose((0, 3, 1, 2))\n",
    "\n",
    "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "        if pred.shape[2] != 1 or pred.shape[3] != 1:\n",
    "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n",
    "\n",
    "    if verbose:\n",
    "        print('done', np.min(images))\n",
    "\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1   : Numpy array containing the activations of a layer of the\n",
    "               inception net (like returned by the function 'get_predictions')\n",
    "               for generated samples.\n",
    "    -- mu2   : The sample mean over activations, precalculated on an\n",
    "               representative data set.\n",
    "    -- sigma1: The covariance matrix over activations for generated samples.\n",
    "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
    "               representative data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(act):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : The images numpy array is split into batches with\n",
    "                     batch size batch_size. A reasonable batch size\n",
    "                     depends on the hardware.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the\n",
    "                     number of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the inception model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the inception model.\n",
    "    \"\"\"\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def extract_lenet_features(imgs, net, cuda):\n",
    "    net.eval()\n",
    "    feats = []\n",
    "    imgs = imgs.reshape([-1, 100] + list(imgs.shape[1:]))\n",
    "    if imgs[0].min() < -0.001:\n",
    "      imgs = (imgs + 1)/2.0\n",
    "    print(imgs.shape, imgs.min(), imgs.max())\n",
    "    if cuda:\n",
    "        imgs = torch.from_numpy(imgs).cuda()\n",
    "    else:\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "    for i, images in enumerate(imgs):\n",
    "        feats.append(net.extract_features(images).detach().cpu().numpy())\n",
    "    feats = np.vstack(feats)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def _compute_activations(path, model, batch_size, dims, cuda, model_type):\n",
    "    if not type(path) == np.ndarray:\n",
    "        import glob\n",
    "        jpg = os.path.join(path, '*.jpg')\n",
    "        png = os.path.join(path, '*.png')\n",
    "        path = glob.glob(jpg) + glob.glob(png)\n",
    "        if len(path) > 25000:\n",
    "            import random\n",
    "            random.shuffle(path)\n",
    "            path = path[:25000]\n",
    "    if model_type == 'inception':\n",
    "        act = get_activations(path, model, batch_size, dims, cuda)\n",
    "    elif model_type == 'lenet':\n",
    "        act = extract_lenet_features(path, model, cuda)\n",
    "\n",
    "    return act\n",
    "\n",
    "\n",
    "def calculate_fid_given_paths(paths, batch_size, cuda, dims, bootstrap=True, n_bootstraps=10, model_type='inception'):\n",
    "    \"\"\"Calculates the FID of two paths\"\"\"\n",
    "    pths = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            raise RuntimeError('Invalid path: %s' % p)\n",
    "        if os.path.isdir(p):\n",
    "            pths.append(p)\n",
    "        elif p.endswith('.npy'):\n",
    "            np_imgs = np.load(p)\n",
    "            if np_imgs.shape[0] > 25000:\n",
    "                np_imgs = np_imgs[:50000]\n",
    "            pths.append(np_imgs)\n",
    "\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "\n",
    "    if model_type == 'inception':\n",
    "        model = InceptionV3([block_idx])\n",
    "    elif model_type == 'lenet':\n",
    "        model = LeNet5()\n",
    "        model.load_state_dict(torch.load('./models/lenet.pth'))\n",
    "    \n",
    "    if cuda:\n",
    "       model.cuda()\n",
    "\n",
    "    act_true = _compute_activations(pths[0], model, batch_size, dims, cuda, model_type)\n",
    "    n_bootstraps = n_bootstraps if bootstrap else 1\n",
    "    pths = pths[1:]\n",
    "    results = []\n",
    "    for j, pth in enumerate(pths):\n",
    "        print(paths[j+1])\n",
    "        actj = _compute_activations(pth, model, batch_size, dims, cuda, model_type)\n",
    "        fid_values = np.zeros((n_bootstraps))\n",
    "        with tqdm(range(n_bootstraps), desc='FID') as bar:\n",
    "            for i in bar:\n",
    "                act1_bs = act_true[np.random.choice(act_true.shape[0], act_true.shape[0], replace=True)]\n",
    "                act2_bs = actj[np.random.choice(actj.shape[0], actj.shape[0], replace=True)]\n",
    "                m1, s1 = calculate_activation_statistics(act1_bs)\n",
    "                m2, s2 = calculate_activation_statistics(act2_bs)\n",
    "                fid_values[i] = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "                bar.set_postfix({'mean': fid_values[:i+1].mean()})\n",
    "        results.append((paths[j+1], fid_values.mean(), fid_values.std()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ec32e7-4f45-4daf-b8b4-7914edc4ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of images is not a multiple of the batch size. Some samples are going to be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [21:50<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/ml/generated_images/WGAN\n",
      "Warning: number of images is not a multiple of the batch size. Some samples are going to be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [28:16<00:00,  8.40s/it]\n",
      "FID: 100%|██████████| 10/10 [01:35<00:00,  9.51s/it, mean=436]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID (/home/e/emandan/ml/generated_images/WGAN): 436.02 (0.531)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = calculate_fid_given_paths(paths, batch_size, gpu != '', dims, model_type=model)\n",
    "for p, m, s in results:\n",
    "    print('FID (%s): %.2f (%.3f)' % (p, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba50b0-c5d4-4c40-b044-aa36f8ce4b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining functions\n",
      "Gathering image paths\n",
      "Gathered all image paths..\n",
      "Starting the calculation of IS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/e/emandan/mlenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"Defining functions\")\n",
    "def inception_score(images, batch_size=32, splits=10):\n",
    "    N = len(images)\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Load the InceptionV3 model\n",
    "    inception_model = models.inception_v3(pretrained=True, transform_input=False).eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear')\n",
    "\n",
    "    def get_pred(x):\n",
    "        x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return torch.nn.functional.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    # Preprocess images\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch = images[i:i + batch_size]\n",
    "        batch = torch.stack([preprocess(Image.open(img_path)) for img_path in batch], dim=0)\n",
    "        with torch.no_grad():\n",
    "            preds[i:i + batch_size] = get_pred(batch)\n",
    "\n",
    "    # Compute the Inception Score\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k + 1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in tqdm(range(part.shape[0])):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(np.sum(pyx * np.log(pyx / py)))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# Define paths to generated images\n",
    "fake_paths = ['/home/e/emandan/ml/generated_images/WGAN']\n",
    "\n",
    "print(f\"Gathering image paths\")\n",
    "# Gather all image paths\n",
    "images = []\n",
    "for path in fake_paths:\n",
    "    for img_file in os.listdir(path):\n",
    "        images.append(os.path.join(path, img_file))\n",
    "print(f\"Gathered all image paths..\")\n",
    "\n",
    "# Calculate Inception Score\n",
    "print(f\"Starting the calculation of IS\")\n",
    "mean_score, std_score = inception_score(images)\n",
    "print(f\"Inception Score: {mean_score:.3f} ± {std_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My ML Environment",
   "language": "python",
   "name": "mymlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
